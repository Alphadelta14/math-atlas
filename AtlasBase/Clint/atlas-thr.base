@rout atlas_threads.h
#ifndef ATLAS_THREADS_H
   #define ATLAS_THREADS_H

#ifndef ATL_PTMAXMALLOC
   #ifndef ATL_PTMAXMALLOC_MB
      #define ATL_PTMAXMALLOC_MB 128
   #endif
   #define ATL_PTMAXMALLOC (((size_t)(ATL_PTMAXMALLOC_MB)<<20)>>ATL_NTHRPOW2)
#endif
/*
 * If we don't have thread affinity, then the thread I'm waiting on may share
 * the core with me.  In this case, yield my time slice when polling
 */
#include "atlas_tsumm.h"
#if defined(ATL_TAFFINITY) && ATL_TAFFINITY
   #define ATL_POLL
#else
   #define ATL_POLL ATL_thread_yield()
#endif

#if defined(ATL_OS_Win64) || defined(ATL_OS_WinNT)
   #ifdef ATL_USE64BITS
@skip      #ifdef __MINGW64__  /* use pthreads if using cygwin gcc */
         #define ATL_WIN64THREADS 1
         #define ATL_WINTHREADS 1
@skip      #endif
   #else
      #define ATL_WIN32THREADS 1
      #define ATL_WINTHREADS 1
   #endif
#endif
@beginskip
/*
 * The following defaults should be probed for, not defined
 * NOTE: OS X & FreeBSD lack the ability to set processor affinity.
 */
#ifdef ATL_OS_WinNT
   #define ATL_WINTHREADS
#elif !defined(ATL_PAFF_LAUNCH) && !defined(ATL_PAFF_SELF)
   #if defined(ATL_OS_Linux)
      #define ATL_PAFF_LAUNCH
      #define ATL_PAFF_SETAFFNP
   #elif defined(ATL_OS_HPUX)
      #define ATL_PAFF_LAUNCH
      #define ATL_PAFF_SETPROCNP
/*
 *  UltraSPARC T2 gets much better || perf w/o affinity;  need to test USIV
 */
   #elif defined(ATL_OS_SunOS) && !defined(ATL_ARCH_UST2)
      #define ATL_PAFF_SELF
      #define ATL_PAFF_PBIND
   #elif defined(ATL_OS_IRIX)
      #define ATL_PAFF_SELF
      #define ATL_PAFF_RUNON
   #elif defined(ATL_OS_AIX)
      #define ATL_PAFF_SELF
      #define ATL_PAFF_BINDP
   #endif
   #ifndef ATL_RANK_IS_PROCESSORID
      #define ATL_RANK_IS_PROCESSORID 1
   #endif
#endif
@endskip
#include "atlas_pthreads.h" /* gened file defs ATL_NTHREADS & ATL_NTHRPOW2 */
#ifdef ATL_WINTHREADS
   #include <windows.h>
   typedef struct
   {
      void *vp;      /* ptr to extra info */
      HANDLE thrH;   /* handle to thread */
      int rank;      /* my rank */
      int P;         /* # of processors in this call */
   } ATL_thread_t;
@beginskip
   #include <windef.h>
   #include <winnt.h>
   typedef struct
   {
      DWORD thrID;
      HANDLE thrH;
   } ATL_thread_t;
   #define ATL_thread_t HANDLE
@endskip
   #ifndef CREATE_SUSPENDED
      #define CREATE_SUSPENDED 0x00000004
   #endif
   #ifndef WAIT_FAILED
      #define WAIT_FAILED (~0)
   #endif
#elif defined(ATL_OMP_THREADS)
   #include <omp.h>
   typedef struct
   {
      void *vp;      /* ptr to extra info */
      int rank;      /* my rank */
      int P;         /* # of processors in this call */
      int paff_set;  /* have I set my process affinity yet? */
   } ATL_thread_t;
#else
/*
 * Note that we only use paff_set when ATL_PAFF_SELF is defined, but
 * we define it all the time, since not everyone includes atlas_taffinity.h,
 * which is where this define comes from
 */
   #include <pthread.h>
   typedef struct
   {
      pthread_t thrH;/* handle of thread */
      void *vp;      /* ptr to extra info */
      int rank;      /* my rank */
      int P;         /* # of processors in this call */
      int paff_set;  /* have I set my process affinity yet? */
   } ATL_thread_t;
@skip   #define ATL_thread_t pthread_t
#endif

typedef struct ATL_LaunchStruct ATL_LAUNCHSTRUCT_t;
struct ATL_LaunchStruct
{
   ATL_thread_t *rank2thr;              /* index by rank to get thread handle */
   void *opstruct;
   int (*OpStructIsInit)(void*);        /* Query to see if struct is valid */
   void (*DoWork)(ATL_LAUNCHSTRUCT_t*, void*);
   void (*DoComb)(void*, const int, const int);  /* combine func */
   volatile int *chkin;                 /* nthr-len checkin array */
   void **acounts;                      /* var-len array of atomic counters */
   void *vp;                            /* misc. extra info ptr */
};

/*
 * Constants for use in chkin array
 */
#define ATL_CHK_DONE_OP   -2048
#define ATL_CHK_DONE_COMB -2049

/* Sets up ATL_LAUNCHSTRUCT_t var and ATL_thread_t array & starts threads*/
void ATL_thread_launch(void *opstruct, int opstructstride, void *OpStructIsInit,
                       void *DoWork, void *CombineOpStructs);
void ATL_goparallel(const unsigned int P, void *DoWork, void *opstruct, void*);
void ATL_goparallel_prank(const unsigned int P, void *DoWork, void *opstruct,
                          void *DoComb);
/*  Starts a thread running, and sets its affinity to proc if possible */
int ATL_thread_start(ATL_thread_t *thr, int proc, int JOINABLE, 
                     void *(*rout)(void*), void*);
int ATL_thread_join(ATL_thread_t*); /* waits on completion of thread */
void ATL_thread_exit(void*);        /* exits currently executing thread */
void *ATL_log2tlaunch(void *vp);    /* min spanning tree launch */
void *ATL_lin0tlaunch(void *vp);    /* 0 linear launches all threads */
void *ATL_dyntlaunch(void *vp);     /* launch done as workpool */
/*
 * Atomic count functions; may be less overhead than mutex on some systems
 */
void *ATL_SetAtomicCount(int cnt);   /* allocates acnt, sets acnt=cnt */
int   ATL_ResetAtomicCount(void *vp, int cnt);  /* reset vp to cnt */
int   ATL_DecAtomicCount(void *vp);  /* returns acnt-- (not --acnt!) */
int   ATL_GetAtomicCount(void *vp);  /* returns acnt */
void  ATL_FreeAtomicCount(void *vp); /* free acount resources */
/*
 * Global count functions, built out of P local counters for good scaling
 */
void *ATL_SetGlobalAtomicCount(int P, int cnt, int percLoc); 
void  ATL_ResetGlobalAtomicCount(void *vp, int cnt, int percLoc);
int   ATL_DecGlobalAtomicCount(void *vp, int rank); 
int   ATL_GetGlobalAtomicCount(void *vp, int rank);
void  ATL_FreeGlobalAtomicCount(void *vp);
/*
 * If using pthreads, just wrapper around pthread mutex funcs, else written
 * in terms of the AtomicCount funcs with init value set to 1
 */
void *ATL_mutex_init(void);       /* returns mutex pointer */
void ATL_mutex_free(void *vp);    /* frees mutex vp */
void ATL_mutex_lock(void *vp);
void ATL_mutex_unlock(void *vp);
int  ATL_mutex_trylock(void *vp); /* opp pthreads: 0 means lock NOT acquired */
void ATL_thread_yield(void);      /* gives up time slice */


#define MindxT(A_,i_) ((void*)( ((char*)(A_)) + ((size_t)i_) ))
#define ATL_tlaunch ATL_log2tlaunch   /* may want linear launch later */
void ATL_tDistMemTouch(size_t N, void *vp);

#endif   /* end of #ifdef protecting include file from redundant inclusion */

@ROUT atlas_tlevel3.h
#ifndef ATLAS_TLEVEL3_H
   #define  ATLAS_TLEVEL3_H
/*
 * ========================================
 * Threaded routines in all four precisions
 * ========================================
 */
@multidef styp double@^*  float@^* double@^ float@^
@multidef typ double float double float
@whiledef pre z c d s
int ATL_@(pre)threadMM(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
                  size_t M, size_t N, size_t K);
@whiledef rt gemm
void ATL_@(pre)t@(rt)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M, 
    ATL_CINT N, ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda, 
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
@endwhile
void ATL_@(pre)tsymm
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo,
    ATL_CINT M, ATL_CINT N, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
   @whiledef rt trmm trsm
void ATL_@(pre)t@(rt)
   (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
    const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag, 
    ATL_CINT M, ATL_CINT N, const @(styp)alpha, const @(typ) *A, ATL_CINT lda, 
    @(typ) *B, ATL_CINT ldb);
   @endwhile
void ATL_@(pre)tsyr2k
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
void ATL_@(pre)tsyrk
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(styp)beta, @(typ) *C, ATL_CINT ldc);
   @undef typ
   @undef styp
@endwhile

/*
 * =======================================================
 * Threaded routines appearing only for complex precisions
 * =======================================================
 */
@multidef sty2 double@^ float@^
@multidef styp double@^*  float@^*
@multidef typ double float
@whiledef pre z c
void ATL_@(pre)themm
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo,
    ATL_CINT M, ATL_CINT N, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
void ATL_@(pre)ther2k
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(sty2)beta, @(typ) *C, ATL_CINT ldc);
void ATL_@(pre)therk
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const @(sty2)alpha, const @(typ) *A, ATL_CINT lda,
    const @(sty2)beta, @(typ) *C, ATL_CINT ldc);
   @undef typ
   @undef styp
   @undef sty2
@endwhile
#endif
@ROUT atlas_tlvl3.h
#ifndef atlas_tlvl3_H
   #define atlas_tlvl3_H

#include "atlas_threads.h"
#ifdef TYPE
   #include "atlas_lvl3.h"
#endif
#ifndef ATL_XOVER_L3
   #ifdef TREAL
      #define ATL_XOVER_L3 2   /* number of NBxNB blocks */
   #else
      #define ATL_XOVER_L3 1
   #endif
#endif

#ifndef ATL_TGEMM_XOVER
   #define ATL_TGEMM_XOVER ATL_XOVER_L3
#endif
#ifndef ATL_TGEMM_ADDP
   #define ATL_TGEMM_ADDP 1
#endif
/*
 * Number of blocks per proc for GEMM to divide M only
 */
#ifndef ATL_TMMMINMBLKS
   #define ATL_TMMMINMBLKS 4
#endif
#ifndef ATL_TGEMM_THRESH_MF
   #define ATL_TGEMM_THRESH_MF  ((((2.0*(ATL_TGEMM_XOVER))*MB)*NB)*KB)
#endif
/*
 * This is the minimal number of flops each thread requires once THRESH
 * is exceeded
 */
#ifndef ATL_TGEMM_PERTHR_MF
   #define ATL_TGEMM_PERTHR_MF  ((((2.0*ATL_TGEMM_ADDP)*MB)*NB)*KB)
#endif
/*
 * For debugging, can define ATL_SERIAL_COMBINE, and then it any required
 * workspaces of C will be allocated before beginning parallel operations,
 * and all required combined will happen after parallel operations are
 * done.
 */
// #define ATL_SERIAL_COMBINE
#ifdef ATL_SERIAL_COMBINE
typedef struct ATL_CombNode ATL_combnode_t;
struct ATL_CombNode
{
   ATL_INT M, N, ldw, ldd;
   void *W, *D;                 /* Work and Destination */
   ATL_combnode_t *next;
};
#endif
/*
 * The array Cinfp holds C partitioning information.  This array holds a
 * list of pointers to nodes whose data I have not been able to combine
 * with my native C partition.  The first nCw entries contain the pointers
 * to the MMNODE of allocated C workspaces that I have not been able to
 * combine.  If my node has C in workspace, I am the first entry in this array.
 * Sometimes, a child thread has been combined with me that owned a piece of
 * the original C.  These values do not need to be combined (they were written
 * to the original C), but we need to combine the range of "owned" workspaces
 * so that we know when it is legal for a parent node to add into the space.
 * The final nCp entries of Cinfp entries of Cinfp hold these original pieces
 * that need to be combined to create larger owned partitions (starting from 
 * the end of the array).  If the C ptr is NULL, that means that entry has
 * been subsumed into a new entry.
 */
typedef struct ATL_TMMNode ATL_TMMNODE_t;
struct ATL_TMMNode
{
   ATL_TMMNODE_t *Cinfp[ATL_NTHREADS];
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
   const void *A, *B;
   void *C, *Cw;
   void *alpha, *beta;
   void *zero, *one;
   ATL_INT ldcw, M, N, K, lda, ldb, ldc;
   int mb, nb, kb;
   int eltsz, eltsh; /* element size, and shift (eg. log_2(eltsz)) */
   int rank;         /* the rank of my thread ([0,P-1]) */
   int nCw;          /* # of workspace entries in 1st nCw elts of Cinfp array */
   int nCp;          /* # of orig. C pieces last nCp elts of Cinfp */
   int ownC;         /* do I own my piece of C, or only wrkspace? */
};
/*
 * This data structure used for dynamically scheduled rank-K update
 * It is needed only by routines that are typed, and thus define TYPE
 */
#ifdef TYPE
typedef struct
{
   void *aNcnt;           /* count on col-panels of C */
   void *aMcnt;           /* count row-panels of A */
   void **aMcnts;         /* P-len array of counts on row-blks of C */
   void **Mlocks;         /* mutexes protecting init of aMcnts */
   int *Js;               /* current C col for each node */
   int Sync0;             /* 0: no sync at end; else thr 0 waits til all done */
   volatile int *chkin;   /* ATL_NTHREAD-len checkin array */
   TYPE **Bws;            /* preallocated thread copy areas */
   TYPE *Aw;              /* workspace for common A */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   #ifdef TREAL
      TYPE alpha;          
      TYPE beta;
   #else
      const TYPE *alpha;          
      const TYPE *beta;
   #endif
   ATL_INT nKb, kr, kr8;
   ATL_INT nMb, mr, nNb, nr;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_RKK_t;
#endif

/*
 * This data structure is used when we split K for SYRK
 */
typedef struct ATL_SyrkK ATL_TSYRK_K_t;
struct ATL_SyrkK
{
   ATL_TSYRK_K_t *Cinfp[ATL_NTHREADS];
@beginskip
   void (*gemmT)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
@endskip
   void (*gemmT)(const enum ATLAS_TRANS, const enum ATLAS_TRANS,
                  ATL_CINT, ATL_CINT, ATL_CINT, const void *,
                  const void *, ATL_CINT, const void *, ATL_CINT,
                  const void *, void *, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   const void *A;
   void *C, *Cw;
   void *DoComb;
   ATL_LAUNCHSTRUCT_t *lp;
   const void *alpha, *beta;
   const void *zero, *one;
   ATL_INT ldcw, N, K, nb, lda, ldc;
   int eltsh, rank, nCw;
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS Trans, TB;
};
/*
 * This data structure used when we divide N only, and NTHREAD is a power of 2
 */
typedef struct 
{
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   void *T;             /* Triangular matrix to do SYRK into*/
   void *C;             /* rect matrix to do GEMM into */
   const void *A0;      /* input matrix for syrk, */
   const void *A;       /* 1st input matrix for GEMM */
   const void *B;       /* 2nd input matrix for GEMM */
   const void *alpha, *beta;
   ATL_INT M;           /* size of SYRK and 1st dim of GEMM */
   ATL_INT N;           /* size of 2nd dim of N */
   ATL_INT K;           /* K of original problem */
   ATL_INT lda, ldc;
   int nb, eltsh;       /* shift to do equivalant of *sizeof */
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS TA, TB;
} ATL_TSYRK_M_t;

@beginskip
#if ATL_NTHREADS == (1<<ATL_NTHRPOW2)
typedef struct
{
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   const void *A0;  /* input matrix, split only along N */
   const void *A1;  /* A of 2nd triangular matrix (C), or B of gemm */
   void *T;         /* 1st triangular matrix */
   void *C;         /* if (T), 2nd triangular mat, else rect matrix */
   const void *alpha, *beta;
   ATL_INT M;      /* if (T) order of 1st triang mat, else 1st dim of C */
   ATL_INT N;      /* if (T) order of 2nd triang mat, else 2nd dim of C */
   ATL_INT K;      /* size of K dim (2nd dim of A, first of A^T) */
   ATL_INT lda, ldc;
   int nb, eltsh;  /* shift to do equivalant of *sizeof */
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS TA, TB;
} ATL_TSYRK_N_t;
/*
 * sets ATL_TSYRK_N_t sy_[i] = sy[j]
 */
#define McpSYN(sy_, i_, j_) \
@skip   memcpy((sy_)+(i_), (sy_)+(j_), sizeof(ATL_TSYRK_N_t));
{ \
   (sy_)[(i_)].gemmK  = (sy_)[(j_)].gemmK; \
   (sy_)[(i_)].tvsyrk = (sy_)[(j_)].tvsyrk; \
   (sy_)[(i_)].numthr = (sy_)[(j_)].numthr; \
   (sy_)[(i_)].A0     = (sy_)[(j_)].A0; \
   (sy_)[(i_)].A1     = (sy_)[(j_)].A1; \
   (sy_)[(i_)].T      = (sy_)[(j_)].T; \
   (sy_)[(i_)].C      = (sy_)[(j_)].C; \
   (sy_)[(i_)].alpha  = (sy_)[(j_)].alpha; \
   (sy_)[(i_)].beta   = (sy_)[(j_)].beta; \
   (sy_)[(i_)].M      = (sy_)[(j_)].M; \
   (sy_)[(i_)].N      = (sy_)[(j_)].N; \
   (sy_)[(i_)].K      = (sy_)[(j_)].K; \
   (sy_)[(i_)].nb     = (sy_)[(j_)].nb; \
   (sy_)[(i_)].lda    = (sy_)[(j_)].lda; \
   (sy_)[(i_)].ldc    = (sy_)[(j_)].ldc; \
   (sy_)[(i_)].eltsh  = (sy_)[(j_)].eltsh; \
   (sy_)[(i_)].Uplo   = (sy_)[(j_)].Uplo; \
   (sy_)[(i_)].TA     = (sy_)[(j_)].TA; \
   (sy_)[(i_)].TB     = (sy_)[(j_)].TB; \
}
#endif
@endskip

typedef struct
{
   const void *A, *alpha;
   void *B;
@skip   void (*trsmK)(ATL_TTRSM_t*);
   ATL_INT M, N, lda, ldb;
@skip   int eltsh;                   /* shift for element size */
   enum ATLAS_SIDE side;
   enum ATLAS_UPLO uplo;
   enum ATLAS_TRANS TA;
   enum ATLAS_DIAG  diag;
} ATL_TTRSM_t;

typedef struct
{
   const void *A, *B, *alpha, *beta;
   void *C;
   ATL_INT M, N, lda, ldb, ldc, nb;
   enum ATLAS_SIDE side;
   enum ATLAS_UPLO uplo;
} ATL_TSYMM_t;
typedef struct
{
   const void *alpha, *alpha2, *beta, *one, *zero;
   void (*tvgemm)(const enum ATLAS_TRANS, const enum ATLAS_TRANS,
                  ATL_CINT, ATL_CINT, ATL_CINT, const void *,
                  const void *, ATL_CINT, const void *, ATL_CINT,
                  const void *, void *, ATL_CINT);
   void (*tvApAt)(const enum ATLAS_UPLO, ATL_CINT, const void *, ATL_CINT, 
                  const void *, void *, ATL_CINT);

   ATL_INT K, lda, ldb, ldc;
   int nb, eltsh;
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS trans, TA, TB,  /* trans for syr2k, TA,TB are for GEMM */
                    TA2, TB2;       /* transpose of TA,TB */
} ATL_SYR2K_t;

@beginskip
typedef struct
{
   const void *alpha, *beta, *one, *zero;
   void (*tvgemm)(const enum ATLAS_TRANS, const enum ATLAS_TRANS,
                  ATL_CINT, ATL_CINT, ATL_CINT, const void *,
                  const void *, ATL_CINT, const void *, ATL_CINT,
                  const void *, void *, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   const void *A, *B;
   void *C;
   ATL_INT T, M, N, K, nb, ia, ja, ib, jb, ic, jc, eltsh, lda, ldc;
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS Trans;
} ATL_TSYRK_t;
@endskip
/*
 * =============================================================================
 * Function prototypes
 * =============================================================================
 */
void ATL_EnforceNonPwr2LO(ATL_TMMNODE_t *ptmms, const int P);
int Mjoin(PATL,threadMM)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
                         size_t M, size_t N, size_t K);
@skip int Mjoin(PATL,tNumGemmThreads)(ATL_CINT M, ATL_CINT N, ATL_CINT K);
void ATL_tvsyr2k_rec(ATL_SYR2K_t *syp, ATL_CINT Nblks, ATL_CINT nr, 
                     const void *A, const void *B, void *C);
#ifdef TYPE
void Mjoin(PATL,tsyrk)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const SCALAR beta, TYPE *C, ATL_CINT ldc);
#ifdef TCPLX
void Mjoin(PATL,therk)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const TYPE alpha, const TYPE *A, ATL_CINT lda,
    const TYPE beta, TYPE *C, ATL_CINT ldc);
#endif
void Mjoin(PATL,tsymm)
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
#ifdef TCPLX
void Mjoin(PATL,themm)
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
#endif
void Mjoin(PATL,tsyr2k)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
void Mjoin(PATL,tsyr2k)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
#ifdef TCPLX
void Mjoin(PATL,ther2k)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const TYPE beta, TYPE *C, ATL_CINT ldc);
#endif

void Mjoin(PATL,ttrsm)(const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo, 
                       const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
                       ATL_CINT M, ATL_CINT N, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb);
void Mjoin(PATL,ttrmm)(const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo, 
                       const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
                       ATL_CINT M, ATL_CINT N, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb);
void Mjoin(PATL,tgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                       ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
                       const SCALAR beta, TYPE *C, ATL_CINT ldc);
void Mjoin(PATL,tvgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                        ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *alpha,
                        const void *A, ATL_CINT lda, const void *B,ATL_CINT ldb,
                        const void *beta, void *C, ATL_CINT ldc);
@whiledef ds rec rkK bigMN_Kp
int Mjoin(PATL,tgemm_@(ds))
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
    const SCALAR beta, TYPE *C, ATL_CINT ldc);
@endwhile
@whiledef TA T N C
   @whiledef TB T N C
@mif TA = "C
#ifdef TCPLX
@endmif
@mif TB = "C
   @mif TA ! "C
#ifdef TCPLX
   @endmif
@endmif
void Mjoin(PATL,tgemm@(TA)@(TB))
   (ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
    const SCALAR beta, TYPE *C, ATL_CINT ldc);
void Mjoin(PATL,tsvgemm@(TA)@(TB))
   (ATL_CINT M, ATL_CINT N, ATL_CINT K, const void* alpha,
    const void *A, ATL_CINT lda, const void *B, ATL_CINT ldb,
    const void *beta, void *C, ATL_CINT ldc);
@mif TA = "C
#endif  /* end ifdef TCPLX */
@endmif
@mif TB = "C
   @mif TA ! "C
#endif  /* end ifdef TCPLX */
   @endmif
@endmif
   @endwhile
@endwhile
#endif  /* end ifdef TYPE */

#endif
@ROUT ATL_set_ucnt
/*
 * This routines provide the basis of ATLAS's fast and almost-contention-free
 * partitioning algorithm.  Before spawning the threads, the problem is
 * divided into n partitions, and the master thread allocates an unordered
 * counter wt call to ATL_set_ucnt(n).  
 * After threading is complete, the space is freed by ATL_free_ucnt(void*vp);
 * Now, threads call ATL_get_ucnt(void *vp), which returns 0 if all partitions
 * have been handled; a number between 1 & n says that that partition of the
 * problem remains to be done.
 * We can then use an assembly command like XCHG to determine if a given
 * set is available for use.
 * To ease the amount of cache coherence message, each thread gets his
 * own region in each space, which looks like:
 * <p> <n1> <rk1 off> ... <nP> <rkP off> <rk1 region>...<rkP region>
 * beginning of space aligned to CL, all regions start on CL boundary.  Each
 * region is n/p long, with any remainders stuck in the first n%P regions.
 * Each thread will pass his rank in and therefore will do the majority of
 * writing on his own region (avoiding ping-ponging lines thru cache).  
 * Only once all his sets are exhausted will he search other thread's sets
 * (thus starting ping-pong).
 * All we need to implement is something like XCHG, which everybody has:
 * SPARC: LDSTUB: ld/store unsigned byte loads value from memory, rights 0xff
 *                to byte atomically 
 *           http://developers.sun.com/solaris/articles/atomic_sparc/
 * x86  : XCHG
 * PPC  : lwarx/stwcx, see: http://www.ibm.com/developerworks/library/pa-atom/
 * MIPS : ll/sc (load linked store conditional); I think similar to PPC
 *
 * In some systems, we can actually using a simple counter:
 *  SPARC: cas (compare & swap), v9 only
 *   x86 : CMPXCHG
 *   PPC : lwarx/stwcx
 *   MIPS: ll/sc
 *
 * So, can use cntr for all harware we now about, so can implement an assembly
 *    int GetAtomicCount(void *vp)
 * In systems wt cntr support, simply use directly, quitting when count is 0
 * or negative.  For systems that can only handle a boolean (like XCHG/LDSTUB),
 * store the boolean immediately after the counter.  This file will be tested
 * in assembly for compile & use, and we set a Make.inc macro to point at
 * something like: GetAtomicCount_[ppc,sparc,x86,mips,mutex].o, all of
 * which appear in the ATLAS/src/threads directory.
 */
void *ATL_set_ucnt(int nsets, int nranks)
{
}
void ATL_free_ucnt(void *vp)
{
}
int ATL_get_ucnt(void *vp, int rank)
/* 
 * RETURNS: 0 if all sets taken, else a number between 1...nsets
{
}
@ROUT ATL_thread_yield
#include "atlas_misc.h"
#include "atlas_threads.h"
#ifndef ATL_WINTHREADS
   #include <sched.h>
#endif
void ATL_thread_yield(void)
{
   #ifdef ATL_WINTHREADS
      Sleep(0);
   #else
      sched_yield();
   #endif
}
@ROUT ATL_mutex_init
#include "atlas_misc.h"
#include "atlas_threads.h"
void *ATL_mutex_init(void)
{
/*
 * On Windows, use known-good x86 code.  OS X's mutex have horrible scaling,
 * so use homebrewed code instead
 */
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   return(ATL_SetAtomicCount(1));
#elif defined(ATL_OMP_THREADS)
   void *vp;
   vp = malloc(sizeof(omp_lock_t));
   ATL_assert(vp);
   omp_init_lock(vp);
   return(vp);
#else
   void *vp;
   vp = malloc(sizeof(pthread_mutex_t));
   ATL_assert(vp);
   ATL_assert(!pthread_mutex_init(vp, NULL));
   return(vp);
#endif
}
@ROUT ATL_mutex_free
#include "atlas_misc.h"
#include "atlas_threads.h"
void ATL_mutex_free(void *vp)
{
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   ATL_FreeAtomicCount(vp);
#elif defined(ATL_OMP_THREADS)
   omp_destroy_lock(vp);
   free(vp);
#else
   ATL_assert(!pthread_mutex_destroy(vp));
   free(vp);
#endif
}
@ROUT ATL_mutex_free
@ROUT ATL_mutex_lock
   @define rt @lock@
@ROUT ATL_mutex_unlock
   @define rt @unlock@
@ROUT ATL_mutex_lock ATL_mutex_unlock
#include "atlas_misc.h"
#include "atlas_threads.h"

void ATL_mutex_@(rt)(void *vp)
{
@ROUT ATL_mutex_lock
#ifdef ATL_WINTHREADS  /* if not using pthreads, use AtomicCount to sim mut */
   while(!ATL_DecAtomicCount(vp));
#elif defined(ATL_OS_OSX) && defined(ATL_SSE1)
   while(!ATL_DecAtomicCount(vp))
      ATL_thread_yield();
@ROUT ATL_mutex_unlock
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   ATL_ResetAtomicCount(vp, 1);
@ROUT ATL_mutex_lock ATL_mutex_unlock
#elif defined(ATL_OMP_THREADS)
@ROUT ATL_mutex_unlock `   omp_unset_lock(vp);`
@ROUT ATL_mutex_lock `   omp_set_lock(vp);`
#else
   ATL_assert(!pthread_mutex_@(rt)(vp));
#endif
}
@ROUT ATL_mutex_trylock
#include "atlas_misc.h"
#include "atlas_threads.h"

int ATL_mutex_trylock(void *vp)
/*
 * return 0 if lock not required, else return non-zero 
 */
{
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   return(ATL_DecAtomicCount(vp));
#elif defined(ATL_OMP_THREADS)
   return(omp_test_lock(vp));
#else
   return(!pthread_mutex_trylock(vp));
#endif
}
@ROUT ATL_SetAtomicCount_mut
#include "atlas_misc.h"
#include "atlas_threads.h"

void *ATL_SetAtomicCount(int cnt)
{
#if defined(ATL_OMP_THREADS)
   char *cp;
   omp_lock_t *mp;
   int *cntp;
   cp = malloc(256+sizeof(int) + sizeof(omp_lock_t));
   ATL_assert(cp);
   cntp = (int*)(cp+128);  /* avoid false sharing wt 128-byte guard */
   mp = (omp_lock_t*)(cntp+2);
   omp_init_lock(mp);
   *cntp = cnt;
   return((void*)cp);
#else
   char *cp;
   pthread_mutex_t *mp;
   int *cntp;
   cp = malloc(256+sizeof(int) + sizeof(pthread_mutex_t));
   ATL_assert(cp);
   cntp = (int*)(cp+128);  /* avoid false sharing wt 128-byte guard */
   mp = (pthread_mutex_t*)(cntp+2);
   ATL_assert(!pthread_mutex_init(mp, NULL));
   *cntp = cnt;
   return((void*)cp);
#endif
}

@ROUT ATL_SetGlobalAtomicCount ATL_ResetGlobalAtomicCount
#include "atlas_misc.h"
#include "atlas_threads.h"

@ROUT ATL_ResetGlobalAtomicCount
void ATL_ResetGlobalAtomicCount(void *vp, int cnt, int percLoc)
/*
 * This routine resets the global atomic counter vp to cnt
 */
{
   int *ip=vp, *lcnts = ip+4;
   const int P = *ip;
   void **cnts = (void**)(lcnts+(((P+3)>>2)<<2));
   int i, b, extra, nL, nG;

@ROUT ATL_SetGlobalAtomicCount
void *ATL_SetGlobalAtomicCount
(
   int P,               /* # of Local counters to use to make global ctr */
   int cnt,             /* total count to start global count at */
   int percLoc          /* fraction of local work to reserve for callers */
)                       /* whose rank is exactly equal to the cnt index */
/*
 * This routine counts down from cnt to 0 for in a thread-safe way.
 * For scalability, the count is split up into P different counters
 * (this minimizes contention on atomic counters).  Further, if percLoc
 * is non-zero, then .01*fracLoc*local(cnt) numbers will be reserved exclusively
 * for callers that set their rank to the local counter index.  This allows
 * us to force a particular node to do at least that many columns, and for
 * those column accesses we can do it a non-rentrant read, which means it
 * runs much faster.  However, this means the caller will need to be sure
 * in this case that two processors cannot call with the same rank!
 */
{
   void **cnts;
   int *ip, *lcnts;
   int i, b, extra, nL, nG;

@ROUT ATL_SetGlobalAtomicCount ATL_ResetGlobalAtomicCount
   b = cnt / P;
   extra = cnt - b*P;
   nL = (percLoc > 0) ? percLoc*.01*b : 0;
   nG = b - nL;
@ROUT ATL_SetGlobalAtomicCount
   i = ((P+3)>>2)<<2;
   ip = malloc(P*sizeof(void*)+(i+4)*sizeof(int));
   ATL_assert(ip);
   lcnts = ip+4;
   cnts = (void**)(lcnts + i);
@ROUT ATL_SetGlobalAtomicCount ATL_ResetGlobalAtomicCount
   ip[0] = P;
   ip[1] = b;
   ip[2] = extra;
   ip[3] = nL;

   for (i=0; i < P; i++)
   {
      int n = nG;
      if (i < extra) n++;
@ROUT ATL_SetGlobalAtomicCount   `      cnts[i] = ATL_SetAtomicCount(n);`
@ROUT ATL_ResetGlobalAtomicCount `      ATL_ResetAtomicCount(cnts[i], n);`
      lcnts[i] = nL;
   }
@ROUT ATL_SetGlobalAtomicCount `   return((void*)ip);`
}

@ROUT ATL_FreeGlobalAtomicCount
#include "atlas_misc.h"
#include "atlas_threads.h"

void ATL_FreeGlobalAtomicCount(void *vp)
{
   int *ip=vp;
   const int P=ip[0];
   void **acnts = (void**)(ip+4+(((P+3)>>2)<<2));
   int i;
   for (i=0; i < P; i++)
      ATL_FreeAtomicCount(acnts[i]);
   free(vp);
}

@ROUT ATL_DecGlobalAtomicCount
   @define op @Dec@
@ROUT ATL_GetGlobalAtomicCount
   @define op @Get@
@ROUT ATL_DecGlobalAtomicCount ATL_GetGlobalAtomicCount
#include "atlas_misc.h"
#include "atlas_threads.h"
int ATL_@(op)GlobalAtomicCount(void *vp, int rank)
/*
 * This routine returns a global counter that has been distributed over
 * P local counters
 */
{
   int i, j, P, b, icnt, extra, nL, *ip=vp, *iloc;
   void **acnts;

   P = ip[0];
   b = ip[1];
   extra = ip[2];
   nL = ip[3];
   iloc = ip+4;
/*
 * See if I can get the index from purely local information
 */
   if (rank < P && rank >= 0 && nL)
   {
      j = iloc[rank];
      if (j)
      {
@ROUT ATL_DecGlobalAtomicCount `         iloc[rank]--;`
         j += b * rank + Mmin(rank, extra);
//fprintf(stderr, "%d: j=%d, LRET\n", rank, j);
         return(j);
      }
   }
   acnts = (void**) (ip+4+(((P+3)>>2)<<2));
/*
 * Otherwise, find an atomic counter that still has count
 */
   for (i=0; i < P; i++)
   {
/* 
 *    If I got a counter value, convert it from local to global
 */
      icnt = (rank+i)%P;
      if (j = ATL_@(op)AtomicCount(acnts[icnt]))
      {
         j += nL + b*icnt + Mmin(icnt,extra);
         break;
      }
   }
//fprintf(stderr, "%d: j=%d, icnt=%d, b=%d P=%d, e=%d\n", rank, j, icnt, b, P, extra);
@skip      j = (b) ? ((j-1)/b)*P*b + icnt*b + (j-1)%b + 1 : icnt+1;
   return(j);
}
@ROUT ATL_FreeAtomicCount_mut
#include <stdlib.h>
#ifdef ATL_OMP_THREADS
   #include <omp.h>
#else
   #include <pthread.h>
#endif
#include "atlas_misc.h"
void ATL_FreeAtomicCount(void *vp)
{
   char *cp=vp;

#ifdef ATL_OMP_THREADS
   omp_destroy_lock((omp_lock_t*)(cp+2*sizeof(int)+128));
#else
   ATL_assert(!pthread_mutex_destroy((pthread_mutex_t*)(cp+2*sizeof(int)+128)));
#endif
   free(vp);
}
@ROUT ATL_ResetAtomicCount_mut ATL_DecAtomicCount_mut
#ifdef ATL_OMP_THREADS
   #include <omp.h>
#else
   #include <pthread.h>
#endif
@ROUT ATL_DecAtomicCount_mut
int ATL_DecAtomicCount(void *vp)
@ROUT ATL_ResetAtomicCount_mut
int ATL_ResetAtomicCount(void *vp, int cnt)
@ROUT ATL_ResetAtomicCount_mut ATL_DecAtomicCount_mut
{
   char *cp=vp;
   #ifdef ATL_OMP_THREADS
      omp_lock_t *mp;
   #else
      pthread_mutex_t *mp;
   #endif
   int *cntp;
   int iret;

   cntp = (int*)(cp+128);
   #ifdef ATL_OMP_THREADS
      mp = (omp_lock_t*)(cntp+2);
      omp_set_lock(mp);
   #else
      mp = (pthread_mutex_t*)(cntp+2);
      pthread_mutex_lock(mp);
   #endif
   iret = *cntp;
@ROUT ATL_DecAtomicCount_mut   `   if (iret) (*cntp)--;`
@ROUT ATL_ResetAtomicCount_mut `   *cntp = cnt;`
   #ifdef ATL_OMP_THREADS
      omp_unset_lock(mp);
   #else
      pthread_mutex_unlock(mp);
   #endif
   return(iret);
}
@ROUT ATL_SetAtomicCount_arch
#include "atlas_misc.h"

void *ATL_SetAtomicCount(int cnt)
{
   int *ip;

   ip = malloc(260); /* make false sharing unlikely by */
   ATL_assert(ip);   /* putting a 128 byte guard on */
   ip[32] = cnt;     /* both sides of counter */
   return((void*)ip);
}
@ROUT ATL_FreeAtomicCount_arch
#include <stdlib.h>
void ATL_FreeAtomicCount(void *vp)
{
   free(vp);   /* could just do #define ATL_FreeAtomicCount free */
}              /* but do this so compiles same as _mut version */
@ROUT ATL_GetAtomicCount
int ATL_GetAtomicCount(void *vp)
{
   volatile int *ip = vp;
   return(ip[32]);
}
@ROUT ATL_ResetAtomicCount_ia32 ATL_ResetAtomicCount_amd64 ATL_ResetAtomicCount_win64
#include "atlas_asm.h"
/*
@ROUT ATL_ResetAtomicCount_amd64
#ifdef ATL_GAS_WOW64
   #define vp  %rcx
   #define cnt %rdx
   #define ecnt %edx
#else
   #define vp  %rdi
   #define cnt %rsi
   #define ecnt %esi
#endif
 * rax                       rdi         rsi
@ROUT ATL_ResetAtomicCount_win64
 * rax                       rcx         rdx
@ROUT ATL_ResetAtomicCount_ia32 ATL_ResetAtomicCount_amd64 ATL_ResetAtomicCount_win64
 * int ATL_ResetAtomicCount(void *vp, int cnt)
 * Sets vp's acnt=cnt.
 * RETURNS: acnt before the reset
 */
.text
.global ATL_asmdecor(ATL_ResetAtomicCount)
ATL_asmdecor(ATL_ResetAtomicCount):
@ROUT ATL_ResetAtomicCount_ia32
   @define mm @%edx@
   @define ct @%ecx@
   movl 4(%esp), @(mm)
   movl 8(%esp), @(ct)
@ROUT ATL_ResetAtomicCount_amd64
   @define mm @vp@
   @define ct @ecnt@
@ROUT ATL_ResetAtomicCount_win64
   @define mm @%rcx@
   @define ct @%edx@
@ROUT ATL_ResetAtomicCount_ia32 ATL_ResetAtomicCount_amd64 ATL_ResetAtomicCount_win64
   sub $-128, @(mm)            /* skip false sharing guard zone */
   ATOMIC_LOOP:
      movl (@(mm)), %eax       /* read acnt from memory */
      lock                    /* make cmpxchg atomic */
      cmpxchg @(ct), (@(mm))   /* put cnt in mem if mem still == acnt in eax */
      je DONE                 /* ZF set if cmpxchg wrote to mem */
   jmp ATOMIC_LOOP            /* ZF=0 means cmpxch failed, try again */
DONE:
   ret
@ROUT ATL_DecAtomicCount_amd64 ATL_DecAtomicCount_win64
#ifdef ATL_GAS_WOW64
   #define vp %rcx
   #define cnt %edx
#else
   #define vp %rdi
   #define cnt %ecx
#endif
@ROUT ATL_DecAtomicCount_ia32
#define cnt %ecx
@ROUT ATL_DecAtomicCount_amd64 ATL_DecAtomicCount_ia32 ATL_DecAtomicCount_win64
#include "atlas_asm.h"
/* rax                  %rdi/rcx/4  */
/* int ATL_DecAtomicCount(void *vp) */
.text
.global ATL_asmdecor(ATL_DecAtomicCount)
ATL_asmdecor(ATL_DecAtomicCount):
@ROUT ATL_DecAtomicCount_ia32
   movl 4(%esp), %edx
   @define mm @%edx@
@ROUT ATL_DecAtomicCount_amd64 
   @define mm @vp@
@ROUT ATL_DecAtomicCount_win64 
   @define mm @%rdx@
   movq %rcx, %rdx
@ROUT ATL_DecAtomicCount_amd64 ATL_DecAtomicCount_ia32 ATL_DecAtomicCount_win64
   sub $-128, @(mm)            /* skip false sharing guard zone */
   ATOMIC_LOOP:
      movl (@(mm)), %eax       /* read cnt from memory */
      movl %eax, cnt        /* cnt = count */
      subl $1, cnt          /* cnt = count-1 */
      jl ZERO_RET           /* return 0 if count already below 1 */
      lock                  /* make cmpxchg atomic */
      cmpxchg cnt, (@(mm))     /* put cnt-1 in mem if mem still == cnt in eax */
      je DONE               /* ZF set if cmpxchg wrote to mem */
   jmp ATOMIC_LOOP          /* ZF=0 means cmpxch failed, try again */

ZERO_RET:
@ROUT ATL_DecAtomicCount_amd64 ATL_DecAtomicCount_win64 `   xor %rax, %rax`
@ROUT ATL_DecAtomicCount_ia32  `   xor %eax, %eax`
@skip   movl %eax, (@(mm))  /* safety to ensure no roll from neg back to pos */
DONE:
   ret
@ROUT ATL_ResetAtomicCount_ppc
   @define op @Reset@
@ROUT ATL_DecAtomicCount_ppc
   @define op @Dec@
@ROUT ATL_DecAtomicCount_ppc ATL_ResetAtomicCount_ppc
#include "atlas_asm.h"
.text
#ifdef ATL_AS_OSX_PPC
   .globl _ATL_@(op)AtomicCount
   _ATL_@(op)AtomicCount:
#else
   #if defined(ATL_USE64BITS) && _CALL_ELF != 2
/*
 *      Official Program Descripter section, seg fault w/o it on Linux/PPC64
 */
        .section        ".opd","aw"
        .align 2
	.globl  ATL_USERMM
        .align  3
ATL_@(op)AtomicCount:
        .quad   Mjoin(.,ATL_@(op)AtomicCount),.TOC.@tocbase,0
        .previous
        .type   Mjoin(.,ATL_@(op)AtomicCount),@function
        .text
	.globl  Mjoin(.,ATL_@(op)AtomicCount)
.ATL_@(op)AtomicCount:
   #else
	.globl  ATL_@(op)AtomicCount
ATL_@(op)AtomicCount:
   #endif
#endif
@ROUT ATL_ResetAtomicCount_ppc
/* r3                                 r3       r4 */
/* int int ATL_ResetAtomicCount(void *vp, int cnt) */
@ROUT ATL_DecAtomicCount_ppc
#error "Code is not reliable on PPC, don't know why"
/* r3                           r3  */
/* int ATL_DecAtomicCount(void *vp) */
@ROUT ATL_DecAtomicCount_ppc ATL_ResetAtomicCount_ppc
RETRY:
   lwarx r5, 0, r3    /* Read int from mem, place reservation */
@ROUT ATL_DecAtomicCount_ppc
   addi  r5, r5, -1   /* decrement value */
   stwcx. r5, 0, r3   /* attempt to store decremented value back to mem */
@ROUT ATL_ResetAtomicCount_ppc
   stwcx. r4, 0, r3   /* attempt to store new value back to mem */
@ROUT ATL_DecAtomicCount_ppc ATL_ResetAtomicCount_ppc
   bne-  RETRY        /* If store failed, retry */
   mr r3, r5
   blr
@ROUT ATL_DecAtomicCount_sparc ATL_ResetAtomicCount_sparc @\
      ATL_DecAtomicCount_mips ATL_ResetAtomicCount_mips
#error "not implemented"
@ROUT ATL_thread_launch
#include "atlas_misc.h"
#include "atlas_threads.h"

/*
 * These redefinitions allow us to try various launch structures
 */
#ifdef ATL_TUNE_LIN
   #ifdef ATL_tlaunch
      #undef ATL_tlaunch
   #endif
   #ifdef ATL_NOAFFINITY
      #define ATL_tlaunch ATL_lin0tlaunch_noaff
      #define ATL_thread_launch ATL_tllin_noaff
      #define ATL_thread_start ATL_thread_start_noaff
   #else
      #define ATL_tlaunch ATL_lin0tlaunch
      #define ATL_thread_launch ATL_tllin
   #endif
#elif defined(ATL_TUNE_LG2)
   #ifdef ATL_tlaunch
      #undef ATL_tlaunch
   #endif
   #ifdef ATL_NOAFFINITY
      #define ATL_tlaunch ATL_log2tlaunch_noaff
      #define ATL_thread_launch ATL_tllg2_noaff
      #define ATL_thread_start ATL_thread_start_noaff
   #else
      #define ATL_tlaunch ATL_log2tlaunch
      #define ATL_thread_launch ATL_tllg2
   #endif
#elif defined(ATL_TUNE_DYN)
   #ifdef ATL_tlaunch
      #undef ATL_tlaunch
   #endif
   #ifdef ATL_NOAFFINITY
      #define ATL_tlaunch ATL_dyntlaunch_noaff
      #define ATL_thread_launch ATL_tldyn_noaff
      #define ATL_thread_start ATL_thread_start_noaff
   #else
      #define ATL_tlaunch ATL_dyntlaunch
      #define ATL_thread_launch ATL_tldyn
   #endif
#endif
void *ATL_tlaunch(void *vp); /* _noaff versions not protoed in threads.h */

/*
 * This routine can be called by any threaded routine to autoset all needed
 * data structures for ATLAS to launch the threads for a parallel operation
 */
void ATL_thread_launch
(
   void *opstruct,              /* P-len struct given to each launched thread */
   int opstructstride,          /* sizeof(opstruct) */
   void *OpStructIsInit,        /* NULL, or test to see if thread is spawned */
   void *DoWork,                /* computation to call (rout launched) */
   void *CombineOpStructs       /* NULL, or combine func */
)
{
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   void *vp;
   int i;

   ls.opstruct = opstruct;
   ls.opstructstride = opstructstride;
   ls.CombineOpStructs = CombineOpStructs;
   ls.OpStructIsInit = OpStructIsInit;
   ls.DoWork = DoWork;
   ls.rank2thr = tp;
   #ifdef ATL_TUNE_DYN
      ls.acounts = &vp;
      ls.acounts[0] = ATL_SetGlobalAtomicCount(ATL_NTHREADS>>1, 
                                               ATL_NTHREADS-1, 0);
      ls.chkin = calloc(ATL_NTHREADS, sizeof(int));
      ATL_assert(ls.chkin);
   #endif

   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
   #ifdef ATL_TUNE_DYN
       ATL_FreeGlobalAtomicCount(ls.acounts[0]);
       free((void*)ls.chkin);
   #endif
}
@ROUT ATL_thread_start
#ifndef ATL_NOAFFINITY
   #include "atlas_taffinity.h"  /* include this file first! */
#elif defined(ATL_TUNING)
   #define ATL_thread_start ATL_thread_start_noaff
#endif
#include "atlas_misc.h"
#include "atlas_threads.h"
int ATL_thread_start(ATL_thread_t *thr, int proc, int JOINABLE,
                     void *(*rout)(void*), void *arg)
/*
 * Creates a thread that will run only on processor proc.
 * RETURNS: 0 on success, non-zero on error
 * NOTE: present implementation dies on error, so 0 is always returned.
 */
{
#ifdef ATL_WINTHREADS
   #ifdef ATL_WIN32THREADS
      DWORD thrID;
   #else
      unsigned int thrID;
   #endif
      
   #ifdef ATL_NOAFFINITY
      thr->thrH = CreateThread(NULL, 0, rout, arg, 0, &thrID);
   @beginskip
      #ifdef ATL_WIN32THREADS
         thr->thrH = CreateThread(NULL, 0, rout, arg, 0, &thrID);
      #else
         thr->thrH = (HANDLE)_beginthreadex(NULL, 0, rout, arg, 0, &thrID);
      #endif
   @endskip
      ATL_assert(thr->thrH);
   #else
      thr->rank = proc;
      thr->thrH = CreateThread(NULL, 0, rout, arg, CREATE_SUSPENDED, &thrID);
   @beginskip
      #ifdef ATL_WIN32THREADS
         thr->thrH = CreateThread(NULL, 0, rout, arg, CREATE_SUSPENDED, &thrID);
      #else
         thr->thrH = (HANDLE)_beginthreadex(NULL, 0, rout, arg, 
                                            CREATE_SUSPENDED, &thrID);
      #endif
   @endskip
      ATL_assert(thr->thrH);
      #ifdef ATL_RANK_IS_PROCESSORID
         ATL_assert(SetThreadAffinityMask(thr->thrH, (1<<proc)));
      #else
         ATL_assert(SetThreadAffinityMask(thr->thrH, 
                    (1<<ATL_affinityIDs[proc%ATL_AFF_NUMID])));
      #endif
      ATL_assert(ResumeThread(thr->thrH) == 1);
   #endif
#elif defined(ATL_OMP_THREADS)
   fprintf(stderr, "Should not call thread_start when using OpenMP!");
   ATL_assert(0);
#elif 0 && defined(ATL_OS_OSX)  /* unchecked special OSX code */
/* http://developer.apple.com/library/mac/#releasenotes/Performance/RN-AffinityAPI/_index.html */
   pthread_attr_t attr;
   #define ATL_OSX_AFF_SETS 2       /* should be probed for */
   thread_affinity_policy ap;

   ap.affinity_tag = proc % ATL_OSX_AFF_SETS;
   ATL_assert(!pthread_attr_init(&attr));
   if (JOINABLE)
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE));
   else
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */

   ATL_assert(!pthread_create(&thr->thrH, &attr, rout, arg));
   ATL_assert(!thread_policy_set(thr->thrH, THREAD_AFFINITY_POLICY, 
                                 (integer_t*)&ap, 
                                 THREAD_AFFINITY_POLICY_COUNT));
   ATL_assert(!pthread_attr_destroy(&attr));
#else
   pthread_attr_t attr;
   #ifndef ATL_NOAFFINITY
      #if defined(ATL_PAFF_SETAFFNP) || defined(ATL_PAFF_SCHED)
         cpu_set_t cpuset;
      #elif defined(ATL_PAFF_PLPA)
         plpa_cpu_set_t cpuset;
      #elif defined(ATL_PAFF_CPUSET) /* untried FreeBSD code */
         cpuset_t mycpuset;
      #endif
      #ifdef ATL_RANK_IS_PROCESSORID
         const int affID = proc;
      #else
         const int affID = ATL_affinityIDs[proc%ATL_AFF_NUMID];
      #endif
      #ifdef ATL_PAFF_SELF
         thr->paff_set = 0;  /* affinity must be set by created thread */
      #endif
   #endif
   thr->rank = proc;
   ATL_assert(!pthread_attr_init(&attr));
   if (JOINABLE)
   {
      #ifdef IBM_PT_ERROR
         ATL_assert(!pthread_attr_setdetachstate(&attr, 
                                                 PTHREAD_CREATE_UNDETACHED));
      #else
         ATL_assert(!pthread_attr_setdetachstate(&attr, 
                                                 PTHREAD_CREATE_JOINABLE));
      #endif
   }
   else
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */
   #ifdef ATL_PAFF_SETAFFNP
      CPU_ZERO(&cpuset);
      CPU_SET(affID, &cpuset);
      ATL_assert(!pthread_attr_setaffinity_np(&attr, sizeof(cpuset), &cpuset));
   #elif defined(ATL_PAFF_SETPROCNP)
      ATL_assert(!pthread_attr_setprocessor_np(&attr, (pthread_spu_t)affID, 
                                               PTHREAD_BIND_FORCED_NP)); 
   #endif
   ATL_assert(!pthread_create(&thr->thrH, &attr, rout, arg));
   #if defined(ATL_PAFF_PBIND)
      ATL_assert(!processor_bind(P_LWPID, thr->thrH, affID, NULL));
      thr->paff_set = 0;  /* affinity set by spawner */
   #elif defined(ATL_PAFF_BINDP)
      ATL_assert(!bindprocessor(BINDTHREAD, thr->thrH, bindID));
      thr->paff_set = 0;  /* affinity set by spawner */
   #elif defined(ATL_PAFF_CPUSET)  /* untried FreeBSD code */
      CPU_ZERO(&mycpuset);         /* no manpage, so guess works like linux */
      CPU_SET(bindID, &mycpuset);
      if (!cpuset_setaffinity(CPU_LEVEL_WHICH, CPU_WHICH_TID, thr->thrH,
                              sizeof(mycpuset), &mycpuset));
         thr->paff_set = 0;  /* affinity set by spawner */
   #endif
   ATL_assert(!pthread_attr_destroy(&attr));
#endif
   return(0);
}
@ROUT ATL_thread_join
#include "atlas_misc.h"
#include "atlas_threads.h"
int ATL_thread_join(ATL_thread_t *thr)   /* waits on completion of thread */
{
#ifdef ATL_WINTHREADS
   ATL_assert(WaitForSingleObject(thr->thrH, INFINITE) != WAIT_FAILED);
   ATL_assert(CloseHandle(thr->thrH));
#elif defined(ATL_OMP_THREADS)
   fprintf(stderr, "Cannot call thread_join using OpenMP!!\n");
   ATL_assert(0);  /* should never enter this rout when using OMP */
#else
   ATL_assert(!pthread_join(thr->thrH, NULL));
#endif
   return(0);
}
@ROUT ATL_thread_exit
#include "atlas_misc.h"
#include "atlas_threads.h"
void ATL_thread_exit(void *retval)
{
#ifdef ATL_WINTHREADS
   ExitThread((DWORD)(retval));
#elif defined(ATL_OMP_THREADS)
   fprintf(stderr, "Cannot call thread_exit using OpenMP!!\n");
   ATL_assert(0);  /* should never enter this rout when using OMP */
#else
   pthread_exit(retval);
#endif
}
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch ATL_goparallel
#ifndef ATL_NOAFFINITY
   #include "atlas_taffinity.h"  /* include this file first! */
#endif
#include "atlas_misc.h"
#include "atlas_threads.h"

@ROUT ATL_goparallel
#if !defined(ATL_NOAFFINITY) && defined(ATL_PAFF_SELF) && defined(ATL_USEOPENMP)
static int ATL_setmyaffinity()
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch
#if !defined(ATL_NOAFFINITY) && defined(ATL_PAFF_SELF)
static int ATL_setmyaffinity(ATL_thread_t *me)
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch ATL_goparallel
/*
 * Attempts to sets the affinity of an already-running thread.  The 
 * aff_set flag is set to true whether we succeed or not (no point in
 * trying multiple times).
 * RETURNS: 0 on success, non-zero error code on error
 */
{
@ROUT ATL_goparallel
   int bindID;
   bindID = omp_get_thread_num();
   #ifdef ATL_RANK_IS_PROCESSORID
      bindID = bindID % ATL_AFF_NUMID;
   #else
      bindID = ATL_affinityIDs[bindID%ATL_AFF_NUMID];
   #endif
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch
   #ifdef ATL_RANK_IS_PROCESSORID
      const int bindID = me->rank % ATL_AFF_NUMID;
   #else
      const int bindID = ATL_affinityIDs[me->rank%ATL_AFF_NUMID];
   #endif
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch ATL_goparallel
#ifdef ATL_PAFF_PLPA
   plpa_cpu_set_t cpuset;
   PLPA_CPU_ZERO(&cpuset);
   PLPA_CPU_SET(bindID, &cpuset);
   if (me->paff_set)
      return(0);
   me->paff_set = 1;
   return(plpa_sched_setaffinity((pid_t)0, sizeof(cpuset), &cpuset));
#elif defined(ATL_PAFF_PBIND)
   return(processor_bind(P_LWPID, P_MYID, bindID, NULL));
#elif defined(ATL_PAFF_SCHED)
   cpu_set_t cpuset;
   CPU_ZERO(&cpuset);
   CPU_SET(bindID, &cpuset);
   if (me->paff_set)
      return(0);
   me->paff_set = 1;
   return(sched_setaffinity(0, sizeof(cpuset), &cpuset));
#elif defined (ATL_PAFF_RUNON)
   if (me->paff_set)
      return(0);
   me->paff_set = 1;
   return(pthread_setrunon_np(bindID));
#elif defined(ATL_PAFF_BINDP)
   if (me->paff_set)
      return(0);
   me->paff_set = 1;
   return(bindprocessor(BINDTHREAD, thread_self(), bindID));
#elif defined(ATL_PAFF_CPUSET)  /* untried FreeBSD code */
   cpuset_t mycpuset;
   CPU_ZERO(&mycpuset);         /* no manpage, so guess works like linux */
   CPU_SET(bindID, &mycpuset);
   if (me->paff_set)
      return(0);
   me->paff_set = 1;
   return(cpuset_setaffinity(CPU_LEVEL_WHICH, CPU_WHICH_TID, -1,
                             sizeof(mycpuset), &mycpuset));
#endif
   return(0);
}
#endif
@ROUT ATL_lin0tlaunch
#ifdef ATL_NOAFFINITY
   #define ATL_tDoWorkWrap ATL_tDoWorkWrap_noaff
#endif
void *ATL_tDoWorkWrap(void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_LAUNCHSTRUCT_t *lp = tp->vp;
   lp->DoWork(lp, tp);
   return(NULL);
}

#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
   #define ATL_lin0tlaunch ATL_lin0tlaunch_noaff
   #define ATL_thread_start ATL_thread_start_noaff
#endif
void *ATL_lin0tlaunch(void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_LAUNCHSTRUCT_t *lp;
   const int P = tp->P;
   int i;
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (!tp->paff_set)
          ATL_setmyaffinity(tp);
   #endif
/*
 * Spawn DoWork to all nodes
 */
   lp = tp->vp;
   for (i=1; i < P; i++)
   {
      ATL_thread_start(tp+i, i, 1, ATL_tDoWorkWrap, tp+i);
   }
/*
 * Thread 0 must also do the operation
 */
   lp->DoWork(lp, tp);
/*
 * Await completion of each task, and do combine (linear!) if requested
 */
   for (i=1; i < P; i++)
   {
      ATL_thread_join(tp+i);
      if (lp->DoComb)  /* do combine if necessary */
         lp->DoComb(lp->opstruct, 0, i);
   }
   return(NULL);
}
@ROUT ATL_dyntlaunch
#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
void *ATL_dyntlaunch_noaff(void *vp)
#else
void *ATL_dyntlaunch(void *vp)
#endif
{
   ATL_thread_t *tp = vp, *btp;
   ATL_LAUNCHSTRUCT_t *lp;
   const int iam = tp->rank, P = tp->P;
   int i, src, dest, nthrP2, mask, abit;
   void *acnt;

   lp = tp->vp;
   acnt = lp->acounts[0];
   btp = tp - iam;
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (!tp->paff_set)
          ATL_setmyaffinity(tp);
   #endif
   dest = ATL_DecGlobalAtomicCount(acnt, iam);
   while(dest)
   {
      dest = tp->P - dest;
      ATL_thread_start(btp+dest, dest, 0, ATL_dyntlaunch, btp+dest);
      dest = ATL_DecGlobalAtomicCount(acnt, iam);
   }
/*
 * Do the operation
 */
   lp->DoWork(lp, tp);
/*
 * Do combine in minimum spanning tree, combining results as required
 */
   for (i=0; (1<<i) < P; i++);
   nthrP2 = i;
   mask = 0;
   for (i=0; i < nthrP2; i++)
   {
      if (!(iam & mask))
      {
         abit = (1<<i);
         if (!(iam & abit))
         {
            src = iam ^ abit;
            if (src < P)
            {
               while (lp->chkin[src] != ATL_CHK_DONE_OP)
                  ATL_POLL;
               if (lp->DoComb)
                  lp->DoComb(lp->opstruct, iam, src);
            }
         }
         else
         {
            lp->chkin[iam] = ATL_CHK_DONE_OP;
            ATL_thread_exit(NULL);
         }
      }
      mask |= abit;
   }
@beginskip
/* 
 * This code does not work, because combine routs assume combine is always
 * right-to-left
 */
/*
 * Node 0 awaits completion of each task, and do combine (linear!) if requested
 * Later on, replace this code with pair-wise summup using AtomicCtrs=2
 */
   if (iam == 0)
   {
      int ndone=1;
      while (ndone < P)
      {
/*
 *       Find an uncombined completed result
 */
         for (i=1; i < P && lp->chkin[i] != ATL_CHK_DONE_OP; i++);
         if (i == P)
         {
            ATL_POLL;
            continue;
         }
/*
 *       Do the combine if necessary, and mark that thread as completely done
 */
         if (lp->DoComb)  /* do combine if necessary */
            lp->DoComb(lp->opstruct, iam, i);
         ndone++;
         lp->chkin[i] = ATL_CHK_DONE_COMB;
      }
   }
@endskip
   return(NULL);
}
@ROUT ATL_ranktlaunch_noaff
static void *ATL_GoToWork(ATL_thread_t *tp, ATL_LAUNCHSTRUCT_t *lp, int iam)
{
/*
 * Do the operation
 */
   lp->DoWork(lp, lp->opstruct+lp->opstructstride*iam);
/*
 * Node 0 awaits completion of each task, and do combine (linear!) if requested
 */
   if (iam == 0)
   {
      for (i=1; i < ATL_NTHREADS; i++)
      {
         if (!lp->OpStructIsInit || 
             lp->OpStructIsInit(lp->opstruct+i*lp->opstructstride))
         {
            ATL_thread_join(tp+i);
            if (lp->CombineOpStructs)  /* do combine if necessary */
               lp->CombineOpStructs(lp->opstruct,
                                    lp->opstruct+lp->opstructstride*i);
         }
      }
   }
   return(NULL);
}
static unsigned int ATL_coreID(void)
{

  int myRetn=-1;
  __asm__ __volatile__ ("\n"
    "movl $1, %%eax\n"
    "cpuid\n"
    "shrl $24, %%ebx\n"
    "movl %%ebx, %0\n"
    : "=m" (myRetn)
    :
    :"%rax", "%rbx"
    );
  
  return(myRetn);
} // end *** ATL_coreId ***
static void ATL_CreateThread(void *vp)
{
   pthread_attr_t attr;
   pthread_t pt;
   void *ATL_ranktlaunch_noaff(void *vp);

   ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */
   ATL_assert(!pthread_create(&pt, &attr, ATL_ranktlaunch_noaff, vp));
}

static void *ATL_KeepLaunching(ATL_thread_t tp, ATL_LAUNCHSTRUCT_t *lp)
{
   void *ranklock = lp->acounts[0];
   int i;
   do
   {
/*
 *    If all cores have gotten a thread, or if any thread has finished the
 *    problem, then stop launching and exit
 */
      if (lp->acounts[1])
         return(NULL);
      for (i=0; i < ATL_NTHREADS; i++)
      {
         if (lp->impdone[i] == -1)
            return(NULL);
      }
      iam = ATL_coreID();      /* may have changed if I've slept */
      if (!lp->rank2thr[iam])
      {
         int SUCCESS=0;
         ATL_mutex_lock(ranklock);
         if (!lp->rank2thr[iam])
         {
            int n;
            tp->rank = iam;
            lp->rank2thr[iam].thrH = pthread_self();
            for (i=0; i < ATL_NTHREADS && !lp->rank2thr[i].thrH; i++);
            if (i == ATL_NTHREADS)         /* if threads on all cores */
               lp->acounts[1] = (void*)1;  /* set flag saying launch complete */
            SUCCESS = 1;
         }
         ATL_mutex_unlock(ranklock);
         if (SUCCESS)
            return(ATL_GoToWork(tp, lp, iam));
      }
/*
 *    If there are still threads that need to run, spawn a new one, and go
 *    to sleep to yield to computation
 */
      ATL_CreateThread(tp);
      sched_yield();  /* go to sleep if no success */
   }
   while(1);
   return(NULL);
}
/*
 * This routine is for launching on platforms w/o affinity that have some
 * way for a running process to establish what core they are on.  Threads
 * are continually launched until there is one on each core, or the first
 * core runs out of work, whichever comes first.
 * Any process that is launched with this technique must not barrier,
 * since the number of cores is unknown, and the will certainly enter
 * the program at very different times.
 * NOTE: This routine calls pthreads directly, because ATLAS supports only
 *       Windows threads and pthreads, and Windows threads have affinity.
 */
void *ATL_ranktlaunch_noaff(void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_LAUNCHSTRUCT_t *lp=tp->vp;

   return(ATL_KeepLaunching(vp, lp));

@ROUT ATL_log2tlaunch
#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
void *ATL_log2tlaunch_noaff(void *vp)
#else
void *ATL_log2tlaunch(void *vp)
#endif
{
   ATL_thread_t *tp = vp, *btp;
   ATL_LAUNCHSTRUCT_t *lp;
   int i, iam, abit, mask, src, dest, nthrP2;
   const int P=tp->P;

   iam = tp->rank;
   for (i=0; (1<<i) < P; i++);
   nthrP2 = i;
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (!tp->paff_set)
          ATL_setmyaffinity(tp);
   #endif
   btp = tp - iam;
   lp = tp->vp;
   mask = (1<<nthrP2) - 1;   /* no threads are in at beginning */
/*
 * Take log_2(NTHR) steps to do log_2 launch 
 */
   for (i=nthrP2-1; i >= 0; i--)
   {
      abit = (1<<i);
      mask ^= abit;   /* double the # of threads participating */
      if (!(iam & mask))
      {
         if (!(iam & abit))
         {
            dest = iam ^ abit;
            if ( dest < P)
               ATL_thread_start(btp+dest, dest, 1, ATL_log2tlaunch, btp+dest);
         }
      }
   }
   lp->DoWork(lp, tp);   /* do the operation */
/*
 * Join tree back up, combining results as required
 */
   mask = 0;
   for (i=0; i < nthrP2; i++)
   {
      if (!(iam & mask))
      {
         abit = (1<<i);
         if (!(iam & abit))
         {
            src = iam ^ abit;
            if (src < P)
            {
               ATL_thread_join(btp+src);
               if (lp->DoComb)
                  lp->DoComb(lp->opstruct, iam, src);
            }
         }
         else
            ATL_thread_exit(NULL);
      }
      mask |= abit;
   }
   return(NULL);
}
@ROUT ATL_goparallel
#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
   void *ATL_log2tlaunch_noaff(void *vp);
   #define ATL_goparallel ATL_goparallel_noaff
   #define ATL_dyntlaunch ATL_log2tlaunch_noaff
   #define ATL_USE_DYNAMIC 0
#elif defined(ATL_TUNING)
   #if defined(ATL_LAUNCH_LINEAR)
      #define ATL_goparallel ATL_goparallel_lin
      #define ATL_dyntlaunch ATL_lin0tlaunch
      #define ATL_USE_DYNAMIC 0
   #elif defined(ATL_LAUNCH_DYNAMIC)
      #define ATL_goparallel ATL_goparallel_dyn
      #define ATL_USE_DYNAMIC 1
   #else
      #ifdef ATL_LAUNCH_LOG2
         #define ATL_goparallel ATL_goparallel_log2
      #endif
      #define ATL_dyntlaunch ATL_log2tlaunch
      #define ATL_USE_DYNAMIC 0
   #endif
#else
   #define ATL_USE_DYNAMIC 1
#endif
void ATL_goparallel
/*
 * This function is used when you pass a single opstruct to all threads;  
 * In this case, we stash opstruct in launchstruct's vp, and then use the
 * rank array as opstruct during the spawn.  Therefore, these routines
 * should expect to get their problem def from ls.vp, and their rank from
 * the second argument.  The DoWork function is the function that should
 * be called from each thread to do the parallel work.  This function should
 * look like:
 * void DoWork_example(ATL_LAUNCHSTRUCT_t *lp, void *vp)
 * {
 *    ATL_thread_t *tp = vp;
 *    const int myrank = tp->rank;
 *    my_prob_def_t *pd = lp->vp;
 *    ... do work based on info in struct pointed to by lp->vp ...
 * }
 * Your DoWork should perform any needed combine before finishing execution,
 * and any return values can be passed in the problem definition structure
 * that you define.
 */
(
   const unsigned int P, /* # of cores to use */
   void *DoWork,         /* func ptr to work function */
   void *opstruct,       /* structure giving tasks to threads */
   void *DoComb          /* function to combine two opstructs */
)
{
   ATL_thread_t *tp;
   int *chkin;
   void *vp, *lc;
   int i;
   ATL_LAUNCHSTRUCT_t ls;

   ls.OpStructIsInit = NULL;
   ls.DoWork = DoWork;
   ls.DoComb = DoComb;
   ls.opstruct = opstruct;
#ifdef ATL_OMP_THREADS
   tp = malloc(sizeof(ATL_thread_t)*P);
   ATL_assert(tp);
   for (i=0; i < P; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
      tp[i].P = P;
   }
   ls.rank2thr = tp;
   omp_set_num_threads(P);
   #pragma omp parallel
   {
/*
 *    Make sure we got the requested nodes, and set affinity if supported
 */
      ATL_assert(omp_get_num_threads() == P);
      #ifdef ATL_PAFF_SELF
         ATL_setmyaffinity();
      #endif
      i = omp_get_thread_num();
      ls.DoWork(&ls, tp+i);
   }
/*
 * Do combine (linear) if requested
 */
   if (DoComb)
      for (i=1; i < P; i++)
         ls.DoComb(ls.opstruct, 0, i);
#else
   #if ATL_USE_DYNAMIC
      ls.acounts = &lc;
      ls.acounts[0] = ATL_SetGlobalAtomicCount(P>>1, P-1, 0);
      vp = malloc(P*(sizeof(ATL_thread_t)+sizeof(int)) + ATL_Cachelen);
      ATL_assert(vp);
      chkin = vp;
      tp = (ATL_thread_t*)(chkin+P);
      tp = ATL_AlignPtr(tp);
   #else
      vp = malloc(P*(sizeof(ATL_thread_t)) + ATL_Cachelen);
      tp = ATL_AlignPtr(vp);
   #endif
   ls.rank2thr = tp;

   for (i=0; i < P; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
      tp[i].P = P;
      #if ATL_USE_DYNAMIC
         chkin[i] = 0;
      #endif
   }
   #if ATL_USE_DYNAMIC
      ls.chkin = (volatile int*) chkin;
   #endif
   ATL_thread_start(tp, 0, 1, ATL_dyntlaunch, tp);
   ATL_thread_join(tp);
   #if ATL_USE_DYNAMIC
      ATL_FreeGlobalAtomicCount(ls.acounts[0]);
   #endif
   free(vp);
#endif
}
@ROUT ATL_goparallel_prank
#include "atlas_misc.h"
#include "atlas_threads.h"
#if defined(ATL_GAS_x8664) || defined(ATL_GAS_x8632)
   #define ATL_HAS_COREID
static unsigned int ATL_coreID(void)
{

  int myRetn=-1;
  __asm__ __volatile__ ("\n"
    "movl $1, %%eax\n"
    "cpuid\n"
    "shrl $24, %%ebx\n"
    "movl %%ebx, %0\n"
    : "=m" (myRetn)
    :
#if defined(ATL_GAS_x8632)
    :"%eax", "%ebx", "%edx", "%ecx"
#elif defined(ATL_GAS_x8664)
    :"%rax", "%rbx", "%rdx", "%rcx"
#endif
    );
  return(myRetn);
}
#else
   #define ATL_HAS_COREID
   #define _GNU_SOURCE 1
   #define __USE_GNU   1
   #include <sched.h>
   #define ATL_coreID sched_getcpu
#endif
#ifndef ATL_HAS_COREID
void ATL_goparallel_prank
(
   const unsigned int P, /* # of cores to use */
   void *DoWork,         /* func ptr to work function */
   void *opstruct,       /* will be stashed in launchstruct's vp */
   void *DoComb
)
{
   fprintf(stderr, "Hey chief, you are screwed:\n");
   fprintf(stderr, 
      "  someone called goparallel_ptrank wt no way to determine prank!\n");
   exit(-1);
}
#else

typedef struct
{
   volatile int *coreIDs;   /* NTHR-len array providing non-unique coreIDs */
   volatile int *thrrnks;   /* NP-len array of chosen thread ranks */
   void *Tcnt;              /* atomic counter of # of threads launched */
   void *Trankcnt;          /* atomic ctr providing thread rank */
   int NT, NP;              /* # of threads & processors */
   int NLC;                 /* # of local cntrs in global acnts Tcnt/Trankcnt */
   pthread_attr_t attr;     /* attribute for pthread_create */
   ATL_LAUNCHSTRUCT_t *lp;  /* to pretend we've been launched normally */
} ATL_ranklaunch_t;


/*
 * Selects an ID from list A which does not appear in C
 * RETURNS: first unique ID, or -1 if no such ID found
 */
static int GetUniqueID
(
   int Na,   /* number of accepted unique IDs in U */
   int *A,   /* list of accepted unique IDs found so far */
   int Nc,   /* number of candidates left */
   int *C    /* non-unique candidates */
)
{
   int ic, ia;
   for (ic=0; ic < Nc; ic++)
   {
      for (ia=0; ia < Na && A[ia] != C[ic]; ia++);
      if (ia == Na)  /* found unique one */
         return(ic);
   }
   return(-1);
}

void *ATL_DoRankLaunch(void *vp)
{
   ATL_ranklaunch_t *rl = vp;
   ATL_LAUNCHSTRUCT_t *lp = rl->lp;
   const int T = rl->NT, P = rl->NP;
   int trank;   /* thread rank between 0 and NT-1 */
   int prank;   /* processor rank between 0 and NP-1 */
   int i, coreID;
   pthread_t pt;
/*
 * Cooperate with master to launch NT threads
 */
   coreID = ATL_coreID();
   trank = coreID % rl->NLC;
   #ifdef ATL_GLOBAL
   while(ATL_DecGlobalAtomicCount(rl->Tcnt, trank))
   #else
   while(ATL_DecAtomicCount(rl->Tcnt))
   #endif
      pthread_create(&pt, &rl->attr, ATL_DoRankLaunch, rl);
/*
 * Get my coreID, and tell master I'm alive by writing it to coreID array
 */
   #ifdef ATL_GLOBAL
      trank = T - ATL_DecGlobalAtomicCount(rl->Trankcnt, trank);
   #else
      trank = T - ATL_DecAtomicCount(rl->Trankcnt);
   #endif
   rl->coreIDs[trank] = coreID;
/*
 * Wait on master to signal ranking have been established
 */
   while(rl->coreIDs[0] == -1)
      ATL_thread_yield();
/* 
 * See if my core has been selected for survival
 */
   for (i=0; i < P; i++)
      if (rl->thrrnks[i] == trank)
         break;
/*
 * If I'm not in worker list, signal completion by writing -2 to coreID array,
 * and quit
 */
   if (i == P)
   {
      rl->coreIDs[trank] = -2;  /* signal thread has completed */
      pthread_exit(NULL);
   }
/* 
 * i is actually now my processor rank, fill my thread info in
 */
   prank = i;
/*   lp->rank2thr[prank].thrH = pthread_self(); */ /* don't need this */
   lp->rank2thr[prank].rank = prank;

   lp->DoWork(lp, lp->rank2thr+prank);  /* do work */

   rl->coreIDs[trank] = -2 ;    /* signal thread completion for master */
}

/*
 * This function is used when we don't have affinity, but do have some way
 * to determine the coreID, which must be a unique non-negative int.
 * It will launch 4*P threads in a detached state; all those threads that
 * start on unique cores will work on the problem, as will some on non-unique
 * cores that are necessary to get P threads out of the 4P created
 */
void ATL_goparallel_prank
(
   const unsigned int P, /* # of cores to use */
   void *DoWork,         /* func ptr to work function */
   void *opstruct,       /* will be stashed in launchstruct's vp */
   void *DoComb
)
{
   ATL_LAUNCHSTRUCT_t ls;
   ATL_ranklaunch_t rl;
   int T, t, i, j, prank, nunique, coreID;
   int *uids;    /* unique coreIDs */
   volatile int *coreIDs;
   pthread_t pt;
   pthread_attr_t *attr;
   void *vp;

   attr = &rl.attr;
   ls.DoWork = DoWork;
   ls.vp = opstruct;
   ls.DoComb = NULL;
@skip   ls.CombineOpStructs = NULL;
@skip   ls.opstructstride = 0;
   ls.chkin = NULL;
   ls.acounts = NULL;
   T = (P >= 8) ? P<<2 : P+P;
   rl.NT = T;
   rl.NP = P;
   #ifdef ATL_GLOBAL
      rl.NLC = P;
      rl.Tcnt = ATL_SetGlobalAtomicCount(rl.NLC, T-1, 0);
      rl.Trankcnt = ATL_SetGlobalAtomicCount(rl.NLC, T-1, 0);
   #else
      rl.NLC = T >> 2;
      rl.Tcnt = ATL_SetAtomicCount(T-1);
      rl.Trankcnt = ATL_SetAtomicCount(T-1);
   #endif
   rl.lp = &ls;
   uids = malloc(ATL_Cachelen+sizeof(int)*(T+P+P)+sizeof(ATL_thread_t)*P);
   ATL_assert(uids);
   coreIDs = rl.coreIDs = (volatile int*) (uids + P);
   rl.thrrnks = (volatile int*) (rl.coreIDs + T);
   vp = (void*) (rl.thrrnks + P);
   ls.rank2thr = ATL_AlignPtr(vp);
/*
 * Initialize attribute: detached with system scope 
 */
   ATL_assert(!pthread_attr_init(attr));
   ATL_assert(!pthread_attr_setdetachstate(attr,PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */
/*
 * Initialize rank arrays with -1; negative #s are codes, -1 means not init,
 * -2: started and then died.
 */
   for (i=0; i < P; i++)
      coreIDs[i] = rl.thrrnks[i] = -1;
   for (i=P; i < T; i++)
      coreIDs[i] = -1;
/*
 * Cooperate with worker threads to spawn all T threads
 */
   #ifdef ATL_GLOBAL
      while(ATL_DecGlobalAtomicCount(rl.Tcnt, 0))
   #else
      while(ATL_DecAtomicCount(rl.Tcnt))
   #endif
         pthread_create(&pt, attr, ATL_DoRankLaunch, &rl);
/*
 * Wait for all created threads to checkin; worker threads will all write
 * their coreID to their entry in the coreIDs array.  Their index in this array
 * is therefore their thread rank, which everyone agrees on due to using
 * the atomic counter.
 */
   coreID = ATL_coreID();  /* get my core ID */
   for (i=1; i < T; i++)
      while(coreIDs[i] == -1)
         ATL_thread_yield();
/*
 * All workers are spinning on coreIDs[0] awaiting my OK, so it is safe
 * to build all processor/thread ranking arrays 
 */
   uids[0] = coreID;
   rl.thrrnks[0] = 0;   /* master is always first worker */
   for (i=1; i < P; i++)
   {
      j = GetUniqueID(i, uids, T-1, (int*)coreIDs+1) + 1;
      if (!j)
        break;
      rl.thrrnks[i] = j;
      uids[i] = coreIDs[j];
   }
   nunique = i;
/*
 * We didn't get P unique coreIDs, so choose some coreIDs to get extra threads
 * Try to map all extra threads to same cores as much as possible, to make
 * it more likely OS gets off its ass and reschedules; also, since we are
 * using dynamically scheduled ops, only a few processors will be running
 * at low speeds.
 */
   while (i < P)
   {
      int k;
      for (k=0; k < i; k++)
      {
         for (j=0; j < T; j++)
            if (coreIDs[j] == uids[k])
               break;
         if (j < T)
         {
            rl.thrrnks[i] = j;
            uids[i] = coreIDs[j];
            i++;
            break;
         }
      }
   }
/*
 * Signal to workers that thread mapping is complete, then do my portion of work
 */
   coreIDs[0] = coreID;
   ls.rank2thr[0].rank = 0;
   ls.DoWork(&ls, ls.rank2thr);  /* do work */
/*
 * I could have freed these resources after first checkin, but have delayed
 * until now to avoid possible context switch due to system call.  Free some
 * resources I'm no longer using
 */
   ATL_assert(!pthread_attr_destroy(attr));  /* spawning complete, release */
   #ifdef ATL_GLOBAL
      ATL_FreeGlobalAtomicCount(rl.Tcnt);
      ATL_FreeGlobalAtomicCount(rl.Trankcnt);
   #else
      ATL_FreeAtomicCount(rl.Tcnt);
      ATL_FreeAtomicCount(rl.Trankcnt);
   #endif
/*
 * Wait for all threads to complete before returning
 */
   if (nunique < P)
      printf("Node 0 awaits completion on %d unique cores; P=%d\n", nunique, P);
   for (i=1; i < T; i++)
      while(coreIDs[i] != -2)
        ATL_thread_yield();
   free(uids);
}
#endif
@ROUT ATL_Xtgemm
#include "atlas_misc.h"
@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
/* 
 * =========================================================================
 * This file contains support routines for TGEMM that are not type-dependent
 * =========================================================================
 */
@ROUT ATL_thrdecompMM ATL_Xtgemm
int ATL_thrdecompMM_rMN
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
/*
 * This routine recursively splits the M & N dimensions over P processors
 */
{
   int pR, pL, rL, rR, np, eltsh;
   ATL_INT nblksL, nblksR, j;
   size_t i;
   double d;

/*
 * Choose to split either M or N.  Want M < N always, so require
 * N to be twice as big before splitting (or be out of M blocks)
 */
   if (P > 1 && Nblks > 1 && (Mblks < 2 || Nblks >= Mblks+Mblks))
   {
      eltsh = ptmms[indx].eltsh;
      pR = P>>1;    /* on right, take P/2 threads */
      pL = P - pR;  /* on left, take remaining threads */
      d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */
      #ifdef DEBUG
         fprintf(stderr, "Cut N\n");
      #endif
      nblksL = (d * Nblks);
      nblksR = Nblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = nr;
      }
      else
      {
         rL = nr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].nb+rL) << eltsh;
      np = ATL_thrdecompMM_rMN(ptmms, TA, TB, Mblks, mr, nblksL, rL, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMN(ptmms, TA, TB, Mblks, mr, nblksR, rR, Kblks, kr,
                                A,  lda, (TB == AtlasNoTrans) ? 
                                MindxT(B,i*ldb) : MindxT(B,i), ldb, 
                                MindxT(C,i*ldc), ldc, pR, indx+pL, COPYC);
      return(np);
   }
/*
 * If we have failed to split N, split M if possible
 */
   if (P > 1 && Mblks > 1)
   {
      eltsh = ptmms[indx].eltsh;
      pR = P>>1;    /* on right, take P/2 threads */
      pL = P - pR;  /* on left, take remaining threads */
      d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */
      #ifdef DEBUG
         fprintf(stderr, "Cut M\n");
      #endif
      nblksL = (d * Mblks);
      nblksR = Mblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = mr;
      }
      else
      {
         rL = mr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].mb+rL) << eltsh;
      np = ATL_thrdecompMM_rMN(ptmms, TA, TB, nblksL, rL, Nblks, nr, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMN(ptmms, TA, TB, nblksR, rR, Nblks, nr, Kblks, kr,
                                (TA==AtlasNoTrans)?MindxT(A,i):MindxT(A,i*lda), 
                                lda, B, ldb, MindxT(C,i), ldc, pR, indx+pL, 
                                COPYC);
      return(np);
   }
/*
 * If no desirable splitting possible, stop recursion
 */
   ptmms[indx].A = A;
   ptmms[indx].B = B;
   ptmms[indx].C = (void*)C;
   ptmms[indx].lda = lda;
   ptmms[indx].ldb = ldb;
   ptmms[indx].ldc = ldc;
   ptmms[indx].M = ptmms[indx].mb*Mblks + mr;
   ptmms[indx].N = ptmms[indx].nb*Nblks + nr;
   ptmms[indx].K = ptmms[indx].kb*Kblks + kr;
   ptmms[indx].ldcw = ptmms[indx].nCw = 0;
   ptmms[indx].nCp = ptmms[indx].ownC = 1;
   ptmms[indx].Cinfp[ATL_NTHREADS-1] = ptmms+indx;
   ptmms[indx].Cw = NULL;
   #ifdef DEBUG
      fprintf(stderr, "%d: M=%d, N=%d, K=%d, ownC=%d, nCp=%d, nCw=%d\n",
              indx, ptmms[indx].M, ptmms[indx].N, ptmms[indx].K, 
              ptmms[indx].ownC, ptmms[indx].nCp, ptmms[indx].nCw);
   #endif
   return(1);
}

int ATL_thrdecompMM_rMNK
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
/*
 * This routine decomposes the GEMM over P processors by splitting any of
 * the dimensions.  We only call this routine when K is very large or
 * M and N are very small (and thus splitting K, with its associated
 * extra workspace and flops, makes sense).
 */
{
   int pR, pL, rL, rR, np, eltsh;
   ATL_INT nblksL, nblksR, j;
   size_t i;
   double d;

   eltsh = ptmms[indx].eltsh;
#ifdef DEBUG
   ATL_assert(P > 0);
#endif
   if (P <= 1 || (Mblks <= 1 && Nblks <= 1 && Kblks <= 1))
      goto STOP_REC;
   pR = P>>1;    /* on right, take P/2 threads */
   pL = P - pR;  /* on left, take remaining threads */
   d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */
/*
 * Do not consider cutting K unless we have some K blocks, and we have either
 * already done so, or if we are sure that we are within our workspace limit
 */
   if (Kblks > 1 && (COPYC || 
       ((Mblks*ptmms[indx].mb+mr) * ((Nblks*ptmms[indx].nb+nr)<<eltsh)
        < ATL_PTMAXMALLOC)))
   {
/*
 *    Before splitting K, ask that we are out of M and N blocks, or that
 *    our K is 4 times M and twice N
 */
      if ( (Mblks < 2 && Nblks < 2) ||
           (Kblks > (Mblks<<2) && Kblks > (Nblks+Nblks)) )
      {
         #ifdef DEBUG
            fprintf(stderr, "Cut K\n");
         #endif
         nblksL = (d * Kblks);
         nblksR = Kblks - nblksL;
         if (nblksR < nblksL)
         {
            rL = 0;
            rR = kr;
         }
         else
         {
            rL = kr;
            rR = 0;
         }
         i = (nblksL*ptmms[indx].kb + rL)<<eltsh;
         np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, Nblks, nr, 
                                   nblksL, rL, A, lda, B, ldb, C, ldc, pL, 
                                   indx, COPYC);
         np += ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, Nblks, nr, 
                                    nblksR, rR, (TA==AtlasNoTrans)?
                                    MindxT(A,lda*i):MindxT(A,i), lda, 
                                    (TB == AtlasNoTrans)?MindxT(B,i):
                                    MindxT(B,i*ldb), ldb, C, ldc, pR, 
                                    indx+pL, 1);
         return(np);
      }
   }
/*
 * Now choose to split either M or N.  Want M < N always, so require
 * N to be twice as big before splitting
 */
   if (Nblks > 1 && (Mblks < 2 || Nblks >= Mblks+Mblks))
   {
      #ifdef DEBUG
         fprintf(stderr, "Cut N\n");
      #endif
      nblksL = (d * Nblks);
      nblksR = Nblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = nr;
      }
      else
      {
         rL = nr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].nb+rL) << eltsh;
      np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, nblksL, rL, Kblks, kr,
                                A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, nblksR, rR, 
                                 Kblks, kr, A,  lda, (TB == AtlasNoTrans) ? 
                                 MindxT(B,i*ldb) : MindxT(B,i), ldb, 
                                 MindxT(C,i*ldc), ldc, pR, indx+pL, COPYC);
      return(np);
   }
/*
 * If we have failed to split N or K, split M if possible
 */
   if (Mblks > 1)
   {
      #ifdef DEBUG
         fprintf(stderr, "Cut M\n");
      #endif
      nblksL = (d * Mblks);
      nblksR = Mblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = mr;
      }
      else
      {
         rL = mr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].mb+rL) << eltsh;
      np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, nblksL, rL, Nblks, nr, Kblks, kr,
                                A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMNK(ptmms, TA, TB, nblksR, rR, Nblks, nr, 
                                 Kblks, kr,
                                 (TA==AtlasNoTrans)?MindxT(A,i):MindxT(A,i*lda),
                                 lda, B, ldb, MindxT(C,i), ldc, pR, indx+pL, 
                                 COPYC);
      return(np);
   }
/*
 * If no desirable splitting possible, stop recursion
 */
STOP_REC:
   ptmms[indx].A = A;
   ptmms[indx].B = B;
   ptmms[indx].C = (void*)C;
   ptmms[indx].lda = lda;
   ptmms[indx].ldb = ldb;
   ptmms[indx].ldc = ldc;
   ptmms[indx].M = ptmms[indx].mb*Mblks + mr;
   ptmms[indx].N = ptmms[indx].nb*Nblks + nr;
   ptmms[indx].K = ptmms[indx].kb*Kblks + kr;
   if (COPYC)
   {
      ptmms[indx].nCw = 1;
      ptmms[indx].nCp = ptmms[indx].ownC = 0;
      ptmms[indx].Cinfp[0] = ptmms+indx;
/*
 *    Make ldcw a multiple of 4 that is not a power of 2
 */
      i = ((ptmms[indx].M + 3)>>2)<<2;
      if (!(i & (i-1)))
         i += 4;
      ptmms[indx].ldcw = i;
   }
   else
   {
      ptmms[indx].ldcw = ptmms[indx].nCw = 0;
      ptmms[indx].nCp = ptmms[indx].ownC = 1;
      ptmms[indx].Cinfp[ATL_NTHREADS-1] = ptmms+indx;
   }
   ptmms[indx].Cw = NULL;
   #ifdef DEBUG
      fprintf(stderr, "%d: M=%d, N=%d, K=%d, ownC=%d, nCp=%d, nCw=%d\n",
              indx, ptmms[indx].M, ptmms[indx].N, ptmms[indx].K, 
              ptmms[indx].ownC, ptmms[indx].nCp, ptmms[indx].nCw);
   #endif
   return(1);

}
@beginskip
int ATL_thrdecompMM_rec
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int pR, pL, rL, rR, np, eltsh;
   ATL_INT nblksL, nblksR, j;
   size_t i;
   double d;

#ifdef DEBUG
   ATL_assert(P > 0);
#endif
/*
 * End recursion if we are down to 1 processor, or if we are out of blocks
 */
   if ( P <= 1 || (Mblks <= 1 && Nblks <= 1 && Kblks <= 2) )
   {
      ptmms[indx].A = A;
      ptmms[indx].B = B;
      ptmms[indx].C = (void*)C;
      ptmms[indx].lda = lda;
      ptmms[indx].ldb = ldb;
      ptmms[indx].ldc = ldc;
      ptmms[indx].M = ptmms[indx].mb*Mblks + mr;
      ptmms[indx].N = ptmms[indx].nb*Nblks + nr;
      ptmms[indx].K = ptmms[indx].kb*Kblks + kr;
      if (COPYC)
      {
         ptmms[indx].nCw = 1;
         ptmms[indx].nCp = ptmms[indx].ownC = 0;
         ptmms[indx].Cinfp[0] = ptmms+indx;
/*
 *       Make ldcw a multiple of 4 that is not a power of 2
 */
         i = ((ptmms[indx].M + 3)>>2)<<2;
         if (!(i & (i-1)))
            i += 4;
@beginskip
         for (j=0; j < sizeof(ATL_INT)*8-1; j++)
         {
            if (!((1<<j)^i))
            {
               i += 4;
               break;
            }
         }
@endskip
         ptmms[indx].ldcw = i;
      }
      else
      {
         ptmms[indx].ldcw = ptmms[indx].nCw = 0;
         ptmms[indx].nCp = ptmms[indx].ownC = 1;
         ptmms[indx].Cinfp[ATL_NTHREADS-1] = ptmms+indx;
      }
      ptmms[indx].Cw = NULL;
#ifdef DEBUG
fprintf(stderr, "%d: M=%d, N=%d, K=%d, ownC=%d, nCp=%d, nCw=%d\n",
        indx, ptmms[indx].M, ptmms[indx].N, ptmms[indx].K, 
        ptmms[indx].ownC, ptmms[indx].nCp, ptmms[indx].nCw);
#endif
      return(1);
   }

   pR = P>>1;    /* on right, take P/2 threads */
   pL = P - pR;  /* on left, take remaining threads */
   d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */

/*
 * Only cut K if it dominates M & N (here we say K must be 4 time larger)
 * and M&N are small enough that we can afford to malloc C 
 * (here we say C workspace must be 16MB or less) 
 */
   if ( ( ((Mblks < 2) && Nblks < 2) ||
          (((Kblks>>2) > Mblks) && ((Kblks>>2) > Nblks)) )
       && (Mblks*((Nblks*ptmms[indx].mb*(ptmms[indx].nb<<ptmms[indx].eltsh)
           +1023)>>10) <= 16*1024))
   {
#ifdef DEBUG
   fprintf(stderr, "Cut K\n");
#endif
      nblksL = (d * Kblks);
      nblksR = Kblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = kr;
      }
      else
      {
         rL = kr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].kb + rL)<<eltsh;
      np = ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, Nblks, nr, nblksL, rL,
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, Nblks, nr, nblksR, rR,
                                (TA==AtlasNoTrans)?MindxT(A,lda*i):MindxT(A,i), 
                                lda, 
                               (TB == AtlasNoTrans)?MindxT(B,i):MindxT(B,i*ldb),
                               ldb, C, ldc, pR, indx+pL, 1);
      return(np);
   }
   else if (Mblks >= Nblks)  /* split M */
   {
#ifdef DEBUG
   fprintf(stderr, "Cut M\n");
#endif
      nblksL = (d * Mblks);
      nblksR = Mblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = mr;
      }
      else
      {
         rL = mr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].mb+rL) << eltsh;
      np = ATL_thrdecompMM_rec(ptmms, TA, TB, nblksL, rL, Nblks, nr, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rec(ptmms, TA, TB, nblksR, rR, Nblks, nr, Kblks, kr,
                                (TA==AtlasNoTrans)?MindxT(A,i):MindxT(A,i*lda), 
                                lda, B, ldb, MindxT(C,i), ldc, pR, indx+pL, 
                                COPYC);
      return(np);
   }
   else /* split N */
   {
#ifdef DEBUG
   fprintf(stderr, "Cut N\n");
#endif
      nblksL = (d * Nblks);
      nblksR = Nblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = nr;
      }
      else
      {
         rL = nr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].nb+rL) << eltsh;
      np = ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, nblksL, rL, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, nblksR, rR, Kblks, kr,
                                A,  lda, (TB == AtlasNoTrans) ? 
                                MindxT(B,i*ldb) : MindxT(B,i), ldb, 
                                MindxT(C,i*ldc), ldc, pR, indx+pL, COPYC);
      return(np);
   }
}
@endskip

int ATL_thrdecompMM_M
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int j, i, m, p;
   const char *a=A, *c=C;
   const int eltsh = ptmms[0].eltsh, mb = ptmms[0].mb, n = ptmms[0].nb*Nblks+nr,
             k = ptmms[0].kb*Kblks+kr, minblks = Mblks / P, 
             extrablks = Mblks - minblks*P;

   for (p=i=0; i < P; i++)
   {
      m = minblks * mb;
      if (i < extrablks)
         m = (minblks + 1)*mb;
      else if (i == extrablks)
         m = minblks*mb + mr;
      else
         m = minblks*mb;
     if (m)
        p++;
         
@skip      j = ATL_launchorder[i];  /* use log2-launch order */
      ptmms[i].A = a;
      ptmms[i].B = B;
      ptmms[i].C = (void*)c;
      ptmms[i].lda = lda;
      ptmms[i].ldb = ldb;
      ptmms[i].ldc = ldc;
      ptmms[i].M = m;
      ptmms[i].N = n;
      ptmms[i].K = (m) ? k : 0;
      ptmms[i].ownC = 1;
      ptmms[i].nCp = ptmms[i].nCw = 0;
      ptmms[i].Cw = NULL;
      ptmms[i].ldcw = 0;
      m <<= eltsh;
      a = (TA == AtlasNoTrans) ? MindxT(a,m) : MindxT(a,m*lda);
      c = MindxT(c,m);
   }
   return(p);
}

int ATL_thrdecompMM_N
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int j, i, n, p;
   const char *b=B, *c=C;
   const int eltsh = ptmms[0].eltsh, nb = ptmms[0].nb, m = ptmms[0].mb*Mblks+mr,
             k = ptmms[0].kb*Kblks+kr, minblks = Nblks / P, 
             extrablks = Nblks - minblks*P;

   for (p=i=0; i < P; i++)
   {
      n = minblks * nb;
      if (i < extrablks)
         n = (minblks + 1)*nb;
      else if (i == extrablks)
         n = minblks*nb + nr;
      else
         n = minblks*nb;
      if (n)
         p++;
         
@skip      j = ATL_launchorder[i];  /* use log2-launch order */
      ptmms[i].A = A;
      ptmms[i].B = b;
      ptmms[i].C = (void*)c;
      ptmms[i].lda = lda;
      ptmms[i].ldb = ldb;
      ptmms[i].ldc = ldc;
      ptmms[i].M = m;
      ptmms[i].N = n;
      ptmms[i].K = (n) ? k : 0;
      ptmms[i].ownC = 1;
      ptmms[i].nCp = ptmms[i].nCw = 0;
      ptmms[i].Cw = NULL;
      ptmms[i].ldcw = 0;
      n <<= eltsh;
      b = (TB == AtlasNoTrans) ? MindxT(b,n*ldb) : MindxT(b,n);
      c = MindxT(c,n*ldc);
   }
   return(p);
}
int ATL_thrdecompMM_K
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int j, i, k, p, ldw;
   const char *a=A, *b=B;
   const int eltsh = ptmms[0].eltsh, kb = ptmms[0].kb, m = ptmms[0].mb*Mblks+mr,
             n = ptmms[0].nb*Nblks+nr, minblks = Kblks / P, 
             extrablks = Kblks - minblks*P;

   for (p=i=0; i < P; i++)
   {
      k = minblks * kb;
      if (i < extrablks)
         k = (minblks + 1)*kb;
      else if (i == extrablks)
         k = minblks*kb + kr;
      else
         k = minblks*kb;
      if (n)
         p++;
         
@skip      j = ATL_launchorder[i];  /* use log2-launch order */
      ptmms[i].A = a;
      ptmms[i].B = b;
      ptmms[i].C = (void*)C;
      ptmms[i].lda = lda;
      ptmms[i].ldb = ldb;
      ptmms[i].ldc = ldc;
      ptmms[i].M = m;
      ptmms[i].N = n;
      ptmms[i].K = k;
      if (i)
      {
         ptmms[i].nCw = 1;
         ptmms[i].nCp = ptmms[i].ownC = 0;
         ldw = ((m + 3)>>2)<<2;  /* make ldw mul of 4 */
         if (!(i & (i-1)))
            ldw += 4;            /* make sure ldw not power of 2 */
         ptmms[i].ldcw = ldw;
         ptmms[i].Cinfp[0] = ptmms+i;
      }
      else
      {
         ptmms[i].ldcw = 0;
         ptmms[i].nCp = ptmms[i].ownC = 1;
         ptmms[i].nCw = 0;
         ptmms[i].Cinfp[ATL_NTHREADS-1] = ptmms+i;
      }
      ptmms[i].Cw = NULL;
      k <<= eltsh;
      a = (TA == AtlasNoTrans) ? MindxT(a,lda*k) : MindxT(a,k);
      b = (TB == AtlasNoTrans) ? MindxT(b,k) : MindxT(b,k*ldb);
   }
   return(p);
}

#include <string.h>
void ATL_linearize_mmnodes(ATL_TMMNODE_t *ptmms, const int P)
/*
 * Takes P intialized entries in ptmms, and makes them contiguous
 * starting from 0 if they aren't already
 */
{
   int i;
   for (i=P-1; i >= 0; i--)
   {
      if (!ptmms[i].K)  /* found empty slot */
      {
         int j;
         for (j=P; !ptmms[j].K; j++);
         memcpy(ptmms+i, ptmms+j, sizeof(ATL_TMMNODE_t));
         if (ptmms[i].nCw || ptmms[i].nCp)
         {
            int k, n;
            n = ptmms[i].nCw;
            for (k=0; k < n; k++)
               if (ptmms[i].Cinfp[k] == ptmms+j)
                  ptmms[i].Cinfp[k] = ptmms+i;
            n = ptmms[i].nCp;
            for (k=0; k < n; k++)
               if (ptmms[i].Cinfp[ATL_NTHREADS-1-k] == ptmms+j)
                  ptmms[i].Cinfp[ATL_NTHREADS-1-k] = ptmms+i;
         }
         ptmms[j].K = 0;
      }
   }
}
@beginskip
void ATL_EnforceNonPwr2LO(ATL_TMMNODE_t *ptmms, const int P)
/*
 * If threads aren't a power of 2, then the recursive decomposition will
 * fill in the ptmms array in a different order than the log2 spawn.
 * As long as P >= NTHREADS, all entries are filled in, so the worst that
 * happens is that thread 3 does the work that we expect to be done by 4.
 * However, if P < NTHREADS, then threads that launch starts won't have
 * work, so we must make sure that the 1st P elts in launchorder have
 * work to do.
 */
{
   int i, j, k, kk, h;

   if (P >= ATL_NTHREADS)
      return;
   for (i=0; i < P; i++)
   {
      j = ATL_launchorder[i];
      if (!ptmms[j].K)  /* we have found an empty required entry! */
      {
/*
 *      Search for a filled in entry that will not be used in launch
 */
        for (kk=ATL_NTHREADS-1; kk >= P; kk--)
        {
           k = ATL_launchorder[kk];
           if (ptmms[k].K)  /* found a non-empty entry */
           {
              ptmms[j].A = ptmms[k].A;
              ptmms[j].B = ptmms[k].B;
              ptmms[j].C = ptmms[k].C;
              ptmms[j].lda = ptmms[k].lda;
              ptmms[j].ldb = ptmms[k].ldb;
              ptmms[j].ldc = ptmms[k].ldc;
              ptmms[j].M = ptmms[k].M;
              ptmms[j].N = ptmms[k].N;
              ptmms[j].K = ptmms[k].K;
              ptmms[j].ownC = ptmms[k].ownC;
              ptmms[j].nCp = ptmms[k].nCp;
              ptmms[j].nCw = ptmms[k].nCw;
              ptmms[j].Cw = ptmms[k].Cw;
              ptmms[j].ldcw = ptmms[k].ldcw;
              for (h=0; h < ptmms[j].nCw; h++)
                 ptmms[j].Cinfp[h] = (ptmms[k].Cinfp[h] != ptmms+k) ?
                                     ptmms[k].Cinfp[h] : ptmms+j;
              for (h=ATL_NTHREADS-1; h >= ATL_NTHREADS-ptmms[j].nCp; h--)
                 ptmms[j].Cinfp[h] = (ptmms[k].Cinfp[h] != ptmms+k) ?
                                     ptmms[k].Cinfp[h] : ptmms+j;
              ptmms[k].K = 0;                /* entry k now empty */
              break;
           }
        }
        ATL_assert(kk >= P);
      }
   }
}
@endskip

int ATL_thrdecompMM
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *A, ATL_INT lda, 
    const void *B, const ATL_INT ldb, const void *C, ATL_CINT ldc, const int P,
    int *DivideK)
{
   int np, i;
   ATL_CINT Mblks = M/ptmms[0].mb, mr = M-Mblks*ptmms[0].mb;
   ATL_CINT Nblks = N/ptmms[0].nb, nr = N-Nblks*ptmms[0].nb;
   ATL_CINT Kblks = K/ptmms[0].kb, kr = K-Kblks*ptmms[0].kb;
   ATL_CINT mnblks = ((Nblks) ? Nblks : 1) * ((Mblks) ? Mblks : 1);

  *DivideK = 0;
/*
 * First, consider cutting K, which we only do if the number of Kblks
 * dominates the number of blocks we can find in cutting both M & N,
 */
@skip   if (mnblks < P || Kblks > P*mnblks)
   if ((mnblks < P && Kblks > mnblks && Kblks >= 8) || Kblks > P*mnblks)
   {
      np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, Nblks, nr, Kblks, kr,
                                A, lda, B, ldb, C, ldc, P, 0, 0);
      for (i=0; i < np; i++)
      {
         if (ptmms[i].K > 0 && ptmms[i].K < K)
         {
            *DivideK = 1;
            break;
         }
      }
      if (np < ATL_NTHREADS)
         ATL_linearize_mmnodes(ptmms, np);
@skip         ATL_EnforceNonPwr2LO(ptmms, np);
      return(np);
   }
/*
 * Divide only the M-dimension to cut down on JIK workspace & improve CE
 * efficiency if we have enough M blocks to make it worthwhile;
 * We ask that we can give each thread at least 4 blocks, and that
 * the N diminsion doesn't dominate
 */
   if ((Mblks >= (P<<2) && Nblks < P*Mblks))
   {
      np = ATL_thrdecompMM_M(ptmms, TA, TB, Mblks, mr, Nblks, nr, Kblks, kr,
                             A, lda, B, ldb, C, ldc, P, 0, 0);
      if (np < ATL_NTHREADS)
         ATL_linearize_mmnodes(ptmms, np);
      return(np);
   }
/*
 * If none of these special cases are triggered, recursively divide up C
 */
   np = ATL_thrdecompMM_rMN(ptmms, TA, TB, Mblks, mr, Nblks, nr, Kblks, kr, 
                            A, lda, B, ldb, C, ldc, P, 0, 0);
   if (np < ATL_NTHREADS)
      ATL_linearize_mmnodes(ptmms, np);
@skip      ATL_EnforceNonPwr2LO(ptmms, np);
   return(np);
}

@ROUT ATL_StructIsInitMM ATL_Xtgemm
int ATL_StructIsInitMM(void *vp)
{
   return(((ATL_TMMNODE_t*)vp)->K);
}

@ROUT ATL_DoWorkMM ATL_Xtgemm
void ATL_DoWorkMM(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/*
 * Current implementation doesn't need lp, but if we had an error queue or
 * something similar we would need it, so keep it around
 */
{
   ATL_thread_t *tp = vp;
   const int myrank = tp->rank;
   ATL_TMMNODE_t *mmp = ((ATL_TMMNODE_t*)lp->opstruct)+myrank;
/*
 * Allocate space if needed, do operation
 */
   if (mmp->nCw)
   {
/*
 *    If malloc fails, we'll do the operation during the combine
 */
      #ifdef ATL_SERIAL_COMBINE
         ATL_assert(mmp->Cw);
      #else
         mmp->Cw = malloc(((mmp->ldcw)<<mmp->eltsh)*mmp->N+ATL_Cachelen);
      #endif
      if (mmp->Cw)
      {
         mmp->gemmK(mmp->M, mmp->N, mmp->K, mmp->alpha, mmp->A, mmp->lda,
                    mmp->B, mmp->ldb, mmp->zero, 
                    ATL_AlignPtr(mmp->Cw), mmp->ldcw);
      }
#ifdef DEBUG
      else
         fprintf(stderr, "%d: unable to allocate C(%dx%d)!!\n", 
                 mmp->rank, mmp->M, mmp->N);
#endif
   }
   else  /* do GEMM directly into original C; no possibility of failure! */
      mmp->gemmK(mmp->M, mmp->N, mmp->K, mmp->alpha, mmp->A, mmp->lda,
                 mmp->B, mmp->ldb, mmp->beta, mmp->C, mmp->ldc);
}
@ROUT ATL_tNumGemmThreads
#include "atlas_misc.h"
#include "atlas_tlvl3.h"

/* 
 * ====================================================================
 * This function will eventually be generated, but for now just written
 * ====================================================================
 */
int Mjoin(PATL,tNumGemmThreads)(ATL_CINT M, ATL_CINT N, ATL_CINT K)
/*
 * RETURNS : estimate of how many threads will be used to run the problem,
 *           assuming we will actually do threading (i.e. THRESH is exceeded)
 *           0 is returned if this number is 1 or less.
 */
{
@beginskip
   int np;
   ATL_TMMNODE_t mms[ATL_NTHREADS];
   #ifdef TCPLX
      TYPE ONE[2] = {1.0, 0.0};
   #else
      TYPE ONE=ATL_rone;
   #endif
   void Mjoin(PATL,InitTMMNodes)
      (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, const TYPE *alpha,
       const TYPE *beta, const TYPE *one, const TYPE *zero, 
       ATL_thread_t *btp, ATL_TMMNODE_t *ptmms);

   np = Mjoin(PATL,threadMM)(AtlasNoTrans, AtlasNoTrans, M, N, K);
   if (np > 1)
   {
      Mjoin(PATL,InitTMMNodes)(AtlasNoTrans, AtlasNoTrans, SADD ONE, SADD ONE, 
                               SADD ONE, SADD ONE, NULL, mms);
      if (np == 1)  /* use recursive distribution */
         np = ATL_thrdecompMM_rec(mms, AtlasNoTrans, AtlasNoTrans, 
                                  M/MB, M%MB, N/NB, N%NB, K/KB, K%KB, 
                                  NULL, M, NULL, K, NULL, M, 
                                  ATL_NTHREADS, 0, 0);
      else
         np = ATL_thrdecompMM_M(mms, AtlasNoTrans, AtlasNoTrans, 
                                M/MB, M%MB, N/NB, N%NB, K/KB, K%KB, 
                                NULL, M, NULL, K, NULL, M, ATL_NTHREADS, 0, 0);
      np = (np < 2) ? 0 : np;
   }
   return(np);
@endskip
   double flops;
   int np;
   if (M < 4 || N < 4 || K < 4)
      return(0);
   flops = ((2.0*M)*N)*K;
   np = flops / ATL_TGEMM_PERTHR_MF;
   np = (np <= 1) ? 0 : np;
   return(np >= ATL_NTHREADS ? ATL_NTHREADS : np);
}

int Mjoin(PATL,GemmWillThread)(ATL_CINT M, ATL_CINT N, ATL_CINT K)
/*
 * Returns: 0 if threshold FLOPS not achieved, rough # of threads used else
 */
{
   if (((2.0*M)*N)*K < ATL_TGEMM_THRESH_MF)
      return(0);
   return(Mjoin(PATL,tNumGemmThreads)(M,N,K));
}
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
/*
 * prototype the typeless tGEMM helper routines
 */
void ATL_DoWorkMM(ATL_LAUNCHSTRUCT_t *lp, void *vp);
int ATL_StructIsInitMM(void *vp);
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K
void ATL_linearize_mmnodes(ATL_TMMNODE_t *ptmms, const int P);
@whiledef rt  ATL_thrdecompMM_M ATL_thrdecompMM_N ATL_thrdecompMM_K ATL_thrdecompMM_rMNK
int @(rt)
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC);
@endwhile
@ROUT ATL_tgemm ATL_tgemm_p
int ATL_thrdecompMM
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *A, ATL_INT lda, 
    const void *B, const ATL_INT ldb, const void *C, ATL_CINT ldc, const int P,
    int *DivideK);

@ROUT ATL_tgemm
@multidef tta AtlasTrans AtlasNoTrans AtlasConjTrans
@whiledef TA T N C
   @multidef ttb AtlasTrans AtlasNoTrans AtlasConjTrans
   @whiledef TB T N C
@mif TA = "C
#ifdef TCPLX
@endmif
@mif TB = "C
   @mif TA ! "C
#ifdef TCPLX
   @endmif
@endmif
void Mjoin(PATL,tsvgemm@(TA)@(TB))
   (ATL_CINT M, ATL_CINT N, ATL_CINT K, const void* alpha,
    const void *A, ATL_CINT lda, const void *B, ATL_CINT ldb, 
    const void *beta, void *C, ATL_CINT ldc)
{
#ifdef FindingCE
void Mjoin(PATL,FindCE_mm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                           const int M, const int N, const int K, 
                           const SCALAR alpha, const TYPE *A, const int lda, 
                           const TYPE *B, const int ldb, const SCALAR beta, 
                           TYPE *C, const int ldc);
   Mjoin(PATL,FindCE_mm)(@(tta), @(ttb), M, N, K, 
                         SVVAL((TYPE*)alpha), A, lda, B, ldb,
                         SVVAL((TYPE*)beta), C, ldc);
#else
   Mjoin(PATL,tgemm@(TA)@(TB))(M, N, K, SVVAL((TYPE*)alpha), A, lda, B, ldb,
                       SVVAL((TYPE*)beta), C, ldc);
#endif
}
@mif TA = "C
#endif  /* end ifdef TCPLX */
@endmif
@mif TB = "C
   @mif TA ! "C
#endif  /* end ifdef TCPLX */
   @endmif
@endmif
      @undef ttb
   @endwhile
   @undef tta
@endwhile
@ROUT ATL_tgemm_p
   @define ds @p@
@ROUT ATL_tgemm_M
   @define ds @M@
@ROUT ATL_tgemm_N
   @define ds @rMN@
@ROUT ATL_tgemm_K
   @define ds @K@
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
void Mjoin(PATL,InitTMMNodes)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, const TYPE *alpha, 
    const TYPE *beta, const TYPE *one, const TYPE *zero, 
    ATL_thread_t *btp, ATL_TMMNODE_t *ptmms);
@ROUT ATL_tgemm

#ifdef ATL_SERIAL_COMBINE
static ATL_combnode_t *ATL_NewCombnode
   (ATL_INT M, ATL_INT N, TYPE *W, ATL_INT ldw, TYPE *D, ATL_INT ldd,
    ATL_combnode_t *next)
{
   ATL_combnode_t *np;
   np = malloc(sizeof(ATL_combnode_t));
   ATL_assert(np);
   np->M = M;
   np->N = N;
   np->W = W;
   np->ldw = ldw;
   np->D = D;
   np->ldd = ldd;
   np->next = next;
   return(np);
}
#endif

void Mjoin(PATL,InitTMMNodes)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, const TYPE *alpha, 
    const TYPE *beta, const TYPE *one, const TYPE *zero, 
    ATL_thread_t *btp, ATL_TMMNODE_t *ptmms)
{
   int i;
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);

   if (TA == AtlasNoTrans)
   {
#ifdef TCPLX
      if (TB == AtlasConjTrans)
         gemmK = Mjoin(PATL,tsvgemmNC);
      else
#endif
      gemmK = (TB == AtlasNoTrans)?Mjoin(PATL,tsvgemmNN):Mjoin(PATL,tsvgemmNT);
   }
#ifdef TCPLX
   else if (TA == AtlasConjTrans)
   {
      if (TB == AtlasNoTrans)
         gemmK = Mjoin(PATL,tsvgemmCN);
      else if (TB == AtlasConjTrans)
         gemmK = Mjoin(PATL,tsvgemmCC);
      else
         gemmK = Mjoin(PATL,tsvgemmCT);
   }
#endif
   else
   {
#ifdef TCPLX
      if (TB == AtlasConjTrans)
         gemmK = Mjoin(PATL,tsvgemmTC);
      else
#endif
      gemmK = (TB == AtlasNoTrans)?Mjoin(PATL,tsvgemmTN):Mjoin(PATL,tsvgemmTT);
   }
   for (i=0; i < ATL_NTHREADS; i++)
   {
      ptmms[i].mb = MB;
      ptmms[i].nb = NB;
      ptmms[i].kb = KB;
      ptmms[i].gemmK = gemmK;
      ptmms[i].eltsz = ATL_sizeof;
      ptmms[i].eltsh = Mjoin(PATL,shift);
      ptmms[i].K = 0;
      ptmms[i].nCp = ptmms[i].nCw = 0;
      ptmms[i].ownC = 0;
      ptmms[i].rank = i;
      ptmms[i].alpha = (void*) alpha;
      ptmms[i].beta  = (void*) beta;
      ptmms[i].one = (void*) one;
      ptmms[i].zero  = (void*) zero;
      ptmms[i].Cinfp[0] = ptmms+i;
   }
}

@ROUT ATL_tgemm_rec ATL_tgemm_K ATL_tgemm_p
void Mjoin(PATL,HandleNewCp)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him);
int Mjoin(PATL,CombineCw)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him);
@ROUT ATL_tgemm
int Mjoin(PATL,CombineCw)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him)
/*
 * This routine combines the data in him->Cw into my->Cw (or my->C), if poss.
 * If his workspace is bigger than mine, I combine instead into his workspace,
 * and then set my pointer to his workspace.  The buffer that has been subsumed
 * is freed after the combine.
 * NOTE: This routine assumes him is *not* an owner of C (i.e. he wrote to
 *       workspace, not to the original C)!
 * RETURNS: 0 if we are able to do the combine, non-zero if buffers are
 *          cannot be combined.
 */
{
   ATL_TMMNODE_t *myCp;
   TYPE *w;
   size_t meB, meE, himB, himE, I, J;   /* begin,end of C range */
   #ifdef TREAL
      const TYPE ONE = 1.0;
   #else
      const TYPE ONE[2] = {1.0, 0.0};
   #endif
   const int eltsh = me->eltsh;

   ATL_assert(!him->ownC);
/*
 * Find starting/ending points of our C partitions
 */
   himB = (size_t)him->C; 
   himE = himB + ((him->N*(him->ldc) + him->M)<<eltsh);
   meB  = (size_t) me->C; 
   meE  = meB + ((me->N*(me->ldc) + me->M)<<eltsh);
/*
 * If I own my piece of the original C, then I can combine any C that is
 * a proper subset of mine; 
 */
   if (me->ownC)
   {
      ATL_assert(!him->ownC);  /* should never be true in this routine */
/*
 *    If his wrkspc is not a subset of mine, I can't combine it into the
 *    piece of C originally owned by me
 */
      if (himB < meB || himE > meE)
         return(1);       /* can't combine non subset of my C */
      else if (him->Cw)  /* his malloc succeeded */
      {
         Mjoin(PATL,geadd)(him->M, him->N, ONE, ATL_AlignPtr(him->Cw), 
                           him->ldcw, ONE, (TYPE*)him->C, him->ldc);
         free(him->Cw);
      }
      else if (him->nCw)  /* must do GEMM since he couldn't malloc */
         him->gemmK(him->M, him->N, him->K, him->alpha, him->A, him->lda,
                    him->B, him->ldb, SADD ONE, him->C, him->ldc);
      return(0);        /* successful combine */
   }
/*
 * *************************************************************************
 * Otherwise, I don't own C, so must combine work into my or his buffer when
 * possible, and return failure when not
 * *************************************************************************
 */
/*
 * If my workspace is a superset of his, use my workspace as the target buffer
 * if I was able to allocate it
 */
   if (meB <= himB && meE >= himE && me->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (himB - meB)>>eltsh;                    /* gap in elements */
      J = I / him->ldc;                           /* column coord */
      I -= J*him->ldc;                            /* row coord */
      if (I+him->M >= me->M || J+him->N >= me->N) /* no intersec after all! */
         return(1);                               /* so cannot combine */
      w = ATL_AlignPtr(me->Cw);
      w += J*me->ldcw + I;
      if (him->Cw)  /* if he succeeded in malloc, combine his op with mine */
      {
         Mjoin(PATL,geadd)(him->M, him->N, ONE, ATL_AlignPtr(him->Cw), 
                           him->ldcw, ONE, w, me->ldcw);
         free(him->Cw);
      }
      else          /* must do GEMM since he didn't */
         him->gemmK(him->M, him->N, him->K, him->alpha, him->A, him->lda,
                    him->B, him->ldb, SADD ONE, w, me->ldcw);
      return(0);        /* successful combine */
   }
/*
 * else if his workspace is a superset of mine, use his as target buffer if
 * he was able to allocate it
 */
   else if (himB <= meB && himE >= meE && him->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (meB - himB)>>eltsh;                    /* gap in elements */
      J = I / me->ldc;                            /* col coordinate */
      I -= J*me->ldc;                             /* row coordinate */
      if (I+me->M >= him->M || J+me->N >= him->N) /* no intersec after all! */
         return(1);                               /* so cannot combine */
      w = ATL_AlignPtr(him->Cw);
      w += J*him->ldcw + I;
      if (me->Cw)  /* if I succeeded in malloc, combine my op with his */
      {
         Mjoin(PATL,geadd)(me->M, me->N, ONE, ATL_AlignPtr(me->Cw), 
                           me->ldcw, ONE, w, him->ldcw);
         free(me->Cw);
      }
      else          /* must do my GEMM into his workspace since I couldn't */
         him->gemmK(me->M, me->N, me->K, me->alpha, me->A, me->lda,
                    me->B, me->ldb, SADD ONE, w, him->ldcw);
      me->C = him->C;
      me->Cw = him->Cw;
      me->ldcw = him->ldcw;
      me->M = him->M;
      me->N = him->N;
      return(0);        /* successful combine */
   }
   return(1);           /* unsuccessful combine */
}

void Mjoin(PATL,HandleNewCp)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him)
/*
 * Handles joining a Cp to my list of C partitions
 */
{
  size_t himB, himE, meB, meE, B, E, I, J;
  ATL_TMMNODE_t *tp;
  ATL_INT ldc;
  int i, j;
  const int eltsh = me->eltsh;
/*
 * Find the extent of his C partition
 */
   himB = (size_t)him->C; 
   himE = himB + ((him->N*(him->ldc) + him->M)<<eltsh);
   ldc = him->ldc;
/*
 * First, see if this partition can be joined to one I already own
 */
   for (i=0; i < me->nCp; i++)
   {
      tp = me->Cinfp[ATL_NTHREADS-1-i];
      if (tp)
      {
         meB  = (size_t) tp->C; 
         meE  = meB + ((tp->N*(tp->ldc) + tp->M)<<eltsh);
         if (meB <= himB)  /* my partition has the base pointer */
         {
            I = (himB - meB)>>eltsh;    /* gap between our C's in elements */
            J = I / ldc;                /* column coord from meB */
            I -= J*ldc;                 /* row coord from meB */
/*
 *          If we have same row (col) coord and he starts at col (row) that 
 *          I stop at, join column (row) panel
 */
            if (!I && J == tp->N)
               tp->N += him->N;
            else if (!J && I == tp->M)
               tp->M += him->M;
            else
               continue;                /* if unjoinable, go next candidate */
            break;   /* partitions joined, quit */
         }
         else              /* his partition has the base pointer */
         {
            I = (meB - himB)>>eltsh;    /* gap between our C's in elements */
            J = I / ldc;                /* column coord from himB */
            I -= J*ldc;                 /* row coord from himB */
/*
 *          If we have same row (col) coord and I start at col (row) that 
 *          he stops at, join column (row) space
 */
            if (!I && J == him->N)
               tp->N += him->N;
            else if (!J && I == tp->M)
               tp->M += him->M;
            else
               continue;                /* if unjoinable, go next candidate */
            tp->C = him->C;
            break;   /* partitions joined, quit */
         }
      }
   }
/*
 * If I can't join his partition to any of mine, add his to list
 */
   if (i == me->nCp)
   {
      (me->nCp)++;
      me->Cinfp[ATL_NTHREADS-(me->nCp)] = him;
      tp = him;
   }
/*
 * Either new partition is in tp, or an expanded partition is.  In either
 * case, see if any of my workspaces can be combined into this new 
 * (or newly expanded) area I own.
 */
   if (i < me->nCp)
   {
      for (i=0; i < me->nCw; i++)
      {
         if (!Mjoin(PATL,CombineCw)(tp, me->Cinfp[i]))
         {
            for (j=i+1; j < me->nCw; j++)
               me->Cinfp[j-1] = me->Cinfp[j];
            (me->nCw)--;
         }
      }
   }
}

@ROUT ATL_tgemm_rec ATL_tgemm_K ATL_tgemm_p
void Mjoin(PATL,CombineStructsMM)(void *vp, const int myrank, const int herank);
@ROUT ATL_tgemm
void Mjoin(PATL,CombineStructsMM)
(
   void *vp,          /* void ptr to P MMNODE_t structs give tasks to threads */
   const int myrank,  /* my entry in MMNODE_t array */
   const int hisrank  /* array entry to be combined into mine */
)
{
   ATL_TMMNODE_t *mme = ((ATL_TMMNODE_t*)vp)+myrank; 
   ATL_TMMNODE_t *mhim = ((ATL_TMMNODE_t*)vp)+hisrank;
   int i, j;
   #ifdef ATL_SERIAL_COMBINE  /* do nothing if combining serially */
      return;
   #endif

/*
 * First, for all of the partitions of the original C that he owns, either
 * join them with mine, or add them to my list of owned partitions
 */
   for (i=0; i < mhim->nCp; i++)
      Mjoin(PATL,HandleNewCp)(mme, mhim->Cinfp[ATL_NTHREADS-1-i]);
/*
 * For all of his workspaces, find out where to combine them into
 */
   for (i=0; i < mhim->nCw; i++)
   {
/*
 *    Look through my partitions of original C for combine partner
 */
      for (j=0; j < mme->nCp; j++)
         if (!Mjoin(PATL,CombineCw)(mme->Cinfp[ATL_NTHREADS-1-j], 
                                    mhim->Cinfp[i]))
            break;
/*
 *    If I can't combine his data directly into C, see if it can be
 *    combined with any of my workspaces
 */
      if (j == mme->nCp)
      {
         for (j=0; j < mme->nCw; j++)
            if (!Mjoin(PATL,CombineCw)(mme->Cinfp[j], mhim->Cinfp[i]))
               break;
/*
 *       If I can't combine his data into any partition or workspace, add his
 *       node to my list of workspaces to be combined later
 */
         if (j == mme->nCw)
         {
            mme->Cinfp[j] = mhim->Cinfp[i];
            mme->nCw = j + 1;
         }
      }
   }
}

@ROUT ATL_tgemm
void Mjoin(PATL,tgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
@ROUT ATL_tgemm_rec
int Mjoin(PATL,tgemm_rec)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
@ROUT ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
int Mjoin(PATL,tgemm_@(ds))(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
                       ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha, 
                       const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
                       const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   #ifdef ATL_SERIAL_COMBINE
      ATL_combnode_t *combb=NULL, *combp;
   #endif
@skip   ATL_thread_t tp[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
   ATL_TMMNODE_t mms[ATL_NTHREADS];
   int i, np, DividedK=0;
   #ifdef TREAL
      TYPE ONE=ATL_rone, ZERO=ATL_rzero;
   #else
      TYPE ONE[2] = {ATL_rone, ATL_rzero}, ZERO[2] = {ATL_rzero, ATL_rzero};
   #endif
@ROUT ATL_tgemm_p `   extern int ATL_FINDP;`

   if (M < 1 || N < 1)
@ROUT ATL_tgemm
      return;
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
      return(0);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
   if (K < 1 || SCALAR_IS_ZERO(alpha))
   {
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,gescal)(M, N, beta, C, ldc);
@ROUT ATL_tgemm
      return;
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
      return(0);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
   }
@ROUT ATL_tgemm
/*
 * See if we are in a case where we've implemented a dynamically scheduled
 * code.  Performance of the dynamically scheduled operations varies:
 * on Intel, its typically faster, and on AMD its slower than
 * statically scheduled *when run on an unloaded machine*.  The difference is
 * that if there is load on one or more processors, dynamically scheduled
 * code is almost twice as fast.  On unloaded machines, the performance 
 * difference is due mostly to the differing partitioning, not the overhead
 * of dynamic scheduling, which always seems to pay for itself.
 * On unloaded AMD machines, the asymptotic loss is roughly 1-2%.
 * Dynamic scheduling seems to always be a performance loss for MAC OSX
 * These routines require strongly-ordered caches, so only enable on x86.
 */
   #if !defined(ATL_OS_OSX) && (defined(ATL_GAS_x8664)||defined(ATL_GAS_x8632))
      #ifdef FindingCE
         ATL_assert(!Mjoin(PATL,tgemm_bigMN_Kp)(TA, TB, M, N, K, alpha, A, lda, 
                                                B, ldb, beta, C, ldc));
         return;
      #endif
/*
 *    Rank-K update has special case
 */
      i = Mmax(ATL_NTHREADS,4)*NB;
      if (K <= 4*NB && M >= 2*MB && N >= 2*NB && Mmax(M,N) >= i)
      {
         if (!Mjoin(PATL,tgemm_rkK)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                    beta, C, ldc))
            return;
      }
/*
 *    Very large matrices loops over rank-Kp updates, where Kp is set by
 *    CacheEdge.  If any dim is very small, use one of the other cases.
 */
      i = Mmin(M,N);
      i = Mmin(i, K);
      if (i > Mmax(8,2*ATL_NTHREADS)*NB)
      {
         if (!Mjoin(PATL,tgemm_bigMN_Kp)(TA, TB, M, N, K, alpha, A, lda, 
                                         B, ldb, beta, C, ldc))
            return;
      }
   #endif
/*
 * See how many processors are optimal for this problem
 */
   np = Mjoin(PATL,threadMM)(TA, TB, M, N, K);
   if (np > 1)
   {
      Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                               SADD ZERO, NULL, mms);
      np = ATL_thrdecompMM(mms, TA, TB, M, N, K, A, lda, B, ldb, C, ldc, 
                           np, &DividedK);
   }
@ROUT ATL_tgemm_p
   Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                            SADD ZERO, NULL, mms);
   np = ATL_thrdecompMM(mms, TA, TB, M, N, K, A, lda, B, ldb, C, ldc, 
                        ATL_FINDP, &DividedK);
   if (np < ATL_FINDP)
      return(0);
@ROUT ATL_tgemm_rec
   Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                            SADD ZERO, NULL, mms);
   np = ATL_thrdecompMM_rMNK(mms, TA, TB, M/MB, M%MB, N/NB, N%NB, K/KB, K%KB,
                             A, lda, B, ldb, C, ldc, ATL_NTHREADS, 0, 0);
   if (np < ATL_NTHREADS)
      ATL_linearize_mmnodes(mms, np);
@beginskip
   #if ATL_NTHREADS != (1<<ATL_NTHRPOW2)
      if (np < ATL_NTHREADS)
         ATL_EnforceNonPwr2LO(mms, np);
   #endif
@endskip
@ROUT ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K
   Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                            SADD ZERO, NULL, mms);
   np = ATL_thrdecompMM_@(ds)(mms, TA, TB, M/MB, M%MB, N/NB, N%NB, K/KB, K%KB,
                          A, lda, B, ldb, C, ldc, ATL_NTHREADS, 0, 0);
   if (np < ATL_NTHREADS)
      ATL_linearize_mmnodes(mms, np);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
#ifdef DEBUG
fprintf(stderr, "np=%d\n\n", np);
#endif
   if (np < 2)
   {
      Mjoin(PATL,gemm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p `      return(1);`
@ROUT ATL_tgemm `      return;`
   }
/*
 * If we are debugging, set up serial combine queue
 */
   #ifdef ATL_SERIAL_COMBINE
      for (i=0; i < ATL_NTHREADS; i++)
      {
         if (mms[i].K)   /* if this struct being used */
         {
            if (!mms[i].ownC)   /* I need a workspace for C */
            {
               mms[i].Cw = calloc(mms[i].ldcw * mms[i].N, ATL_sizeof);
               ATL_assert(mms[i].Cw);
               combb = ATL_NewCombnode(mms[i].M, mms[i].N, mms[i].Cw, 
                                       mms[i].ldcw, mms[i].C, mms[i].ldc,
                                       combb);
            }
         }
      }
   #endif

@beginskip
   ls.opstruct = (char*) mms;
   ls.opstructstride = (int) ( ((char*)(mms+1)) - (char*)mms );
   ls.OpStructIsInit = ATL_StructIsInitMM;
@ROUT ATL_tgemm_M ATL_tgemm_N
   ls.CombineOpStructs = NULL;
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_K ATL_tgemm_p
   ls.CombineOpStructs = DividedK ? Mjoin(PATL,CombineStructsMM) : NULL;
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
   ls.DoWork = ATL_DoWorkMM;
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_log2tlaunch, tp);
   ATL_thread_join(tp);
@endskip
@ROUT ATL_tgemm ATL_tgemm_K ATL_tgemm_p ATL_tgemm_rec
   ATL_goparallel(np, ATL_DoWorkMM, mms, 
                  DividedK ? Mjoin(PATL,CombineStructsMM) : NULL);
@ROUT ATL_tgemm_M ATL_tgemm_N
   ATL_goparallel(np, ATL_DoWorkMM, mms, NULL);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
/*
 * If we are debugging, serially combine all workspaces back to original C
 */
   #ifdef ATL_SERIAL_COMBINE
      while(combb)
      {
         Mjoin(PATL,geadd)(combb->M, combb->N, ONE, combb->W, combb->ldw,
                           ONE, combb->D, combb->ldd);
         free(combb->W);
         combp = combb;
         combb = combb->next;
         free(combp);
      }
   #endif
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p `   return(np);`
}
@ROUT ATL_tgemm

void Mjoin(PATL,tvgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                        ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *alpha,
                        const void *A, ATL_CINT lda, const void *B,ATL_CINT ldb,
                        const void *beta, void *C, ATL_CINT ldc)
/* 
 * This void wrapper for tgemm is used in some typeless structures
 */
{
   Mjoin(PATL,tgemm)(TA, TB, M, N, K, SVVAL((const TYPE*)alpha), A, lda,
                     B, ldb, SVVAL((const TYPE*)beta), C, ldc);
}
@ROUT ATL_tsymm
   @define rt @symm@
   @define trans @AtlasTrans@
@ROUT ATL_themm
   @define rt @hemm@
   @define trans @AtlasConjTrans@
@ROUT ATL_themm ATL_tsymm
#include "atlas_misc.h"
@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
int Mjoin(PATL,StructIsInit@up@(rt))(void *vp)
{
   return(((ATL_TSYMM_t*)vp)->M);
}

void Mjoin(PATL,DoWork@up@(rt))(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_TSYMM_t *sp=((ATL_TSYMM_t*)lp->opstruct)+tp->rank;
   Mjoin(PATL,@(rt))(sp->side, sp->uplo, sp->M, sp->N, SVVAL((TYPE*)sp->alpha),
                     sp->A, sp->lda, sp->B, sp->ldb, SVVAL((TYPE*)sp->beta),
                     sp->C, sp->ldc);
}

static void ATL_@(rt)L_rec
   (ATL_TSYMM_t *syp, ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr,
    const TYPE *A, const TYPE *B, TYPE *C)
{
   const TYPE *A10, *A01, *B10;
   TYPE *C10;
   const int nb = syp->nb;
   ATL_INT nbR, nbL, rR, rL, nL, nR;
   #ifdef TCPLX
      TYPE ONE[2] = {ATL_rone,ATL_rzero};
   #else
      TYPE ONE = ATL_rone;
   #endif

   nbR = Mblks>>1;
   nbL = Mblks - nbR;
/*
 * Stop recursion once we are no longer getting parallelism
 */
@skip   if (nbR*Nblks < ATL_TGEMM_XOVER)
   if (Mjoin(PATL,threadMM)(AtlasNoTrans, AtlasNoTrans, 
                            nbR*nb, Nblks*nb+nr, nbR*nb) < 2)
   {
      Mjoin(PATL,@(rt))(syp->side, syp->uplo, Mblks*nb+mr, syp->N, 
                       SVVAL((TYPE*)syp->alpha), A, syp->lda, B, syp->ldb,
                       SVVAL((TYPE*)syp->beta), C, syp->ldc);
      return;
   }
   rL = (nbR == nbL) ? mr : 0;
   rR = mr - rL;
   nL = nbL*nb + rL;
   nR = nbR*nb + rR;
   B10 = B + (nL SHIFT);
   C10 = C + (nL SHIFT);
   ATL_@(rt)L_rec(syp, nbL, rL, Nblks, nr, A, B, C);
   ATL_@(rt)L_rec(syp, nbR, rR, Nblks, nr, A+(syp->lda+1)*(nL SHIFT), B10, C10);
   if (syp->uplo == AtlasLower)
   {
      A10 = A + (nL SHIFT);
      Mjoin(PATL,tgemm)(@(trans), AtlasNoTrans, nL, syp->N, nR, 
                        SVVAL((TYPE*)syp->alpha), A10, syp->lda, B10, syp->ldb, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, nR, syp->N, nL, 
                        SVVAL((TYPE*)syp->alpha), A10, syp->lda, B, syp->ldb, 
                        ONE, C10, syp->ldc);
   }
   else
   {
      A01 = A + nL*(syp->lda SHIFT);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, nL, syp->N, nR, 
                        SVVAL((TYPE*)syp->alpha), A01, syp->lda, B10, syp->ldb, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(@(trans), AtlasNoTrans, nR, syp->N, nL, 
                        SVVAL((TYPE*)syp->alpha), A01, syp->lda, B, syp->ldb, 
                        ONE, C10, syp->ldc);
   }
}
static void ATL_@(rt)R_rec
   (ATL_TSYMM_t *syp, ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr,
    const TYPE *A, const TYPE *B, TYPE *C)
{
   const TYPE *A10, *A01, *B01;
   TYPE *C01;
   const int nb = syp->nb;
   ATL_INT nbR, nbL, rR, rL, nL, nR;
   #ifdef TCPLX
      TYPE ONE[2] = {ATL_rone,ATL_rzero};
   #else
      TYPE ONE = ATL_rone;
   #endif

   nbR = Nblks>>1;
   nbL = Nblks - nbR;
/*
 * Stop recursion once we are no longer getting parallelism
 */
@skip   if (nbR*Mblks < ATL_TGEMM_XOVER)
   if (Mjoin(PATL,threadMM)(AtlasNoTrans, AtlasNoTrans, 
                            Mblks*nb+mr, nbR*nb, nbR*nb) < 2)
   {
      Mjoin(PATL,@(rt))(syp->side, syp->uplo, syp->M, Nblks*nb+nr,
                       SVVAL((TYPE*)syp->alpha), A, syp->lda, B, syp->ldb,
                       SVVAL((TYPE*)syp->beta), C, syp->ldc);
      return;
   }
   rL = (nbR == nbL) ? nr : 0;
   rR = nr - rL;
   nL = nbL*nb + rL;
   nR = nbR*nb + rR;
   B01 = B + (nL*syp->ldb SHIFT);
   C01 = C + (nL*syp->ldc SHIFT);
   ATL_@(rt)L_rec(syp, Mblks, mr, nbL, rL, A, B, C);
   ATL_@(rt)L_rec(syp, Mblks, mr, nbR, rR, A+(syp->lda+1)*(nL SHIFT), B01, C01);
   if (syp->uplo == AtlasLower)
   {
      A10 = A + (nL SHIFT);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, syp->M, nL, nR, 
                        SVVAL((TYPE*)syp->alpha), B01, syp->ldb, A10, syp->lda, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(AtlasNoTrans, @(trans), syp->M, nR, nL, 
                        SVVAL((TYPE*)syp->alpha), B, syp->ldb, A10, syp->lda, 
                        ONE, C01, syp->ldc);
   }
   else
   {
      A01 = A + (syp->lda SHIFT);
      Mjoin(PATL,tgemm)(AtlasNoTrans, @(trans), syp->M, nL, nR, 
                        SVVAL((TYPE*)syp->alpha), B01, syp->ldb, A01, syp->lda, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, syp->M, nR, nL, 
                        SVVAL((TYPE*)syp->alpha), B, syp->ldb, A01, syp->lda, 
                        ONE, C01, syp->ldc);
   }
}

static void ATL_t@(rt)_SYsplit
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc,
    ATL_CINT nb)
/*
 * This routine is specialized for the case where we cannnot split the
 * B matrix, and must instead split the symmetric matrix (A).  It calls
 * a recursive GEMM-based BLAS, that gets its parallel performance from
 * calling threaded GEMM.
 */
{
   ATL_TSYMM_t ss;
   ss.side = Side;
   ss.uplo = Uplo;
   ss.M = M;
   ss.N = N;
   ss.nb = nb;
   ss.alpha = SADD alpha;
   ss.beta  = SADD beta;
   ss.lda = lda;
   ss.ldb = ldb;
   ss.ldc = ldc;
   if (Side == AtlasLeft)
      ATL_@(rt)L_rec(&ss, M/nb, M%nb, N/nb, N%nb, A, B, C);
   else
      ATL_@(rt)R_rec(&ss, M/nb, M%nb, N/nb, N%nb, A, B, C);

}

/*
 * The XOVER is the min # of blocks required to do parallel operation
 */
#ifndef ATL_TSYMM_XOVER
   #ifdef ATL_TGEMM_XOVER
      #define ATL_TSYMM_XOVER ATL_TGEMM_XOVER
   #else
      #define ATL_TSYMM_XOVER 4  /* want 4 blocks for parallel execution */
   #endif
#endif
/*
 * Once you have achieved enough blocks to do computation, minimum number
 * of blocks to give each processor
 */
#ifndef ATL_TSYMM_ADDP
   #ifdef ATL_TGEMM_ADDP 
      #define ATL_TSYMM_ADDP  ATL_TGEMM_ADDP 
   #else
      #define ATL_TSYMM_ADDP  1  /* want 1 blocks to add proc to workers */
   #endif
#endif
void Mjoin(PATL,t@(rt))
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   ATL_INT n, nblks, tblks, nr, minblks, extrablks, p, i, j;
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_TSYMM_t symms[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   const TYPE *b;
   TYPE *c;
   static int nb=0;

   if (M < 1 || N < 1)
      return;
   if (SCALAR_IS_ZERO(alpha))
   {
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,gescal)(M, N, beta, C, ldc);
      return;
   }
   if (!nb) nb = Mjoin(PATL,GetNB());
   if (Side == AtlasLeft)
   {
      nblks = N / nb;
      nr = N - nblks*nb;
      tblks = ((double)(M*N)) / ( (double)nb * nb );
      p = (nblks+ATL_TSYMM_ADDP-1)/ATL_TSYMM_ADDP;
      if (p < ATL_NTHREADS)  /* N not big enough to give blk to each proc */
      {
/*
 *       If I can't split N, and M is the dominant cost, use recursion to
 *       decompose symmetric matrix; parallelism will come from TGEMM calls
 */
         if (M > (N<<(ATL_NTHRPOW2+2)))
         {
            ATL_t@(rt)_SYsplit(Side, Uplo, M, N, alpha, A, lda, B, ldb, 
                              beta, C, ldc, nb);
            return;
         }
      }
      else
         p = ATL_NTHREADS;
      if (p < 2)
         goto SERIAL;
/*
 *    Distribute N over the processors
 */
      b = B;
      c = C;
      minblks = nblks / p;
      extrablks = nblks - minblks*p;
      for (i=0; i < p; i++)
      {
         if (i < extrablks)
            n = (minblks+1)*nb;
         else if (i == extrablks)
            n = minblks*nb + nr;
         else
            n = minblks*nb;
@skip         j = ATL_launchorder[i];
         symms[i].A = A;
         symms[i].B = b;
         symms[i].alpha = SADD alpha;
         symms[i].beta = SADD beta;
         symms[i].C = c;
         symms[i].M = M;
         symms[i].N = n;
         symms[i].lda = lda;
         symms[i].ldb = ldb;
         symms[i].ldc = ldc;
         symms[i].side = Side;
         symms[i].uplo = Uplo;
         b = MindxT(b, ATL_MulBySize((size_t)ldb)*n);
         c = MindxT(c, ATL_MulBySize((size_t)ldc)*n);
      }
      for (; i < ATL_NTHREADS; i++)  /* flag rest of struct as uninitialized */
         symms[i].M = 0;
   }
   else  /* Side == AtlasRight */
   {
      nblks = M / nb;
      nr = M - nblks*nb;
      tblks = ((double)(M*N)) / ( (double)nb * nb );
      p = (nblks+ATL_TSYMM_ADDP-1)/ATL_TSYMM_ADDP;
      if (p < ATL_NTHREADS)  /* N not big enough to give blk to each proc */
      {
/*
 *       If I can't split M, and N is the dominant cost, use recursion to
 *       decompose symmetric matrix; parallelism will come from TGEMM calls
 */
         if (N > (M<<(ATL_NTHRPOW2+2)))
         {
            ATL_t@(rt)_SYsplit(Side, Uplo, M, N, alpha, A, lda, B, ldb, 
                              beta, C, ldc, nb);
            return;
         }
      }
      else
         p = ATL_NTHREADS;
      if (p < 2)
         goto SERIAL;
/*
 *    Distribute M over the processors
 */
      b = B;
      c = C;
      minblks = nblks / p;
      extrablks = nblks - minblks*p;
      for (i=0; i < p; i++)
      {
         if (i < extrablks)
            n = (minblks+1)*nb;
         else if (i == extrablks)
            n = minblks*nb + nr;
         else
            n = minblks*nb;
@skip         j = ATL_launchorder[i];
         symms[i].A = A;
         symms[i].B = b;
         symms[i].alpha = SADD alpha;
         symms[i].beta = SADD beta;
         symms[i].C = c;
         symms[i].M = n;
         symms[i].N = N;
         symms[i].lda = lda;
         symms[i].ldb = ldb;
         symms[i].ldc = ldc;
         symms[i].side = Side;
         symms[i].uplo = Uplo;
         b = MindxT(b, ATL_MulBySize((size_t)n));
         c = MindxT(c, ATL_MulBySize((size_t)n));
      }
      for (; i < ATL_NTHREADS; i++)  /* flag rest of struct as uninitialized */
         symms[i].M = 0;
   }
   if (p < 2)
   {
SERIAL:
      Mjoin(PATL,@(rt))(Side, Uplo, M, N, alpha, A, lda, B, ldb, beta, C, ldc);
      return;
   }
   ATL_goparallel(p, Mjoin(PATL,DoWork@up@(rt)), symms, NULL);
@beginskip
   ls.opstruct = (char*) symms;
   ls.opstructstride = (int) ( ((char*)(symms+1)) - (char*)(symms) );
   ls.CombineOpStructs = NULL;
   ls.OpStructIsInit = Mjoin(PATL,StructIsInit@up@(rt));
   ls.DoWork = Mjoin(PATL,DoWork@up@(rt));
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
@endskip
}

@ROUT ATL_ttrmm
   @define rt @trmm@
@ROUT ATL_ttrsm
   @define rt @trsm@
@ROUT ATL_ttrsm ATL_ttrmm
#include "atlas_misc.h"
@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
int Mjoin(PATL,StructIsInit@up@(rt))(void *vp)
{
   return(((ATL_TTRSM_t*)vp)->B != NULL);
}

@beginskip
void Mjoin(PATL,tvtrsm)(ATL_TTRSM_t *tp)
{
   Mjoin(PATL,trsm)(tp->side, tp->uplo, tp->TA, tp->diag, tp->M, tp->N,
                    SVVAL((TYPE*)tp->alpha), tp->A, tp->lda, tp->B, tp->ldb);
}
@endskip

void Mjoin(PATL,DoWork@up@(rt))(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *thp=vp;
   ATL_TTRSM_t *tp=((ATL_TTRSM_t*)lp->opstruct) + thp->rank;
   Mjoin(PATL,@(rt))(tp->side, tp->uplo, tp->TA, tp->diag, tp->M, tp->N,
                    SVVAL((TYPE*)tp->alpha), tp->A, tp->lda, tp->B, tp->ldb);
}

#ifndef ATL_TTRSM_XOVER
   #define ATL_TTRSM_XOVER 4   /* want 4 total blocks before adding proc */
#endif
void Mjoin(PATL,t@(rt))(const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo, 
                       const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
                       ATL_CINT M, ATL_CINT N, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
@skip   ATL_thread_t tp[ATL_NTHREADS];
   ATL_TTRSM_t trsms[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
   TYPE *b;
   ATL_INT n, nblks, minblks;
   double tblks;
   int nr, p, i, j, extrablks;
   static int nb=0;

   if (M < 1 || N < 1)
      return;
   if (SCALAR_IS_ZERO(alpha))
   {
      Mjoin(PATL,gezero)(M, N, B, ldb);
      return;
   }
/*
 * Distribute RHS over the processors
 */
   if (!nb) nb = Mjoin(PATL,GetNB)();
   if (side == AtlasLeft)
   {
      nblks = N/nb;
      nr = N - nblks*nb;
      tblks = ((double)(M*N)) / ( (double)nb * nb );
      p = (tblks+ATL_TTRSM_XOVER-1)/ATL_TTRSM_XOVER;
      p = Mmin(p, ATL_NTHREADS);
      p = p ? p : 1;

      b = B;
      minblks = nblks / p;
      extrablks = nblks - minblks*p;
      for (i=0; i < p; i++)
      {
         if (i < extrablks)
            n = (minblks+1)*nb;
         else if (i == extrablks)
            n = minblks*nb + nr;
         else 
            n = minblks*nb;
@skip         j = ATL_launchorder[i];
@skip         trsms[j].eltsh = Mjoin(PATL,shift);
@skip         trsms[j].trsmK = Mjoin(PATL,tvtrsm);
         trsms[i].A = A;
         trsms[i].M = M;
         trsms[i].N = n;
         trsms[i].lda = lda;
         trsms[i].ldb = ldb;
         trsms[i].B = b;
         trsms[i].alpha = SADD alpha;
         trsms[i].side = side;
         trsms[i].uplo = uplo;
         trsms[i].TA   = TA;
         trsms[i].diag = diag;
         n *= (ldb << Mjoin(PATL,shift));
         b = MindxT(b, n);
      }
   }
   else /* Side == AtlasRight */
   {
      nblks = M/nb;
      nr = M - nblks*nb;
      tblks = (N/nb)*nblks;
      p = (tblks+ATL_TTRSM_XOVER-1)/ATL_TTRSM_XOVER;
      p = Mmin(p, ATL_NTHREADS);
      p = p ? p : 1;

      b = B;
      minblks = nblks / p;
      extrablks = nblks - minblks*p;
      for (i=0; i < p; i++)
      {
         if (i < extrablks)
            n = (minblks+1)*nb;
         else if (i == extrablks)
            n = minblks*nb + nr;
         else 
            n = minblks*nb;
@skip         j = ATL_launchorder[i];
@skip         trsms[j].eltsh = Mjoin(PATL,shift);
@skip         trsms[j].trsmK = Mjoin(PATL,tvtrsm);
         trsms[i].A = A;
         trsms[i].M = n;
         trsms[i].N = N;
         trsms[i].lda = lda;
         trsms[i].ldb = ldb;
         trsms[i].B = b;
         trsms[i].alpha = SADD alpha;
         trsms[i].side = side;
         trsms[i].uplo = uplo;
         trsms[i].TA   = TA;
         trsms[i].diag = diag;
         n <<= Mjoin(PATL,shift);
         b = MindxT(b, n);
      }
   }
   if (p < 2)
   {
      Mjoin(PATL,@(rt))(side, uplo, TA, diag, M, N, alpha, A, lda, B, ldb);
      return;
   }
   for (; i < ATL_NTHREADS; i++)  /* flag rest of struct as uninitialized */
      trsms[i].B = NULL;  
   ATL_goparallel(p, Mjoin(PATL,DoWork@up@(rt)), trsms, NULL);
@beginskip
   ls.opstruct = (char*) trsms;
   ls.opstructstride = (int) ( ((char*)(trsms+1)) - (char*)(trsms) );
   ls.CombineOpStructs = NULL;
   ls.OpStructIsInit = Mjoin(PATL,StructIsInit@up@(rt));
   ls.DoWork = Mjoin(PATL,DoWork@up@(rt));
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
@endskip
}
@ROUT ATL_Xtsyr2k
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
                  
static int ATL_tsyr2kK(ATL_SYR2K_t *syp, ATL_CINT N, ATL_CINT K, 
                       const void *A, const void *B, void *C)
/*
 * Attempts to allocate workspace W, then do:
 *   (1) W = alpha*A*B' or alpha*A'B (GEMM)
 *   (2) C <- beta*C + W + W'
 * RETURNS: 0 on success, nonzero if unable to allocate memory
 */
{
   void *v, *W;
   ATL_INT ldw;
   int i;
   size_t sz;
   const int eltsh = syp->eltsh;

/*
 * Make ldw a multiple of 4 that is not a power of 2
 */
   ldw = ((N+3)>>2)<<2;
   if (!(ldw&(ldw-1)))
      ldw += 4;
@beginskip
   for (i=0; i <= sizeof(ldw)*8; i++)
   {
      if (!(ldw^(1<<i)))
      {
         ldw += 4;
         break;
      }
   }
@endskip
   sz = (ldw*N)<<eltsh;
   if (sz <= ATL_NTHREADS*ATL_PTMAXMALLOC)
      v = malloc(sz + ATL_Cachelen);
   if (!v)
      return(1);  /* signal we can't get memory */

   W = ATL_AlignPtr(v);
   syp->tvgemm(syp->TA, syp->TB, N, N, K, syp->alpha, A, syp->lda, B, syp->ldb,
               syp->zero, W, ldw);
   syp->tvApAt(syp->Uplo, N, W, ldw, syp->beta, C, syp->ldc);
   free(v);
   return(0);
}

void ATL_tvsyr2k_rec
   (ATL_SYR2K_t *syp, ATL_CINT Nblks, ATL_CINT nr, const void *A, 
    const void *B, void *C)
/*
 * Do SYR2K/HER2K, either by mallocing space and calling GEMM, or recurring
 * until C is small enough that space can be allocated.  Gets its parallelism
 * from the calls to parallel GEMM
 */
{
   const int nb = syp->nb, eltsh = syp->eltsh;
   ATL_INT nL, nR, nbL, nbR, rL, rR;
   void *gc, *sc;   /* ptr to C to update with gemm & 2nd syr2k call, resp */
   void *A1, *B1;   /* ptr to 2nd block of a/b resp */
/*
 * Attempt to halt recursion by allocating workspace, and calling GEMM
 */
   if (!ATL_tsyr2kK(syp, Nblks*nb+nr, syp->K, A, B, C))
      return;
   ATL_assert(Nblks>1 || (Nblks==1 && nr));  /* must have something to split */
/*
 * Must recur in order to make problem small enough to allocate C workspace
 */
   nbR = Nblks>>1;
   nbL = Nblks - nbR;
   rL = (nL == nR) ? nr : 0;
   rR = nr - rL;
   nL = nbL*nb + rL;
   nR = nbR*nb + rR;
   sc = MindxT(C, (((size_t)nL*(syp->ldc+1))<<eltsh));
   if (syp->trans == AtlasNoTrans)
   {
      A1 = MindxT(A, ((size_t)nL<<eltsh));
      B1 = MindxT(B, ((size_t)nL<<eltsh));
   }
   else  /* index like transpose */
   {
      A1 = MindxT(A, (((size_t)nL*syp->lda)<<eltsh));
      B1 = MindxT(B, (((size_t)nL*syp->ldb)<<eltsh));
   }

   ATL_tvsyr2k_rec(syp, nbL, rL, A, B, C);
   if (syp->Uplo == AtlasUpper)
   {
      gc = MindxT(C, (((size_t)nL*syp->ldc)<<eltsh));
      syp->tvgemm(syp->TA, syp->TB, nL, nR, syp->K, syp->alpha, A, syp->lda, 
                  B1, syp->ldb, syp->beta, gc, syp->ldc);
      syp->tvgemm(syp->TA, syp->TB, nL, nR, syp->K, syp->alpha2, B, syp->ldb, 
                  A1, syp->lda, syp->one, gc, syp->ldc);
   }
   else
   {
      gc = MindxT(C, ((size_t)nL<<eltsh));
      syp->tvgemm(syp->TA, syp->TB, nR, nL, syp->K, syp->alpha, A1, syp->lda, 
                  B, syp->ldb, syp->beta, gc, syp->ldc);
      syp->tvgemm(syp->TA, syp->TB, nR, nL, syp->K, syp->alpha2, B1, syp->ldb, 
                  A, syp->lda, syp->one, gc, syp->ldc);
   }
   ATL_tvsyr2k_rec(syp, nbR, rR, A1, B1, sc);

}

@ROUT ATL_tsyr2k
   @define rt @syr2k@
   @define ApA @syApAt@
   @define trans @AtlasTrans@
@ROUT ATL_ther2k
   @define rt @her2k@
   @define ApA @heApAc@
   @define trans @AtlasConjTrans@
@ROUT ATL_tsyr2k ATL_ther2k
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
#include "atlas_lvl3.h"

void Mjoin(PATL,tv@(ApA))(const enum ATLAS_UPLO Uplo, ATL_CINT N, const void *A,
                          ATL_CINT lda, const void *beta, void *C, ATL_CINT ldc)
{
   Mjoin(PATL,@(ApA))(Uplo, N, A, lda, SVVAL((const TYPE*)beta), C, ldc);
}

void Mjoin(PATL,t@(rt))
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
@ROUT ATL_tsyr2k
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
@ROUT ATL_ther2k
    const TYPE *B, ATL_CINT ldb, const TYPE beta0, TYPE *C, ATL_CINT ldc)
@ROUT ATL_tsyr2k ATL_ther2k
{
   ATL_SYR2K_t sy;
   #ifdef TREAL
      const TYPE ONE = ATL_rone, ZERO = ATL_rzero;
   #else
      const TYPE ONE[2]={ATL_rone, ATL_rzero}, ZERO[2]={ATL_rzero,ATL_rzero};
@ROUT ATL_ther2k 
      const TYPE alpha2[2]={*alpha,(alpha[1]!=ATL_rzero)?-alpha[1]:ATL_rzero};
      const TYPE beta[2] = {beta0, ATL_rzero};
@ROUT ATL_tsyr2k ATL_ther2k
   #endif

   if (N < 1)
      return;
   if (SCALAR_IS_ZERO(alpha) || K < 1)
   {
@ROUT ATL_ther2k 
      if (beta0 != ATL_rone)
         Mjoin(PATL,hescal)(Uplo, N, N, beta0, C, ldc);
@ROUT ATL_tsyr2k 
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,trscal)(Uplo, N, N, beta, C, ldc);
@ROUT ATL_tsyr2k ATL_ther2k
      return;
   }
/*
 * This call to serial is usually a performance optimization, but was actually
 * put in to avoid a failure in the LAPACK eigenvalue tests that is caused
 * by subtractive cancellation.  There is no bug, but our parallel algorithm
 * does the operations in a different order than small-case serial, and in
 * one case used by the lapack testers, this causes subtractive cancellation
 * to occur (the serial code can use different orders for differing problem
 * sizes).  Other problems will experience cancelation under the serial order,
 * so AFAIK, neither is more correct numerically.
 */
   if (N < 3*NB && K < 3*NB)
   {
@ROUT ATL_tsyr2k
      Mjoin(PATL,syr2k)(Uplo, Trans, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
@ROUT ATL_ther2k 
      Mjoin(PATL,her2k)(Uplo, Trans, N, K, alpha, A, lda, B, ldb, 
                        beta0, C, ldc);
@ROUT ATL_tsyr2k ATL_ther2k
      return;
   }
@ROUT ATL_tsyr2k
   sy.alpha2 = sy.alpha = SADD alpha;
   sy.beta  = SADD beta;
@ROUT ATL_ther2k
   sy.alpha = SADD alpha;
   sy.alpha2 = alpha2;
   sy.beta  = beta;
@ROUT ATL_tsyr2k ATL_ther2k
   sy.one = SADD ONE;
   sy.zero = SADD ZERO;
   sy.tvgemm = Mjoin(PATL,tvgemm);
   sy.tvApAt = Mjoin(PATL,tv@(ApA));
   sy.K = K;
   sy.lda = lda;
   sy.ldb = ldb;
   sy.ldc = ldc;
   sy.eltsh = Mjoin(PATL,shift);
   sy.Uplo = Uplo;
   sy.trans = Trans;
   if (Trans == AtlasNoTrans)
   {
      sy.TA = AtlasNoTrans;
      sy.TB = @(trans);
      sy.TA2 = @(trans);
      sy.TB2 = AtlasNoTrans;
   }
   else
   {
      sy.TA = @(trans);
      sy.TB = AtlasNoTrans;
      sy.TA2 = AtlasNoTrans;
      sy.TB2 = @(trans);
   }
   sy.nb = Mjoin(PATL,GetNB)();
   ATL_tvsyr2k_rec(&sy, N/sy.nb, N%sy.nb, A, B, C);
}

@beginskip
/*
 * NOTE: want to put these primitives in src/auxil, then I write higher
 *       level drivers that call them on blocks
 */
#ifdef TREAL
void Mjoin(PATL,putAAt_L)
   (ATL_CINT N, const TYPE beta, const TYPE *A, ATL_CINT lda, 
    TYPE *L, ATL_CINT ldl)
/*
 * L <- A+At, L lower triangular
 */
{
   TYPE *Ar, *Ac=A;
   ATL_INT i, j;

   for (j=0; j < N; j++)
   {
      for (Ar=A+j,i=j; i < M; i++, Ar += lda)
      #ifdef BETA0
         Cc[i] = Ac[i] + *Ar;
      #elif defined(BETA1)
         Cc[i] += Ac[i] + *Ar;
      #else
         Cc[i] = beta*C[i] + Ac[i] + *Ar;
      #endif
      Ac += lda;
   }
}
void Mjoin(PATL,putABt_L)
   (ATL_CINT M, ATL_CINT N, const TYPE beta, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, TYPE *C, ATL_CINT ldc)
/*
 * C <- beta*C + A + B'
 */
{
   TYPE *Br;
   for (j=0; j < N; j++)
   {
      for (Br=B,i=0; i < M; i++, Br += ldb)
      #ifdef BETA0
         Cc[i] = A[i] + *Br;
      #elif defined(BETA1)
         Cc[i] += A[i] + *Br;
      #else
         Cc[i] = beta*C[i] + A[i] + *Br;
      #endif
      C += ldc;
      A += lda;
      B++;
   }
}
#else
#endif

void Mjoin(PATL,vsyr2k_putL)(ATL_CINT N, const void *beta0, const void *A, 
                             ATL_CINT lda, void *C, ATL_CINT ldc)
/* 
 * Takes A with property (A + A') = (A + A')', 
 *    C <- beta*C + A + A'
 * NOTE: This kernel is unblocked for initial try, which could cause TLB 
 *        disaster.  Need to write blocked version, and perhaps thread.
 */
{
   const TYPE *Ac = A, *Ar;
   const TYPE beta = *((const TYPE*)beta0);
   TYPE *Cc = C;
   ATL_CINT i, j;

   if (beta == ATL_rzero)
   {
      for (j=0; j != N; j++, Ac += lda, Cc += ldc)
      {
         for (Ar=Ac+j, i=j; i != N; i++, Ar += lda)
            Cc[i] = Ac[i] + *Ar;
      }
   }
   else if (beta == ATL_rone)
   {
      for (j=0; j != N; j++, Ac += lda, Cc += ldc)
      {
         for (Ar=Ac+j, i=j; i != N; i++, Ar += lda)
            Cc[i] += Ac[i] + *Ar;
      }
   }
   else
   {
      for (j=0; j != N; j++, Ac += lda, Cc += ldc)
      {
         for (Ar=Ac+j, i=j; i != N; i++, Ar += lda)
            Cc[i] = beta*Cc[i] + Ac[i] + *Ar;
      }
   }
}
@endskip
@ROUT ATL_Xtsyrk
#include "atlas_misc.h"
@skip #define ATL_LAUNCHORDER
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
#include "math.h"
/*
 * Recursive decompositon on trapazoidal-shaped matrix ($C$ after splitting)
 */
#ifndef ATL_MINL3THRFLOPS
   #ifdef ATL_TGEMM_ADDP
      #define ATL_MINL3THRFLOPS \
         (((2.0*ATL_TGEMM_ADDP)*ATL_TGEMM_ADDP)*ATL_TGEMM_ADDP)
   #else
      #define ATL_MINL3THRFLOPS (((2.0*MB)*NB)*KB)
   #endif
#endif
@beginskip
int ATL_tsyrkdecomp_tr
   (ATL_TSYRK_t *psyrk, const int P, ATL_CINT Tblks, ATL_CINT tr, 
    ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr, ATL_CINT K,
    ATL_CINT ia, ATL_CINT ja, ATL_CINT ib, ATL_CINT jb, 
    ATL_CINT ic, ATL_CINT jc)
{
   double flops;
   double percL;  /* % of calculation to do on left size */
   int pL, pR; 
   ATL_INT i, nL, nR, rL, rR;
   const int nb = psyrk->nb;
  
   pR = (P>>1);
   pL = P - pR;
@skip pL=1; pR=P-1;   /* HERE HERE HERE: debug */
   percL = (pL == pR) ? 0.5 : ((double)pL)/((double)P);
   
/*
 * If problem is triangular, divide up problem so LEFT does SYRK+GEMM, and
 * RIGHT does SYRK only; this means nL = T(1-sqrt(percentL))
 */
   if (!Mblks && !Nblks)
   {
@skip      nL = 0.5 + ((pL == pR) ? 0.29289322*Tblks : 2.0*percL*Tblks*(1.0-0.5*sqrt(2)));
      nL = (0.58578664*Tblks)*percL;
      nR = Tblks - nL;
      flops = nR*nb;
      flops *= flops;
      flops *= K;
      if (P < 2 || !nL || !nR || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
fprintf(stderr, "FLOPS=%.2f\n", (1.0*psyrk->T)*psyrk->T);
         psyrk->M = psyrk->N = 0;
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
      rL = (nL > nR) ? 0 : tr;
      rR = tr - rL;
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      i = ATL_tsyrkdecomp_tr(psyrk, pL, nL, rL, nR, rR, nL, rL, K, 
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, nR, rR, 0, 0, 0, 0, K,
                              ia+nL*nb+rL, ja, ib, jb+nL*nb+rL, 
                              ic+nL*nb+rL, jc+nL*nb+rL);
      return(i);
   }
/*
 * Only divide M if all the SYRK flops can be done in LEFT's work.
 * Divides gemm's M asymmetrically to match the SYRK flops
 */
   if (Mblks>=Nblks)
   {
      if (Tblks)
         ATL_assert(Nblks == Tblks && nr == tr);
      nL = 0.5*percL*(Mblks+Mblks-Tblks);
      if (nL < 1) nL = 1;
      nR = Mblks - nL;
      flops = 2.0*nR*nb*Nblks*nb*K;
      if (P < 2 || !nL || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
         psyrk->M = Mblks*nb + mr;
         psyrk->N = Nblks*nb + nr;
fprintf(stderr, "T=%d, M-%d, N=%d, FLOPS=%.2f\n", psyrk->T, psyrk->M, psyrk->N,
        (1.0*psyrk->T)*psyrk->T + (2.0*psyrk->M)*psyrk->N);
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      rL = (!Tblks && nL <= nR) ? mr : 0;
      rR = mr - rL;
      i = ATL_tsyrkdecomp_tr(psyrk, pL, Tblks, tr, nL, rL, Nblks, nr, K,
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, 0, 0, nR, rR, Nblks, nr, K,
                              ia+(nL+Tblks)*nb+rL+tr, ja, ib, jb, 
                              ic+(nL+Tblks)*nb+rL+tr, jc);
      return(i);
   }
/*
 * As a last choice, cut both GEMM and SYRK (N & T) together
 * Must be done asymmetrically to balance differently sized gemms
 */
   if (Nblks && Tblks && nr == tr) /* divide N & T*/
   {
      nL = 2.0*percL*(Nblks+Mblks-
         sqrt((0.5*Nblks*Nblks)+((double)Nblks)*Mblks+((double) Mblks)*Mblks));
      if (nL < 1) nL = 1;
      nR = Nblks - nL;
      flops = nR * nb;
      flops = flops*flops + (2.0*Mblks)*(nb*flops);
      flops *= K;
      if (P < 2 || !nL || !nR || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
         psyrk->M = Mblks*nb + mr;
         psyrk->N = Nblks*nb + nr;
fprintf(stderr, "bT=%d, M-%d, N=%d, FLOPS=%.2f\n", psyrk->T, psyrk->M, psyrk->N,
        (1.0*psyrk->T)*psyrk->T + (2.0*psyrk->M)*psyrk->N);
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      i = ATL_tsyrkdecomp_tr(psyrk, pL, nL, 0, Mblks+nR, tr, nL, 0, K,
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, nR, tr, Mblks, mr, nR, tr, K,
                              ia+nL*nb, ja, ib, jb+nL*nb, ic+nL*nb, jc+nL*nb);
      return(i);
   }
   else  /* dividing a GEMM on N-dimension only */
   {
      ATL_assert(!Tblks && !tr && Nblks >= Mblks);
      nL = percL*Nblks + 0.5;
      nR = Nblks - nL;
      flops = ((2.0*nR*nb)*Mblks)*nb*K;
      if (P < 2 || !nL || !nR || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
         psyrk->M = Mblks*nb + mr;
         psyrk->N = Nblks*nb + nr;
fprintf(stderr, "bT=%d, M-%d, N=%d, FLOPS=%.2f\n", psyrk->T, psyrk->M, psyrk->N,
        (1.0*psyrk->T)*psyrk->T + (2.0*psyrk->M)*psyrk->N);
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      rL = (nL > nR) ? 0 : nr;
      rR = nr - rL;
      i = ATL_tsyrkdecomp_tr(psyrk, pL, 0, 0, Mblks, mr, nL, rL, K, 
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, 0, 0, Mblks, mr, nR, rR, K,
                              ia, ja, ib, jb+nL*nb+rL, ic, jc+nL*nb+rL);
      return(i);
   }
}

#include <string.h>
void SortSYRKByFlopCount(int P, ATL_TSYRK_t *syp)
{
   ATL_TSYRK_t stmp;
   int i, j, ib, jj;
   double mf0, mf;

   for (i=0; i < P-1; i++)
   {
      ib = ATL_launchorder[i];
      if (syp[ib].ia < 0)
         continue;
      mf0 = ((double)syp[ib].T)*syp[ib].T + (2.0*syp[ib].M)*syp[ib].N;
      for (j=i+1; j < P; j++)
      {
         jj = ATL_launchorder[j];
         if (syp[jj].ia < 0)
            continue;
         mf = ((double)syp[jj].T)*syp[jj].T + (2.0*syp[jj].M)*syp[jj].N;
         if (mf > mf0)
         {
            mf0 = mf;
            ib = jj;
         }
      }
      jj = ATL_launchorder[i];
      if (ib != jj)
      {
         memcpy(&stmp, syp+ib, sizeof(ATL_TSYRK_t));
         memcpy(syp+ib, syp+jj, sizeof(ATL_TSYRK_t));
         memcpy(syp+jj, &stmp, sizeof(ATL_TSYRK_t));
      }
   }
}
int ATL_StructIsInitSYRK(void *vp)
{
   return( ((ATL_TSYRK_t*)vp)->ia >= 0 );
}

void ATL_DoWorkSYRK(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_t *syp = ((ATL_TSYRK_t*)lp->opstruct)+tp->rank;
   const int lda = syp->lda, ldc = syp->ldc, eltsh = syp->eltsh;

   if (syp->T)
      syp->tvsyrk(syp->Uplo, syp->Trans, syp->T, syp->K, syp->alpha, 
                  syp->A+((syp->ia+syp->ja*lda)<<eltsh), lda, syp->beta,
                  syp->C+((syp->ic+syp->jc*ldc)<<eltsh), ldc);
   if (syp->M && syp->N)
      syp->tvgemm(AtlasNoTrans, AtlasTrans, syp->M, syp->N, syp->K, syp->alpha,
                  syp->A+((syp->ia+syp->T+syp->ja*lda)<<eltsh), lda,
                  syp->A+((syp->jb+syp->ib*lda)<<eltsh), lda, syp->beta,
                  syp->C+((syp->ic+syp->T+syp->jc*ldc)<<eltsh), ldc);
}
@endskip
@beginskip
#ifndef ATL_DoMMParallel
   #ifndef ATL_TGEMM_MINFLOPS
      #define ATL_TGEMM_MINFLOPS 512000.0
   #endif
   #define ATL_DoMMParallel(M_, N_, K_) \
      (((((double)(M_))*(N_))*(K_)) >= ATL_TGEMM_MINFLOPS)
#endif

int ATL_tsyrkdecomp_MM(ATL_TSYRK_N_t *psy, ATL_CINT nthr)
/*
 * This routine splits a gemm coming from SYRK until nthr is exhausted
 */
{
   int lo[ATL_NTHREADS];
   ATL_CINT M=psy->M, N=psy->N, K=psy->K, nb=psy->nb;
   ATL_INT m, nblks, nr, minblks, extrablks;
   const int eltsh=psy->eltsh, amul=(psy->TA == AtlasNoTrans) ? 1 : psy->lda;
   const int bmul = (psy->TB == AtlasNoTrans) ? 1 : psy->lda;
   int i, j, k, nt, np;
   const void *a;
   void *c;

   if (nthr == 1 || !psy->numthr(M, N, K))
      return(1);
   nt = nthr >> 1;
/*
 * If M is large enough, cut it for entire GEMM distribution in order to
 * optimize ATLAS's common JIK pattern
 */
   if (M >= nb*nthr*ATL_TMMMINMBLKS)
   {
/*
 *    Determine launchorder on this subset of processors
 */
      lo[0] = 0;
      for (i=0; (1<<i)^nthr; i++);
      lo[0] = 0;
      k=1;
      for (i--; i >= 0; i--)
      {
         for (j=0; j < k; j++)
            lo[k+j] = lo[j] + (1<<i);
         k += k;
      }
/*
 *    Find how many blocks we've got
 */
      nblks = M / nb;
      nr = M - nblks*nb;
      minblks = nblks / nthr;
      extrablks = nblks - minblks*nthr;
/*
 *    Get everyone a copy of entire data structure & assign subpieces
 */
      c = psy->C;
      a = psy->A0;
      for (i=0; i < nthr; i++)
      {
         j = lo[i];
         if (i) { McpSYN(psy, j, 0); }
         if (i < extrablks)
            m = (minblks+1)*nb;
         else if (i == extrablks)
            m = minblks*nb + nr;
         else
            m = minblks*nb;
         psy[j].M = m;
         psy[j].C = c;
         psy[j].A0 = a;
         m <<= eltsh;
         a = MindxT(a,m*amul);
         c = MindxT(c,m);
      }
      return(nthr);
   }
   else if (M >= N) /* split M */
   {
      McpSYN(psy, nt, 0);
      m = M>>1;
      psy[nt].M = m;
      psy->M = m = M - m;
      m <<= eltsh;
      psy[nt].A0 = MindxT(psy->A0,m*amul);
      psy[nt].C = MindxT(psy->C,m);
      np = ATL_tsyrkdecomp_MM(psy, nt);
      np += ATL_tsyrkdecomp_MM(psy+nt, nt);
      return(np);
   }
   else        /* split N */
   {
      McpSYN(psy, nt, 0);
      m = N>>1;
      psy[nt].N = m;
      psy->N = m = N-m;
      m <<= eltsh;
      psy[nt].C = MindxT(psy->C,m*psy->ldc);
      psy[nt].A1 = MindxT(psy->A1,m*amul);
      np = ATL_tsyrkdecomp_MM(psy, nt);
      np += ATL_tsyrkdecomp_MM(psy+nt, nt);
      return(np);
   }
}

int ATL_IsInitSYRK_N(void *vp)
{
   return( ((ATL_TSYRK_N_t*)vp)->K );
}

int ATL_tsyrkdecomp_N(ATL_TSYRK_N_t *psy, ATL_CINT nthr)
{
   void *vp;
   ATL_INT nL, nR, iL, iR;
   int nt, p;
   const int ISUPPER = (psy->Uplo == AtlasUpper), 
             ISNOTRANS = (psy->TA == AtlasNoTrans);

   if (nthr == 1)
      return(1);
   nt = nthr >> 1;  /* nthr is power of two, so both sides get nt threads */
/*
 * Copy present psy struct into new one, and then modify both as required
 */
   if (psy->C)   /* we are splitting two SYRKs */
   {
      if (!psy->numthr(psy->N, psy->N, psy->K))
         return(1);
      McpSYN(psy, nt, 0);   /* psy[nt] = psy[0] */
      psy[nt].T = psy->C;
      psy[nt].A0 = psy[nt].A1;
      psy[nt].C = psy->C = NULL;
      psy[nt].M = psy->N;
      psy->N = psy[nt].N = 0;
      p = ATL_tsyrkdecomp_N(psy, nt);
      p += ATL_tsyrkdecomp_N(psy+nt, nt);
      return(p);
   }
   else         /* we are splitting one SYRK into 2 SYRKS and one GEMM */
   {
      if (!psy->numthr(psy->M, (psy->M)>>1, psy->K))
         return(1);
      McpSYN(psy, nt, 0);   /* psy[nt] = psy[0] */
      nL = (psy->M)>>1;
      nR = psy->M - nL;
      iL = nL << psy->eltsh;
      iR = nR << psy->eltsh;
/*
 *    Give psy[0] both SYRKs to do, and continue splitting
 */
      psy->C = MindxT(psy->T, iL*(psy->ldc+1));
      psy->A1 = (ISNOTRANS) ? MindxT(psy->A0, iL) : MindxT(psy->A0,iL*psy->lda);
      psy->M = nL;
      psy->N = nR;
      p = ATL_tsyrkdecomp_N(psy, nt);
/*
 *    Give psy[nt] a GEMM to do, and call routine to split GEMMs
 */
      psy[nt].C = (ISUPPER) ? MindxT(psy[nt].T,iL*psy[nt].ldc) : 
                              MindxT(psy[nt].T,iL);
      psy[nt].T = NULL;
      if (ISUPPER)
      {
         psy[nt].M = nL;
         psy[nt].N = nR;
         psy[nt].A1 = (ISNOTRANS) ? 
            MindxT(psy[nt].A0,iL) : MindxT(psy[nt].A0,iL*psy[nt].lda);
      }
      else
      {
         psy[nt].M = nR;
         psy[nt].N = nL;
         if (ISNOTRANS)
         {
            psy[nt].A1 = psy[nt].A0;
            psy[nt].A0 = MindxT(psy[nt].A0,iL);
         }
         else  /* A is Transposed */
         {
            psy[nt].A1 = psy[nt].A0;
            psy[nt].A0 = MindxT(psy[nt].A0,iL*psy[nt].lda);
         }
      }
      p += ATL_tsyrkdecomp_MM(psy+nt, nt);
      return(p);
   }
}

void ATL_DoWorkSYRK_N(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_N_t *syp=((ATL_TSYRK_N_t*)lp->opstruct)+tp->rank;
   if (syp->T)  /* doing SYRKs */
   {
      syp->tvsyrk(syp->Uplo, syp->TA, syp->M, syp->K, syp->alpha, 
                  syp->A0, syp->lda, syp->beta, syp->T, syp->ldc);
      if (syp->C)
         syp->tvsyrk(syp->Uplo, syp->TA, syp->N, syp->K, syp->alpha, 
                     syp->A1, syp->lda, syp->beta, syp->C, syp->ldc);
   }
   else /* doing GEMM */
      syp->gemmK(syp->M, syp->N, syp->K, syp->alpha, syp->A0, syp->lda, 
                 syp->A1, syp->lda, syp->beta, syp->C, syp->ldc);
}
@endskip

int ATL_tsyrkdecomp_K
   (ATL_TSYRK_K_t *psyrk, 
    void (*syrkK)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT),
    int np, const int eltsh, const int nb, const void *zero, const void *one,
    const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, 
    ATL_CINT N, ATL_CINT Kblks, const int kr, 
    const void *alpha, const void *A, ATL_CINT lda,
    const void *beta, void *C, ATL_CINT ldc)
{
@skip   ATL_CINT minblks = Kblks / ATL_NTHREADS, 
@skip            extrablks = Kblks - minblks*ATL_NTHREADS;
   ATL_INT minblks, extrablks, j, k, ldcw;
   int i;

/*
 * Note that this routine is essentially for large K, so we don't consider
 * any K smaller than NB for a processor
 */
   minblks = Kblks / np;
   if (minblks)
      extrablks = Kblks - minblks*np;
   else
   {
      np = Kblks;
      minblks = 1;
      extrablks = 0;
   }
/*
 * Find a good ldcw: multiple of 4 that is not a power of two
 */
   ldcw = ((N+3)>>2)<<2;   /* multiple of 4 */
   if (!(ldcw&(ldcw-1)))
      ldcw += 4;
@beginskip
   for (i=0; i < sizeof(ldcw)*8; i++)
   {
      if (!((1<<i)^ldcw))  /* if it is a power of two this is eventually 0 */
      {
         ldcw += 4;
         break;
      }
   }
@endskip
   if ((ldcw<<eltsh)*N > ATL_PTMAXMALLOC)
      return(0);
   for (i=0; i < np; i++)
   {
      if (i < extrablks)
         k = (minblks + 1)*nb;
      else if (i == extrablks)
         k = minblks*nb + kr;
      else
         k = minblks * nb;
      j = N;
@skip      lo = ATL_launchorder[i];   /* use log2-launch order */
      psyrk[i].alpha = alpha;
      psyrk[i].beta  = beta ;
      psyrk[i].one   = one  ;
      psyrk[i].zero  = zero ;
      psyrk[i].Uplo = Uplo;
      psyrk[i].Trans = Trans;
      psyrk[i].N = N;
      psyrk[i].K = k;
      psyrk[i].A = A;
      psyrk[i].C = C;
      psyrk[i].lda = lda;
      psyrk[i].ldc = ldc;
      psyrk[i].eltsh = eltsh;
      if (!i)
         psyrk[0].nCw = psyrk[0].ldcw = 0;
      else
      {
         psyrk[i].nCw = 1;
         psyrk[i].ldcw = ldcw;
      }
      psyrk[i].Cw = NULL;
      psyrk[i].Cinfp[0] = psyrk + i;
      psyrk[i].tvsyrk = syrkK;
      k = (Trans == AtlasNoTrans) ? lda * k : k;
      k <<= eltsh;
      A = MindxT(A,k);
   }
   for (; i < ATL_NTHREADS; i++)
      psyrk[i].N = 0;
   return(np);
}

void ATL_tsyrk_K(ATL_TSYRK_K_t *syp, int np, ATL_CINT N, ATL_CINT K, 
                 const void *A, void *C)
{
   const int nb = syp->nb;
   void ATL_DoWorkSYRK_K(ATL_LAUNCHSTRUCT_t *lp, void *vp);

   if (np < 1 || Mmin(N,K) < 8)
      np = 1;
   else
      np = ATL_tsyrkdecomp_K(syp, syp->tvsyrk, np, syp->eltsh, nb, syp->zero,
                             syp->one, syp->Uplo, syp->Trans, N, K/nb, K%nb, 
                             syp->alpha, A, syp->lda, syp->beta, C, syp->ldc);
   if (np < 2)
   {
      syp->tvsyrk(syp->Uplo, syp->Trans, N, K, syp->alpha, A, syp->lda, 
                  syp->beta, C, syp->ldc);
      return;
   }
   ATL_goparallel(np, ATL_DoWorkSYRK_K, syp, syp->DoComb);
@beginskip
   ATL_thread_start(syp->lp->rank2thr, 0, 1, ATL_tlaunch, syp->lp->rank2thr);
   ATL_thread_join(syp->lp->rank2thr);
@endskip
}

void ATL_tsyrk_K_rec(ATL_TSYRK_K_t *syp, int np, ATL_CINT Nblks, ATL_CINT nr, 
                     ATL_CINT K, const void *A0, void *C00)
/*
 * This routine recurs on N until we can allocate the full NxN workspace,
 * at which point it stops the recursion and distributes K for parallel
 * operation
 */
{
   const enum ATLAS_TRANS TA = syp->Trans;
   ATL_CINT lda = syp->lda, ldc = syp->ldc, eltsh = syp->eltsh;
   ATL_CINT nb = syp->nb, N = Nblks*nb+nr;
   ATL_INT sz, nblksL, nblksR, nrL, nrR, nL, nR;
   const void *A1;
   void *C10, *C01, *C11;
/*
 * Stop recursion & call threaded SYRK if we can allocate workspace for all of C
 */
   sz = (N * N) << eltsh;
/*
 * Quit recurring if we can allocate space for C workspace and we can
 * no longer usefully split Nblks, or we can usefully split K
 */
   if (sz <= ATL_PTMAXMALLOC && (nb*ATL_NTHREADS < K || Nblks < ATL_NTHREADS))
   {
      ATL_tsyrk_K(syp, np, Nblks*nb+nr, K, A0, C00);
      return;
   }
   nblksL = (Nblks+1)>>1;
   nblksR = Nblks - nblksL;
   if (nblksL >= nblksR)
   {
      nrL = nr;
      nrR = 0;
   }
   else
   {
      nrL = 0;
      nrR = nr;
   }

   nL = nblksL * nb + nrL;
   nR = nblksR * nb + nrR;
   if (syp->Uplo == AtlasUpper) 
   {
      sz = nL<<eltsh;
      C01 = MindxT(C00,sz*ldc);
      A1 = (TA == AtlasNoTrans) ? MindxT(A0,sz) : MindxT(A0,sz*lda);
      C11 = MindxT(C01,sz);
      ATL_tsyrk_K_rec(syp, np, nblksL, nrL, K, A0, C00);
      syp->gemmT(syp->Trans, syp->TB, nL, nR, K, syp->alpha, A0, lda, A1, lda, 
                 syp->beta, C01, ldc);
      ATL_tsyrk_K_rec(syp, np, nblksR, nrR, K, A1, C11);
   }
   else /* Lower triangular matrix */
   {
      sz = nL<<eltsh;
      C10 = MindxT(C00,sz);
      A1 = (TA == AtlasNoTrans) ? MindxT(A0,sz) : MindxT(A0,sz*lda);
      sz += (ldc*nL)<<eltsh;
      C11 = MindxT(C00,sz);
      ATL_tsyrk_K_rec(syp, np, nblksL, nrL, K, A0, C00);
      syp->gemmT(syp->Trans, syp->TB, nR, nL, K, syp->alpha, A1, lda, A0, lda, 
                 syp->beta, C10, ldc);
      ATL_tsyrk_K_rec(syp, np, nblksR, nrR, K, A1, C11);
   }
}

void ATL_DoWorkSYRK_K(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_K_t *syp = ((ATL_TSYRK_K_t *)lp->opstruct)+tp->rank;
/*
 * Allocate space if needed, and then do SYRK into it
 */
   if (syp->nCw)
   {
      syp->Cw = malloc((syp->ldcw << syp->eltsh)*syp->N+ATL_Cachelen);
      if (syp->Cw)
         syp->tvsyrk(syp->Uplo, syp->Trans, syp->N, syp->K, syp->alpha, syp->A,
                     syp->lda, syp->zero, ATL_AlignPtr(syp->Cw), syp->ldcw);
   }
   else /* do SYRK directly into original C: no poss of failure */
      syp->tvsyrk(syp->Uplo, syp->Trans, syp->N, syp->K, syp->alpha, 
                  syp->A, syp->lda, syp->beta, syp->C, syp->ldc);
}

int ATL_IsInitSYRK_K(void *vp)
{
   return( ((ATL_TSYRK_K_t*)vp)->N );
}

@ROUT tsttr
#include <stdio.h>
#include <stdlib.h>
#ifndef ATL_MU
   #define ATL_MU 4
#endif
#ifndef ATL_NTHREADS
   #define ATL_NTHREADS 8
#endif
#ifndef NB
   #define NB 56
#endif
#define ATL_INT int
#define ATL_CINT int
#define ATL_MINL3THRFLOPS (2.0*NB*NB*NB)
#define ATL_TGEMM_PERTHR_MF (2.0*NB*NB*NB)
@ROUT ATL_Xtsyrk tsttr
int ATL_tsyrkdecomp_tr1D(int P, ATL_CINT N, ATL_CINT K, 
                         ATL_CINT nb, ATL_CINT mu, double minmf, ATL_INT *Ms)
/*
 * Partitions triangular matrix from SYRK into roughly equal flop count
 * regions, with the first such region being strictly triangular, and the
 * rest trapazoidal row-panels.
 * Ms : must be of length P at least, on output contains the correct size
 *      matrix to give to each processor.
 * RETURNS: number of processors used
 */
{
   double Pflops, myflops, tflops, pinv;
   const int incM = (nb >= 60) ? ((24+mu-1)/mu)*mu : 
                    ((nb < 16) ? nb : ((16+mu-1)/mu)*mu);
   ATL_INT n, m, j;
   int k, p;

   for (k=0; k < P; k++)
      Ms[k] = 0;
   tflops = (((double)N)*N)*K;
   while (P && (Pflops = tflops/((double)P)) < minmf) P--;
   if (P < 2)
      return(0);

   tflops /= K;
/*
 * For each processor, find m that balances the flop count
 */
   for (n=p=0; p < P; p++)
   {
      Pflops = tflops / (P-p);
      if (tflops*K < minmf)
      {
         if (p < 2)
            return(0);
         Ms[p-1] += N - n;
         return(p);
      }
/*
 *    Finds the largest m that is a multiple of nb that generates <= Pflops
 */
      m = nb;  /* number of rows in row-panel */
      k = 1;   /* number of blocks in m */
      do
      {
         myflops = m;
         myflops *= myflops + n + n;
         if (myflops == Pflops)
            break;
         else if (myflops > Pflops)
         {
            m -= nb;
            k--;
            myflops = m;
            myflops *= myflops + n + n;
            break;
         }
         m += nb;
         k++;
      }
      while (1);
/*
 *    If we are below target flop count, see how to adjust
 */
      if (myflops < Pflops)
      {
         j = (k < 4) ? incM : mu;  /* for small M, don't tolerate cleanup */
         while ((((double)m)*((((double)m)+n)+n)) < Pflops) m += j;
         myflops = (((double)m)*((((double)m)+n)+n));
      }
      j = N - n;
      if (m >= j)
      {
         if (j < incM)
         {
            if (p < 1)
               return(0);
            Ms[p-1] += j;
            return(p);
         }
         Ms[p] = j;
         return(p+1);
      }
      else if (p == P-1)
         m = N - n;
      n += m;
      Ms[p] = m;
      tflops -= myflops;
   }
   return(p);
}
@ROUT tsttr
int main(int nargs, char **args)
{
   int N=1000, K=1000;
   int i, p, n;
   ATL_INT Ms[ATL_NTHREADS];
   double myflops, tflops;
   if (nargs > 1)
      N = atoi(args[1]);
   if (nargs > 2)
      K = atoi(args[2]);
   p = ATL_tsyrkdecomp_tr1D(ATL_NTHREADS, N, K, NB, ATL_MU, 
                            ATL_TGEMM_PERTHR_MF, Ms);
   printf("\n\nN=%d, K=%d:\n", N, K);
   if (p < 1)
      printf("   Unable to distribute!\n");
   else
   {
      tflops = (((double)N)*N)*K;
      printf("   P       M       N       FLOPS\n");
      printf("====  ======  ======  ==========\n\n");
      n = 0;
      for (i=0; i < p; i++)
      {
         myflops = (((double)Ms[i])*(Ms[i]+n+n))*K;
         printf("%4d %7d %7d %.0f (%.2f)\n", i, Ms[i], N, 
                myflops, myflops/tflops);
                
         n += Ms[i];
      }
      printf("Total M = %d\n\n", n);
   }
   return(0);
}
@ROUT ATL_Xtsyrk
int ATL_IsInitSYRK_M(void *vp)
{
   return( ((ATL_TSYRK_M_t*)vp)->K );
}

int ATL_tsyrkdecomp_M
(
   ATL_TSYRK_M_t *syp,          /* output: parallel decomposition structs */
   const enum ATLAS_UPLO Uplo,
   const enum ATLAS_TRANS TA,
   ATL_CINT N, ATL_CINT K,      /* original problem size */
   const void *alpha,
   const void *A,
   ATL_CINT lda,
   const void *beta,
   void *C,
   ATL_CINT ldc,
   ATL_CINT nb,                 /* MB of GEMM kernel */
   const int mu,                /* reg blking factor along M of MM kernel */
   const int eltsh,
   const enum ATLAS_TRANS TB,   /* Dual of TA (Conj for herk, trans for syrk) */
   double minmf,
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT),
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT)
)
{
   ATL_INT Ms[ATL_NTHREADS];
   int k, j, p;
   ATL_CINT incA = lda << eltsh, incC = (ldc+1) << eltsh;
   ATL_INT n, m, JJ;
   const int ISNOTRANS = (TA == AtlasNoTrans);

   p = ATL_tsyrkdecomp_tr1D(ATL_NTHREADS, N, K, nb, mu, minmf, Ms);
   if (p < 2)
      return(0);
   if (Uplo == AtlasLower)
   { 
      n = 0;
      for (k=0; k < p; k++)
      {
@skip         j = ATL_launchorder[k];
         m = Ms[k];
         syp[k].gemmK = gemmK;
         syp[k].tvsyrk = tvsyrk;
         syp[k].alpha = alpha;
         syp[k].beta  = beta ;
         syp[k].K = K;
         syp[k].lda = lda;
         syp[k].ldc = ldc;
         syp[k].nb = nb;
         syp[k].eltsh = eltsh;
         syp[k].Uplo = Uplo;
         syp[k].TA = TA;
         syp[k].TB = TB;
         syp[k].M = m;
         syp[k].N = n;
         syp[k].T = MindxT(C,((size_t)n*incC));
         syp[k].C = (n > 0) ? MindxT(C,((size_t)n<<eltsh)) : NULL;
         syp[k].A0 = (ISNOTRANS) ? MindxT(A,((size_t)n<<eltsh)) 
                                 : MindxT((size_t)A,n*incA);
         syp[k].A = syp[k].A0;
         syp[k].B = A;
         n += m;
      }
   }
   else  /* Uplo == AtlasUpper */
   {
      n = 0;
      for (k=0; k < p; k++)
      {
@skip         j = ATL_launchorder[k];
         m = Ms[k];
         syp[k].gemmK = gemmK;
         syp[k].tvsyrk = tvsyrk;
         syp[k].alpha = alpha;
         syp[k].beta  = beta ;
         syp[k].K = K;
         syp[k].lda = lda;
         syp[k].ldc = ldc;
         syp[k].nb = nb;
         syp[k].eltsh = eltsh;
         syp[k].Uplo = Uplo;
         syp[k].TA = TA;
         syp[k].TB = TB;
         syp[k].M = m;
         syp[k].N = n;
         JJ = N - n - m;
         syp[k].T = MindxT(C,((size_t)JJ*incC));
         syp[k].C = (n > 0) ? MindxT(C,((size_t)JJ*incC+m*(ldc<<eltsh))) : NULL;
         if (ISNOTRANS)
         {
            syp[k].A = syp[k].A0 = MindxT(A,((size_t)JJ<<eltsh));
            syp[k].B = MindxT(syp[k].A0, ((size_t)m<<eltsh)); 
         }
         else
         {
            syp[k].A = syp[k].A0 = MindxT(A,((size_t)JJ*incA));
            syp[k].B = MindxT(syp[k].A0, ((size_t)m*incA)); 
         }
         n += m;
      }
   }
   for (k=p; k < ATL_NTHREADS; k++)
      syp[k].K = 0;
   return(p);
}

void ATL_DoWorkSYRK_M(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_M_t *syp = ((ATL_TSYRK_M_t*)lp->opstruct) + tp->rank;

   syp->tvsyrk(syp->Uplo, syp->TA, syp->M, syp->K, syp->alpha, 
               syp->A0, syp->lda, syp->beta, syp->T, syp->ldc);
   if (syp->C)
      syp->gemmK(syp->M, syp->N, syp->K, syp->alpha, syp->A, syp->lda,
                 syp->B, syp->lda, syp->beta, syp->C, syp->ldc);
}

@ROUT ATL_tsyrk
   @define TAC @T@
   @define TRANS @AtlasTrans@
   @define sadd @SADD@
   @define rt @syrk@
   @define styp @SCALAR@
@ROUT ATL_therk
   @define TAC @C@
   @define TRANS @AtlasConjTrans@
   @define sadd @&@
   @define rt @herk@
   @define styp @TYPE@
@ROUT ATL_tsyrk ATL_therk
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"

/*
 * Prototype functions in ATL_Xtsyrk
 */
int ATL_IsInitSYRK_M(void *vp);
void ATL_DoWorkSYRK_M(ATL_LAUNCHSTRUCT_t *lp, void *vp);
int ATL_tsyrkdecomp_M
   (ATL_TSYRK_M_t *syp, const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS TA,
    ATL_CINT N, ATL_CINT K, const void *alpha, const void *A, ATL_CINT lda,
    const void *beta, void *C, ATL_CINT ldc, ATL_CINT nb, const int mu,
    const int eltsh, const enum ATLAS_TRANS TB, double minmf,
    void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                  ATL_CINT,const void*, ATL_CINT, const void*, void*, ATL_CINT),
    void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                   ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                   void*, ATL_CINT));
@beginskip
int ATL_tsyrkdecomp_N(ATL_TSYRK_N_t *psy, ATL_CINT nthr);
int ATL_IsInitSYRK_N(void *vp);
void ATL_DoWorkSYRK_N(ATL_LAUNCHSTRUCT_t *lp, void *vp);
@endskip
int ATL_tsyrkdecomp_K
   (ATL_TSYRK_K_t *psyrk, 
    void (*syrkK)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT),
    const int eltsh, const int nb, const void *zero, const void *one,
    const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, 
    ATL_CINT N, ATL_CINT Kblks, const int kr, 
    const void *alpha, const void *A, ATL_CINT lda,
    const void *beta, void *C, ATL_CINT ldc);
void ATL_tsyrk_K_rec(ATL_TSYRK_K_t *syp, int np, ATL_CINT Nblks, ATL_CINT nr, 
                     ATL_CINT K, const void *A, void *C);
void ATL_DoWorkSYRK_K(ATL_LAUNCHSTRUCT_t *lp, void *vp);
int ATL_IsInitSYRK_K(void *vp);
@beginskip
int ATL_tsyrkdecomp_tr
   (ATL_TSYRK_t *psyrk, const int P, ATL_CINT Tblks, ATL_CINT tr, 
    ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr, ATL_CINT K,
    ATL_CINT ia, ATL_CINT ja, ATL_CINT ib, ATL_CINT jb, 
    ATL_CINT ic, ATL_CINT jc);
int ATL_StructIsInitSYRK(void *vp);
void ATL_DoWorkSYRK(ATL_LAUNCHSTRUCT_t *lp, void *vp);
void SortSYRKByFlopCount(int P, ATL_TSYRK_t *syp);
@endskip

void Mjoin(PATL,tv@(rt))
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, 
    ATL_CINT N, ATL_CINT K, const void *alpha, const void *A, ATL_CINT lda, 
    const void *beta, void *C, ATL_CINT ldc)
{
@ROUT ATL_tsyrk
   Mjoin(PATL,@(rt))(Uplo, Trans, N, K, SVVAL((TYPE*)alpha), A, lda, 
                   SVVAL((TYPE*)beta), C, ldc);
@ROUT ATL_therk
   Mjoin(PATL,@(rt))(Uplo, Trans, N, K, *((TYPE*)alpha), A, lda, 
                   *((TYPE*)beta), C, ldc);
@ROUT ATL_tsyrk ATL_therk
}

@beginskip
static void ATL_init@up@(rt)_t
   (const int P, ATL_TSYRK_t *syp, const void *alpha, const void *beta, 
    const void *one, const void *zero, enum ATLAS_UPLO Uplo, 
    enum ATLAS_TRANS Trans, ATL_CINT K, const void *A, ATL_CINT lda, 
    void *C, ATL_CINT ldc)
{
   int i, nb;
   nb = Mjoin(PATL,GetNB)();

   for (i=0; i < P; i++)
   {
      syp[i].alpha = alpha;
      syp[i].beta  = beta ;
      syp[i].one   = one  ;
      syp[i].zero  = zero ;
      syp[i].Uplo = Uplo;
      syp[i].Trans = Trans;
      syp[i].A = A;
      syp[i].lda = lda;
      syp[i].ldc = ldc;
      syp[i].K = K;
      syp[i].C = C;
      syp[i].tvgemm = Mjoin(PATL,tvgemm);
      syp[i].tvsyrk = Mjoin(PATL,tv@(rt));
      syp[i].eltsh = Mjoin(PATL,shift);
      syp[i].ia = -1;  /* flag that this entry is not being used */
      syp[i].nb = nb;
   }
}
@endskip

static int CombineCw(ATL_TSYRK_K_t *me, ATL_TSYRK_K_t *him)
/*
 * This routine combines the data in him->Cw into my->Cw, if possible.
 * If his workspace is bigger than mine, I combine instead into his workspace,
 * and then set my pointer to his workspace.  The buffer that has been subsumed
 * is freed after the combine.
 * RETURNS: 0 if we are able to do the combine, non-zero if buffers are
 *          cannot be combined.
 */
{
   TYPE *w;
   size_t meB, meE, himB, himE, I, J;   /* begin,end of C range */
   #ifdef TREAL
      const TYPE ONE = 1.0;
   #else
      const TYPE ONE[2] = {1.0, 0.0};
   #endif

/*
 * If I'm the master (owner of original C), then I can always do combine
 * into the original C
 */
   if (me->nCw == 0)
   {
      if (him->Cw)  /* his malloc succeeded */
      {
         Mjoin(PATL,tradd)(him->Uplo, him->N, ATL_AlignPtr(him->Cw),
                           him->ldcw, ONE, (TYPE*)him->C, him->ldc);
         free(him->Cw);
      }
      else if (him->nCw)  /* must do GEMM since he couldn't malloc */
         him->tvsyrk(him->Uplo, him->Trans, him->N, him->K, him->alpha, 
                     him->A, him->lda, him->one, him->C, him->ldc);
      return(0);        /* successful combine */
   }
/*
 * *************************************************************************
 * Otherwise, I don't own C, so must combine work into my or his buffer when
 * possible, and return failure when not
 * *************************************************************************
 */
   meB  = (size_t) me->C; 
   meE  = meB + (((me->N*(me->ldc + 1)))<<(me->eltsh));
   himB = (size_t)him->C; 
   himE = himB + (((him->N*(him->ldc + 1)))<<(me->eltsh));
/*
 * If my workspace is a superset of his, use my workspace as the target buffer
 * if I was able to allocate it
 */
   if (meB <= himB && meE >= himE && me->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (himB - meB)>>(him->eltsh);           /* gap in elts */
      J = I / him->ldc;                         /* col coord */
      I -= J*him->ldc;                          /* row coord */
      ATL_assert(I == J);
      w = ATL_AlignPtr(me->Cw);
      w += J*me->ldcw + I;
      if (him->Cw)  /* if he succeeded in malloc, combine his op with mine */
      {
         Mjoin(PATL,tradd)(him->Uplo, him->N, ATL_AlignPtr(him->Cw),
                           him->ldcw, ONE, w, him->ldcw);
         free(him->Cw);
      }
      else          /* must do SYRK since he didn't */
         him->tvsyrk(him->Uplo, him->Trans, him->N, him->K, him->alpha, 
                     him->A, him->lda, him->one, w, me->ldcw);
      return(0);        /* successful combine */
   }
/*
 * else if his workspace is a superset of mine, use his as target buffer if
 * he was able to allocate it
 */
   else if (himB <= meB && himE >= meE && him->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (meB - himB)>>(him->eltsh);           /* gap in elements */
      J = I / him->ldc;                         /* column coord */
      I -= J*him->ldc;                          /* row coord */
      ATL_assert(I == J);
      w = ATL_AlignPtr(him->Cw);
      w += J*him->ldcw + I;
      if (me->Cw)  /* if I succeeded in malloc, combine my op with his */
      {
         Mjoin(PATL,tradd)(me->Uplo, me->N, ATL_AlignPtr(me->Cw),
                           me->ldcw, ONE, w, him->ldcw);
         free(me->Cw);
      }
      else          /* must do my SYRK into his workspace since I couldn't */
         him->tvsyrk(me->Uplo, me->Trans, me->N, me->K, me->alpha, 
                     me->A, me->lda, me->one, w, him->ldcw);
      me->C = him->C;
      me->Cw = him->Cw;
      me->ldcw = him->ldcw;
      me->N = him->N;
      me->K = him->K;
      return(0);        /* successful combine */
   }
   return(1);           /* unsuccessful combine */
}

void Mjoin(PATL,CombineStructs@up@(rt))
   (void *opstruct, const int myrank, const int hisrank)
/*
 * This routine written like GEMM, so that SYRK can have been split
 * with N, even though present code only splits K (so everyone is writing
 * to entire C).  I may want the extra functionality later, so programmed
 * it using GEMM as model.
 * NOTE: this version actually wouldn't work if we split both N & K for
 *       all cases; I later had to redesign the GEMM combine to account
 *       for the fact that you have to sum up the pieces of the original C
 *       you own, instead of always modifying C when you own only  a piece
 *       of it.  This problem only shows up on systems with non-power-of-2
 *       # of processors, where the launch recursive distribution doesn't
 *       match the recursive launch/combine procedure.  Will need to rewrite
 *       based on present GEMM combine if I ever go to true recursive
 *       distribution on both N & K.
 */
{
   #ifdef TREAL
      TYPE ONE = ATL_rone;
   #else
      TYPE ONE[2] = {ATL_rone, ATL_rzero};
   #endif
   ATL_TSYRK_K_t *me = ((ATL_TSYRK_K_t*)opstruct)+myrank; 
   ATL_TSYRK_K_t *him = ((ATL_TSYRK_K_t*)opstruct)+hisrank, *himcp, *mycp;
   int i, j;

/*
 * Need to combine only if joining thread has C in workspace
 */
   if (him->nCw)
   {
/*
 *    For all his workspaces, find out where to combine them into
 */
      for (i=0; i < him->nCw; i++)
      {
/*
 *       If I can't combine his data into my primary workspace, see if it
 *       can be combined with any of my other workspaces
 */
         if (CombineCw(me, him->Cinfp[i]))
         {
            for (j=1; j < me->nCw; j++)
               if (!CombineCw(me->Cinfp[j], him->Cinfp[i]))
                  break;
/*
 *          If I can't combine his data into any existing auxiliary space,
 *          add his node to my list of workspaces to be combined later
 */
            if (j == me->nCw)
            {
               me->Cinfp[j] = him->Cinfp[i];
               me->nCw = j + 1;
            }
         }
      }
   }
}

void Mjoin(PATL,t@(rt)_K_rec)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const SCALAR beta, TYPE *C, ATL_CINT ldc, ATL_CINT nb)
/*
 * This typed wrapper routine sets up type-specific data structures, and
 * calls the appropriate typeless recursive routine in order to recursively
 * cut N until workspace can be allocated, and then the K-dimension will be
 * threaded.  During the recursion, parallel performance is achieved by
 * calling the threaded GEMM.
 */
{
   ATL_CINT Nblks = N/nb, nr = N - nb*Nblks;
   ATL_TSYRK_K_t syp[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
@skip   ATL_thread_t tp[ATL_NTHREADS];
   #ifdef TCPLX
      TYPE ZERO[2] = {ATL_rzero, ATL_rzero}, ONE[2] = {ATL_rone, ATL_rzero};
   #else
      TYPE ZERO=ATL_rzero, ONE=ATL_rone;
   #endif
   int i;

@beginskip
   ls.opstructstride = (int) ( ((char*)(syp+1)) - (char*)syp );
   ls.OpStructIsInit = ATL_IsInitSYRK_K;
   ls.DoWork = ATL_DoWorkSYRK_K;
   ls.CombineOpStructs = Mjoin(PATL,CombineStructs@up@(rt));
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   syp[0].lp = &ls;
@endskip
   syp[0].DoComb = Mjoin(PATL,CombineStructs@up@(rt));
   syp[0].Uplo = Uplo;
   syp[0].Trans = Trans;
@ROUT ATL_therk `   syp[0].TB = (Trans == AtlasNoTrans) ? AtlasConjTrans : AtlasNoTrans;`
@ROUT ATL_tsyrk `   syp[0].TB = (Trans == AtlasNoTrans) ? AtlasTrans : AtlasNoTrans;`
   syp[0].K = K;
   syp[0].alpha = SADD alpha;
   syp[0].beta = SADD beta;
   syp[0].zero = SADD ZERO;
   syp[0].one  = SADD ONE;
   syp[0].lda = lda;
   syp[0].ldc = ldc;
   syp[0].gemmT = Mjoin(PATL,tvgemm);
@skip   syp[0].gemmT = (Trans == AtlasNoTrans) ? 
@skip      Mjoin(PATL,tsvgemmN@(TAC)) : Mjoin(PATL,tsvgemm@(TAC)N);
   syp[0].tvsyrk = Mjoin(PATL,tv@(rt));
   syp[0].eltsh = Mjoin(PATL,shift);
   syp[0].nb = nb;
@skip   ls.opstruct = (char*) syp;
   ATL_tsyrk_K_rec(syp, Mjoin(PATL,threadMM)(Trans, syp[0].TB, N>>1, N>>1, K), 
                   Nblks, nr, K, A, C);
}

static int ATL_t@(rt)_M
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS TA, ATL_CINT N,
    ATL_CINT K, const void *alpha, const TYPE *A, ATL_CINT lda,
    const void *beta, TYPE *C, ATL_CINT ldc)
{
@skip   ATL_thread_t tp[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
   ATL_TSYRK_M_t syp[ATL_NTHREADS];
   int i, p;
   p = ATL_tsyrkdecomp_M(syp, Uplo, TA, N, K, alpha, A, lda, beta, C, ldc,
                         MB, ATL_mmMU, Mjoin(PATL,shift), 
                         (TA == AtlasNoTrans) ? @(TRANS) : AtlasNoTrans,
                         ATL_TGEMM_PERTHR_MF, (TA == AtlasNoTrans) ? 
                         Mjoin(PATL,tsvgemmN@(TAC)):Mjoin(PATL,tsvgemm@(TAC)N),
                         Mjoin(PATL,tv@(rt)));
   if (p < 2)
      return(0);
   ATL_goparallel(p, ATL_DoWorkSYRK_M, syp, NULL);
@beginskip
   ls.opstruct = (char*) syp;
   ls.opstructstride = (int) ( ((char*)(syp+1)) - (char*)syp );
   ls.OpStructIsInit = ATL_IsInitSYRK_M;
   ls.DoWork = ATL_DoWorkSYRK_M;
   ls.CombineOpStructs = NULL;
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
@endskip
   return(p);
}

@beginskip
#if ATL_NTHREADS == (1<<ATL_NTHRPOW2)
static int ATL_t@(rt)_N
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const void *alpha, const TYPE *A, ATL_CINT lda,
    const void *beta, TYPE *C, ATL_CINT ldc)
{
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   ATL_TSYRK_N_t psy[ATL_NTHREADS];
   int i, p;

   psy[0].gemmK = (Trans == AtlasNoTrans) ? 
      Mjoin(PATL,tsvgemmN@(TAC)) : Mjoin(PATL,tsvgemm@(TAC)N);
   psy[0].tvsyrk = Mjoin(PATL,tv@(rt));   
   psy[0].numthr = Mjoin(PATL,tNumGemmThreads);
   psy[0].A0 = A;
   psy[0].A1 = NULL;
   psy[0].T = C;
   psy[0].C = NULL;
   psy[0].alpha = alpha;
   psy[0].beta  = beta;
   psy[0].M = N;
   psy[0].N = 0;
   psy[0].K = K;
   psy[0].lda = lda;
   psy[0].ldc = ldc;
   psy[0].nb = Mjoin(PATL,GetNB)();
   psy[0].eltsh = Mjoin(PATL,shift);
   psy[0].Uplo = Uplo;
   psy[0].TA = Trans;
   psy[0].TB = (Trans == AtlasNoTrans) ? @(TRANS) : AtlasNoTrans;
   for (i=1; i < ATL_NTHREADS; i++)
      psy[i].K = 0;
   p = ATL_tsyrkdecomp_N(psy, ATL_NTHREADS);
   if (p < 2)
      return(0);
   ls.opstruct = (char*) psy;
   ls.opstructstride = (int) ( ((char*)(psy+1)) - (char*)psy );
   ls.OpStructIsInit = ATL_IsInitSYRK_N;
   ls.DoWork = ATL_DoWorkSYRK_N;
   ls.CombineOpStructs = NULL;
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
   return(p);
}
#endif
@endskip

void Mjoin(PATL,t@(rt))
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
@rout ATL_therk
    ATL_CINT K, const @(styp) alpha0, const TYPE *A, ATL_CINT lda,
    const @(styp) beta0, TYPE *C, ATL_CINT ldc)
@ROUT ATL_tsyrk
    ATL_CINT K, const @(styp) alpha, const TYPE *A, ATL_CINT lda,
    const @(styp) beta, TYPE *C, ATL_CINT ldc)
@ROUT ATL_tsyrk ATL_therk
{
@beginskip
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   ATL_TSYRK_t syrks[ATL_NTHREADS];
   ATL_TSYRK_K_t psyrks[ATL_NTHREADS];
@endskip
   #ifdef TREAL
      const TYPE ONE = ATL_rone, ZERO = ATL_rzero;
   #else
      const TYPE ONE[2]={ATL_rone, ATL_rzero}, ZERO[2]={ATL_rzero, ATL_rzero};
@ROUT ATL_therk `      const TYPE alpha[2]={alpha0, ATL_rzero}, beta[2]={beta0, ATL_rzero};`
   #endif
   size_t nblksN;
   int i, np, nb;
   void Mjoin(PATL,pt@(rt))
      (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
       ATL_CINT K, const @(styp) alpha, const TYPE *A, ATL_CINT lda,
       const @(styp) beta, TYPE *C, ATL_CINT ldc);

   if (Mjoin(PATL,threadMM)(Trans, 
                            (Trans == AtlasNoTrans) ? AtlasTrans:AtlasNoTrans,
                            N, N>>1, K) < 2)
      goto DOSERIAL;
   if (N < 1)
      return;
@ROUT ATL_therk `   if (alpha0 == ATL_rzero || K < 1)`
@ROUT ATL_tsyrk `   if (SCALAR_IS_ZERO(alpha) || K < 1)`
   {
@ROUT ATL_therk
      if (beta0 != ATL_rone)
         Mjoin(PATL,hescal)(Uplo, N, N, beta0, C, ldc);
@ROUT ATL_tsyrk
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,trscal)(Uplo, N, N, beta, C, ldc);
@ROUT ATL_tsyrk ATL_therk
      return;
   }

   nb = MB;
   if (K > (N<<ATL_NTHRPOW2) && (((size_t)N)*N*sizeof(TYPE) <= ATL_PTMAXMALLOC))
   {
      Mjoin(PATL,t@(rt)_K_rec)(Uplo, Trans, N, K, alpha, A, lda, beta, 
                               C, ldc, nb);
@ROUT ATL_therk `      Mjoin(PATLU,zero)(N, C+1, lda+lda+2);  /* zero imag on diag */`
      return;
   }
@beginskip
#if 0 && ATL_NTHREADS == (1<<ATL_NTHRPOW2)
   np = ATL_t@(rt)_N(Uplo, Trans, N, K, SADD alpha, A, lda, 
                     SADD beta, C, ldc);
#endif
@endskip
   np = ATL_t@(rt)_M(Uplo, Trans, N, K, SADD alpha, A, lda, 
                     SADD beta, C, ldc);
   if (np < 2)
   {
DOSERIAL:
@ROUT ATL_tsyrk `      Mjoin(PATL,@(rt))(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);`
@ROUT ATL_therk `      Mjoin(PATL,@(rt))(Uplo, Trans, N, K, alpha0, A, lda, beta0, C, ldc);`
      return;
   }
}
@beginskip
/*
 *  Can only use special N-only decomposition of #proc is a power of two
 */
#if ATL_NTHREADS == (1<<ATL_NTHRPOW2)

/*
 * Distribute N unless K dominates N, or N is degenerate 
 */
   nblksN = N/nb;
   nblksN = nblksN*nblksN - nblksN;
   if ( ((K+K)/N < N && nblksN >= ATL_NTHREADS+ATL_NTHREADS) || 
        (((size_t)N)*N*sizeof(TYPE) > ATL_PTMAXMALLOC) )
   if ( (N > (nb<<(ATLNTHRPOW2+1))) || N > K ||
        (((size_t)N)*N*sizeof(TYPE) > ATL_PTMAXMALLOC) ||
        ((K>>ATL_NTHRPOW2) < nb && N >= (nb<<ATL_NTHRPOW2-1)) ||
        (N >= K && N > (nb<<(ATL_NTHRPOW2+1))) )
    {
       if (ATL_t@(rt)_N(Uplo, Trans, N, K, SADD alpha, A, lda, 
                        SADD beta, C, ldc))
          return;
    }
#endif
@endskip
@beginskip
   np = ATL_tsyrkdecomp_K(psyrks, 
      Mjoin(PATL,tv@(rt)), Mjoin(PATL,shift), nb, SADD ZERO, SADD ONE, 
      Uplo, Trans, N, K/nb, K%nb, SADD alpha, A, lda, SADD beta, C, ldc);
   if (np < 2)
      goto DOSERIAL;
   ls.opstruct = (char*) psyrks;
   ls.opstructstride = (int) ( ((char*)(psyrks+1)) - (char*)psyrks );
   ls.OpStructIsInit = ATL_IsInitSYRK_K;
   ls.DoWork = ATL_DoWorkSYRK_K;
   ls.CombineOpStructs = Mjoin(PATL,CombineStructs@up@(rt));
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
   return;
@ROUT ATL_therk `   Mjoin(PATLU,zero)(N, C+1, lda+lda+2);  /* zero imag on diag */`
   return;

   if (Uplo == AtlasLower && Trans == AtlasNoTrans)
   {
      ATL_init@up@(rt)_t(ATL_NTHREADS, syrks, @(sadd) alpha, @(sadd) beta, 
                        SADD ONE, SADD ZERO, Uplo, Trans, K, A, lda, C, ldc);
      nb = syrks[0].nb;
      np = ATL_tsyrkdecomp_tr(syrks, ATL_NTHREADS, N/nb, N%nb, 0, 0, 0, 0, K, 
                              0, 0, 0, 0, 0, 0);
      if (np < 2 || Mmin(N,K) < 8)
      {
         Mjoin(PATL,@(rt))(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);
         return;
      }
      ls.opstruct = (char*) syrks;
@skip      ls.opstructstride = (int) ( ((char*)(syrks+1)) - (char*)syrks );
      ls.OpStructIsInit = ATL_StructIsInitSYRK;
      ls.DoWork = ATL_DoWorkSYRK;
@skip      ls.CombineOpStructs = NULL;
      ls.DoComb = NULL;
      ls.rank2thr = tp;
      for (i=0; i < ATL_NTHREADS; i++)
      {
         tp[i].vp = &ls;
         tp[i].rank = i;
      }
      ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
      ATL_thread_join(tp);
   }
   else
   {
      SortSYRKByFlopCount(np, syrks);
      ls.opstruct = (char*) syrks;
@skip      ls.opstructstride = (int) ( ((char*)(syrks+1)) - (char*)syrks );
      ls.OpStructIsInit = ATL_StructIsInitSYRK;
      ls.DoWork = ATL_DoWorkSYRK;
      ls.DoComb = NULL;
@skip      ls.CombineOpStructs = NULL;
      ls.rank2thr = tp;
      for (i=0; i < ATL_NTHREADS; i++)
      {
         tp[i].vp = &ls;
         tp[i].rank = i;
      }
      ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
      ATL_thread_join(tp);
   }
}
@endskip
@ROUT atlas_tlvl2.h
#ifndef ATLAS_TLVL2_H
   #define ATLAS_TLVL2_H

#include "atlas_threads.h"
#ifdef TYPE
   #include "atlas_lvl2.h"
#endif
 
#endif          /* end of ifndef ATLAS_TLVL2_H */
@ROUT atlas_tlapack.h
#ifndef ATLAS_TLAPACK_H
   #define ATLAS_TLAPACK_H

@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#include "atlas_lapack.h"

typedef struct
{
   ATL_INT M;       /* matrix rows to distribute across processors */
   ATL_INT N;       /* matrix columns */
   volatile ATL_INT *maxindx;  /* this array starts wt all values -1 */
   volatile ATL_INT *stage;    /* this ptr starts wt all values -1 */
   void *A;
   ATL_INT lda;
   int *ipiv;
   int rank, p, info;
   void *works;    /* ptr to array of ptrs */
} ATL_TGETF2_M_t;

#endif                  /* end of ifndef ATLAS_TLAPACK_H */
@ROUT atlas_pca.h
#ifndef ATLAS_PCA_H
   #define ATLAS_PCA_H
/*
 * Only x86 is known to be strongly ordered (ATLAS does not use the special
 * writes that make it weakly-ordered).
 */
   #define ATL_membarrier
   #if defined(ATL_GAS_x8632) || defined(ATL_GAS_x8632)
      #define ATL_USEPCA 1
   #elif defined(ATL_USEPCA)
      #undef ATL_USEPCA
   #endif
@beginskip
/*
 * PowerPCs, POWERs and ARMs are weakly ordered, meaning that a given
 * processor's writes  may appear out-of-order to other processors, 
 * which breaks PCA's syncs since PCA depends on in-order writes.
 * To fix, we must issue a memory barrier call before giving the go-ahead.  
 * PowerPC: SYNC ensures that all prior stores complete before the next one.
 * POWER: DCS waits until all pending writes are written before preceeding
 * ARM: DMB (data mem barrier) - all prior mem accesses (in program order)
 *      complete before DMB returns
 *
 * Older x86's have a special mode where stores can become out-of-order, but
 * it was rarely enabled and does not seem to exist on modern hardware, so
 * we don't have to bother there.
 *
 * SPARCs do not change the order of stores.
 *
 * PowerPC and ARM syncs do not fix problem, so don't allow PCA on machines
 * with out-of-order write schemes.
 */
#if defined(ATL_ARCH_PPCG4) || defined(ATL_ARCH_PPCG5)
   #ifdef __GNUC__
      #define ATL_membarrier __asm__ __volatile__ ("sync")
/*      #define ATL_USEPCA 1 */
   #endif
#elif defined(ATL_ARCH_POWER3) || defined(ATL_ARCH_POWER4) || \
      defined(ATL_ARCH_POWER5) || defined(ATL_ARCH_POWER6) || \
      defined(ATL_ARCH_POWER7) || defined(ATL_ARCH_POWER8) || \
      defined(ATL_VSX)
   #if defined(__GNUC__) || defined(__xlC__)
      #define ATL_membarrier __asm__ __volatile__ ("dcs")
/*      #define ATL_USEPCA 1 */
   #endif
/*
 * Unfortunately, none of the memory fence instructions seems to work
 * adequately on ARM
 */
#elif defined(ATL_GAS_ARM64) || defined(ATL_GAS_ARM)
@skip   #ifdef __GNUC__
@skip      #define ATL_membarrier __asm__ __volatile__ ("dmb")
        #define ATL_membarrier
/*      #define ATL_USEPCA 1 */  
@skip   #endif
#elif defined(ATL_ARCH_IA64Itan) || defined(ATL_ARCH_IA64Itan2)
   #ifdef __GNUC__
      #define ATL_membarrier __asm__ __volatile__ ("mf")
/*      #define ATL_USEPCA 1 */
   #endif
#else
   #define ATL_membarrier
   #define ATL_USEPCA 1
#endif
@endskip

#endif
@ROUT ATL_tgetf2
#include "atlas_tlapack.h"
#include "atlas_pca.h"
#include "atlas_level2.h"

void Mjoin(PATL,DoWorkGETF2_nowrk)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TGETF2_M_t *lup=((ATL_TGETF2_M_t*)lp->opstruct)+tp->rank;
   int *ipiv = lup->ipiv;
   ATL_CINT M=lup->M, N=lup->N, lda=lup->lda, MN = Mmin(M,N);
   const int p = lup->p, rank = lup->rank;
   ATL_CINT mp = M/p, mr = M - mp*p;
   ATL_INT m, locpiv, globpiv, k, j, i;
   #ifdef TCPLX
      ATL_CINT lda2 = lda+lda;
   #else 
      #define lda2 lda
      #define none ATL_rnone
   #endif
   TYPE *A, *Ac, *a, *v;
   TYPE pivval, apv, apv2;
   #ifdef TCPLX
      const TYPE none[2] = {ATL_rnone, ATL_rzero};
   #endif
   volatile ATL_INT *maxindx=lup->maxindx, *stage=lup->stage;
   void (*my_ger)(const int M, const int N, const SCALAR alpha, 
                  const TYPE *X, const int incX, 
                  const TYPE *Y, const int incY, TYPE *A, const int lda);

   #ifdef TCPLX
      my_ger = Mjoin(PATL,geru);
   #else
      my_ger = Mjoin(PATL,ger);
   #endif
   m = (rank) ? mp : mp+mr;
   Ac = A = lup->A;
   a = (rank) ? A + ((m*rank + mr)SHIFT) : A;
   for (j=0; j < MN; j++, Ac += lda2, a += lda2)
   {
      locpiv = cblas_iamax(m, a, 1);
/*
 *    Combine local pivot into global
 */
      if (!rank)
      {
         globpiv = j+locpiv;
         #ifdef TCPLX
            apv = Mabs(Ac[globpiv+globpiv]) + Mabs(Ac[globpiv+globpiv+1]);
         #else
            apv = Mabs(Ac[globpiv]);
         #endif
         for (i=1; i < p; i++)
         {
            while(stage[i] < j);
            k = maxindx[i];
            apv2 = Mabs(Ac[k SHIFT]);
            #ifdef TCPLX
               apv2 += Mabs(Ac[k+k+1]);
            #endif
            if (apv < apv2)
            {
               apv = apv2;
               globpiv = k;
            }
            maxindx[i] = -1;
         }
         ipiv[j] = globpiv;
         if (globpiv != j)
            cblas_swap(N, A+(j SHIFT), lda, A+(globpiv SHIFT), lda);
         ATL_membarrier;
         stage[0] = j;
         m--;                                           /* just finished */
         #ifdef TCPLX
            a += 2;                                     /* one row */
         #else
            a++;                                        /* one row */
         #endif
      }
      else /* all threads except 0 write their results, and await 0 */
      {
         maxindx[rank] = locpiv+rank*mp+mr;
         stage[rank] = j;
         while (stage[0] < j);
      }
      #ifdef TCPLX
         if (Ac[j+j] != ATL_rzero || Ac[j+j+1] != ATL_rzero)
         {
            TYPE inv[2];
            Mjoin(PATL,cplxinvert)(1, Ac+j+j, 1, inv, 1);
            cblas_scal(m, inv, a, 1);
         }
      #else
         pivval = Ac[j];
         if (pivval != ATL_rzero)
            cblas_scal(m, ATL_rone/pivval, a, 1);
      #endif
      else /* pivot is zero, we have a singular matrix! */
         lup->info = j;   /* all threads have same info */

      #ifdef TCPLX
         my_ger(m, N-j-1, none, a, 1, Ac+((j+lda)<<1), lda, a+lda2, lda);
         my_ger = Mjoin(PATL,geru_L2);
      #else
         my_ger(m, N-j-1, ATL_rnone, a, 1, Ac+j+lda, lda, a+lda, lda);
         my_ger = Mjoin(PATL,ger_L2);
      #endif
   }
}

void Mjoin(PATL,DoWorkGETF2)(ATL_LAUNCHSTRUCT_t *lp, void *vp0)
{
   ATL_thread_t *tp=vp0;
   ATL_TGETF2_M_t *lup=((ATL_TGETF2_M_t*)lp->opstruct)+tp->rank;
   int *ipiv = lup->ipiv;
   ATL_CINT M=lup->M, N=lup->N, lda=lup->lda, MN = Mmin(M,N);
   const int p = lup->p, rank = lup->rank;
   int pivrank;
   ATL_CINT mp = M/p, mr = M - mp*p;
   ATL_INT m, locpiv, globpiv, k, j, i, ldw, ldw0, ldw1;
   void *vp;
   TYPE *a, *W, *Wc, *w, **WRKS=lup->works, *v;
   TYPE pivval, apv, apv2, pv2;
   volatile ATL_INT *maxindx=lup->maxindx, *stage=lup->stage;
   #ifdef TCPLX
      const TYPE none[2] = {ATL_rnone, ATL_rzero};
   #endif

   m = (rank) ? mp : mp+mr;
   a = (rank) ? (((TYPE*)lup->A)+((mp*rank + mr)SHIFT)) : lup->A;
/*
 * Make ldw's a multiple of 16 bytes that is not a power of 2; 0's ldw 
 * is larger by mr than all other ldws (ldw1)
 */
#if defined(DREAL) || defined(SCPLX)
   ldw0 = ((mp+mr+1)>>1)<<1;
   ldw1 = ((mp+1)>>1)<<1;
   if (!(ldw0 & (ldw0-1)))
      ldw0 += 2;
   if (!(ldw1 & (ldw1-1)))
      ldw1 += 2;
#elif defined(SREAL)
   ldw0 = ((mp+mr+3)>>2)<<2;
   ldw1 = ((mp+3)>>2)<<2;
   if (!(ldw0 & (ldw0-1)))
      ldw0 += 4;
   if (!(ldw1 & (ldw1-1)))
      ldw1 += 4;
#else
   ldw0 = mp+mr;
   ldw1 = mp;
   if (!(ldw0 & (ldw0-1)))
      ldw0++;
   if (!(ldw1 & (ldw1-1)))
      ldw1++;
#endif
   ldw = (rank) ? ldw1 : ldw0;
   vp = malloc(ATL_MulBySize(ldw)*N+ATL_Cachelen);
/*
 * If anyone fails to allocate the space, free any allocated spaces and
 * call the no-copy version
 */
   j = (vp != NULL);
   if (!rank)
   {
      for (i=1; i < p; i++)
      {
         while (stage[i] != -2);
         j &= maxindx[i];
         maxindx[i] = -1;
      }
      *maxindx = j;
      stage[0] = -2;
   }
   else
   {
      maxindx[rank] = j;
      stage[rank] = -2;
      while (stage[0] != -2);
   }
   if (*maxindx == 0)
   {
      if (vp)
         free(vp);
      Mjoin(PATL,DoWorkGETF2_nowrk)(lp, vp0);
      return;
   }
   ATL_assert(vp);
   WRKS[rank] = w = W = ATL_AlignPtr(vp);
   Mjoin(PATL,gecopy)(m, N, a, lda, W, ldw);
   for (j=0; j < MN; j++, w += (ldw SHIFT))
   {
      locpiv = cblas_iamax(m, w, 1);
/*
 *    Combine local pivot into global
 */
      if (!rank)
      {
         globpiv = j+locpiv;
         pivrank = 0;
         apv = Mabs(w[locpiv SHIFT]);
         #ifdef TCPLX
            apv += Mabs(w[locpiv+locpiv+1]);
         #endif
         for (i=1; i < p; i++)
         {
            while(stage[i] < j);
            k = maxindx[i];
            apv2 = WRKS[i][(j*ldw1+k)SHIFT];
            apv2 = Mabs(apv2);
            #ifdef TCPLX
               apv2 += Mabs(WRKS[i][((j*ldw1+k)<<1)+1]);
            #endif
            if (apv < apv2)
            {
               apv = apv2;
               globpiv = k;
               pivrank = i;
            }
            maxindx[i] = -1;
         }
         if (pivrank)
         {
            ipiv[j] = mr+pivrank*mp+globpiv;
            cblas_swap(N, W+(j SHIFT), ldw, 
                       WRKS[pivrank]+(globpiv SHIFT), ldw1);
         }
         else
         {
            ipiv[j] = globpiv;
            if (globpiv != j)
               cblas_swap(N, W+(j SHIFT), ldw, W+(globpiv SHIFT), ldw);
         }
         ATL_membarrier;
         stage[0] = j;
         m--;                                           /* just finished */
         #ifdef TCPLX
            w += 2;                                     /* one row */
         #else
            w++;                                        /* one row */
         #endif
      }
      else /* all threads except 0 write their results, and await 0 */
      {
         maxindx[rank] = locpiv;
         stage[rank] = j;
         while (stage[0] < j);
      }
      #ifdef TCPLX
         v = &WRKS[0][(j*ldw0+j)SHIFT];
         if (*v != ATL_rzero || v[1] != ATL_rzero)
         {
            TYPE inv[2];
            Mjoin(PATL,cplxinvert)(1, v, 1, inv, 1);
            cblas_scal(m, inv, w, 1);
         }
      #else
         pivval = WRKS[0][j*ldw0+j];
         if (pivval != ATL_rzero)
            cblas_scal(m, ATL_rone/pivval, w, 1);
      #endif
      else /* pivot is zero, we have a singular matrix! */
         lup->info = j;   /* all threads have same info */

      #ifdef TCPLX
         Mjoin(PATL,geru_L2)(m, N-j-1, none, w, 1, 
                             WRKS[0]+((j*(ldw0+1)+ldw0)SHIFT), ldw0, 
                             w+ldw+ldw, ldw);
      #else
         Mjoin(PATL,ger_L2)(m, N-j-1, ATL_rnone, w, 1, WRKS[0]+j*(ldw0+1)+ldw0,
                            ldw0, w+ldw, ldw);
      #endif
   }
   stage[rank] = MN;  /* let core 0 know we are done */
/*
 * Copy answer back out of workspace and then free workspace
 */
   Mjoin(PATL,gecopy)(rank?mp:mp+mr, N, W, ldw, a, lda);
/*
 * Core 0 waits for all other cores to finish before he frees his work:
 * all non-zero cores access 0's workspace, but 0 does not access others' work
 * after iamax barrier
 */
   if (!rank)
   {
      for (i=1; i < p; i++)
         while(stage[i] != MN);
   }
   free(vp);
}

int Mjoin(PATL,StructIsInitGETF2)(void *vp)
{
   return(((ATL_TGETF2_M_t*)vp)->M);
}

@multidef trt Mjoin(PATL,DoWorkGETF2) Mjoin(PATL,DoWorkGETF2_nowrk)
@define pf @@
@whiledef pf _nocp
int Mjoin(PATL,tgetf2@(pf))(ATL_CINT M, ATL_CINT N, TYPE *A, ATL_CINT lda, int *ipiv)
{
@skip   ATL_thread_t tp[ATL_NTHREADS];
   ATL_TGETF2_M_t lu2s[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
   ATL_INT maxindx[ATL_NTHREADS], stage[ATL_NTHREADS];
   TYPE *works[ATL_NTHREADS];

   ATL_CINT MN = Mmin(M,N);
   ATL_INT p = ATL_NTHREADS, m, mr, i, j;

   if (M < 1 || N < 1)
      return(0);
   m = M / ATL_NTHREADS;
   mr = M - m*ATL_NTHREADS;
/*
 * This logic is necessary since tgetf2 assumes only one processor owns entire
 * logical block.  Can remove if we rewrite tgetf2 to allow the diagonal to
 * span multiple processors
 */
   if (m+mr < N)
   {
      p = M / N;
      if (p)
         m = M / p;
   }
   if (p < 2)   /* not enough rows, call serial algorithm */
      return(Mjoin(PATL,getf2)(M, N, A, lda, ipiv));
   for (i=0; i < p; i++)
   {
      stage[i] = maxindx[i] = -1;
@skip      j = ATL_launchorder[i];
      lu2s[i].M = M;
      lu2s[i].N = N;
      lu2s[i].A = A;
      lu2s[i].lda = lda;
      lu2s[i].ipiv = ipiv;  /* only thread 0 will write ipiv */
      lu2s[i].info = 0;
      lu2s[i].maxindx = maxindx;
      lu2s[i].stage = stage;
      lu2s[i].p = p;
      lu2s[i].rank = i;
      lu2s[i].works = works;
   }
   for (; i < ATL_NTHREADS; i++)
      lu2s[i].M = 0;
   ATL_goparallel(p, Mjoin(PATL,DoWorkGETF2), lu2s, NULL);
@beginskip
   ls.opstruct = (char*) lu2s;
   ls.opstructstride = (int) ( ((char*)(lu2s+1)) - (char*)(lu2s) );
   ls.CombineOpStructs = NULL;
   ls.OpStructIsInit = Mjoin(PATL,StructIsInitGETF2);
@skip   ls.DoWork = Mjoin(PATL,DoWorkGETF2);
   ls.DoWork = @(trt);
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
@endskip
   return(lu2s[0].info);
}
   @undef trt
@endwhile
#ifndef TCPLX
   #undef lda2
#endif
@ROUT ATL_ger_L2
#include "atlas_misc.h"
#include "atlas_lvl2.h"
#include "atlas_lvl3.h"
// #include "atlas_r1.h"


void Mjoin(PATL,ger1k)
   (ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *X, ATL_CINT incX, const TYPE *Y,
     ATL_CINT incY1, TYPE *A, ATL_CINT lda1);

#ifdef TREAL
   #define ATL_ger Mjoin(PATL,ger_L2)
   #define ATL_ger1 Mjoin(PATL,ger1k)
#else
   #ifdef Conj_
      #define ATL_ger Mjoin(PATL,gerc)
   #else
      #define ATL_ger Mjoin(PATL,geru)
      #define ATL_ger1 Mjoin(PATL,ger1u_a1_x1_yX)
   #endif
#endif
void ATL_ger(const int M, const int N, const SCALAR alpha,
             const TYPE *X, const int incX, const TYPE *Y, const int incY,
             TYPE *A, const int lda)
{
   int imb, mb, mb0, m=M, i;
   int incy=incY;
   #ifdef TREAL
      #define one ATL_rone
   #else
      static TYPE one[2] = {ATL_rone, ATL_rzero};
   #endif
   void *vx=NULL;
   size_t Aa, Ax;
   TYPE *x, *y = (TYPE*) Y;
   void (*getX)(const int N, const SCALAR alpha, const TYPE *X,
                const int incX, TYPE *Y, const int incY);
   #ifdef Conj_
      void (*ATL_ger1)(const int M, const int N, const SCALAR alpha,
                       const TYPE *X, const int incX, const TYPE *Y,
                       const int incY, TYPE *A, const int lda);
      ATL_ger1 = Mjoin(PATL,ger1c_a1_x1_yX);
   #endif

   if ( !M || !N || SCALAR_IS_ZERO(alpha) ) return;
   if (lda&1)
   {
//    fprintf(stderr, "WARNING: not using L2-tuned GER kernel!\n");
      Mjoin(PATL,ger)(M, N, alpha, X, incX, Y, incY, A, lda);
      return;
   }
  
//   fprintf(stderr, "in %s! M=%i, N=%i, lda=%i, alpha=%f, A[64]=%i.\n", 
//   __FILE__, M, N, lda, alpha, (int) (((long unsigned int) A) & 63));
//   fflush(stderr);

   imb = mb = M;

   Aa = (size_t) A;
   Ax = (size_t) X;
   if (Aa%16 != Ax%16 || incX != 1 || !SCALAR_IS_ONE(alpha))
   {
/*
 *    Apply alpha to Y if X has stride 1 & Y is MUCH smaller
 *    The LAPACK barfs if you  switch which vector alpha is applied to,
 *    since it tests tiny matrices, so make it apply to X when they are
 *    close to even
 */
      if (incX == 1 && N < (M>>4) && Aa%16 == Ax%16)
      {
         vx = malloc(ATL_Cachelen + ATL_MulBySize(N));
         ATL_assert(vx);
         y = ATL_AlignPtr(vx);
         #ifdef Conj_
            Mjoin(PATL,moveConj)(N, alpha, Y, incY, y, 1);
            ATL_ger1 = Mjoin(PATL,ger1u_a1_x1_yX);
         #else
            Mjoin(PATL,cpsc)(N, alpha, Y, incY, y, 1);
         #endif
         incy = 1;
         getX = NULL;
      }
      else
      {
         i = Mmax(imb,mb);
         i = Mmin(i,M);
         vx = malloc(2*ATL_Cachelen + ATL_MulBySize(i));
         ATL_assert(vx);
         Ax = (size_t) vx;
         for (i=Aa%16; Ax%16 != i; Ax++);
         x = (TYPE*) Ax;
         getX = Mjoin(PATL,cpsc);
      }
   }
   else getX = NULL;

   if (imb) mb0 = Mmin(imb,m);
   else mb0 = Mmin(mb,m);
   do
   {
      if (getX) getX(mb0, alpha, X, incX, x, 1);
      else x = (TYPE*) X;
      ATL_ger1(mb0, N, one, x, 1, y, incy, A, lda);
      A += mb0 SHIFT;
      X += mb0*incX SHIFT;
      m -= mb0;
      mb0 = Mmin(m,mb);
   }
   while(m);
   if (vx) free(vx);
}
@ROUT ATL_gerk
#include <xmmintrin.h>
#include "atlas_misc.h"
#include <stdio.h>

void Mjoin(PATL,ger1k)
   (ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *X, ATL_CINT incX, const TYPE *Y,
    ATL_CINT incY1, TYPE *A, ATL_CINT lda1)
{/* BEGIN GER: nMU=1, MU=8, NU=4 */
   ATL_INT i, j;
// ATL_CINT MAp = ( (((((size_t)A)+15)>>4)<<4) - (((((size_t)A))>>4)<<4) )/sizeof(TYPE);
   ATL_CINT MAp = ( ((size_t)A)&(15) ) / sizeof(TYPE);
   ATL_CINT MA=M-MAp;
   ATL_CINT M8=((MA/8)*8)+MAp, M2=((MA>>1)<<1)+MAp, N4=((N/4)*4), lda2=lda1+lda1, incY2=incY1+incY1, lda3=lda2+lda1, incY3=incY2+incY1, lda4=lda3+lda1, incY4=incY3+incY1;
   __m128d x0, x1, x2, x3, x4, x5, x6, x7, y0, y1, y2, y3, a0_0, m0_0, a1_0, m1_0, a2_0, m2_0, a3_0, m3_0, a4_0, m4_0, a5_0, m5_0, a6_0, m6_0, a7_0, m7_0, a0_1, m0_1, a1_1, m1_1, a2_1, m2_1, a3_1, m3_1, a4_1, m4_1, a5_1, m5_1, a6_1, m6_1, a7_1, m7_1, a0_2, m0_2, a1_2, m1_2, a2_2, m2_2, a3_2, m3_2, a4_2, m4_2, a5_2, m5_2, a6_2, m6_2, a7_2, m7_2, a0_3, m0_3, a1_3, m1_3, a2_3, m2_3, a3_3, m3_3, a4_3, m4_3, a5_3, m5_3, a6_3, m6_3, a7_3, m7_3;

   for (j=0; j < N4; j += 4, A += lda4, Y += incY4)
   {/* BEGIN N-LOOP UR=4 */
      y0 = _mm_load1_pd(Y);
      y1 = _mm_load1_pd(Y+incY1);
      y2 = _mm_load1_pd(Y+incY2);
      y3 = _mm_load1_pd(Y+incY3);
      if (MAp)
      {/* peel to force X/A alignment */
         i=0;                                /* bug fix. */
         x0 = _mm_load_sd(X+i+0);
         a0_0 = _mm_load_sd(A+i+0);
         m0_0 = _mm_mul_sd(x0, y0);
         a0_0 = _mm_add_sd(a0_0, m0_0);
         _mm_store_sd(A+i+0, a0_0);
         a0_1 = _mm_load_sd(A+i+0+lda1);
         m0_1 = _mm_mul_sd(x0, y1);
         a0_1 = _mm_add_sd(a0_1, m0_1);
         _mm_store_sd(A+i+0+lda1, a0_1);
         a0_2 = _mm_load_sd(A+i+0+lda2);
         m0_2 = _mm_mul_sd(x0, y2);
         a0_2 = _mm_add_sd(a0_2, m0_2);
         _mm_store_sd(A+i+0+lda2, a0_2);
         a0_3 = _mm_load_sd(A+i+0+lda3);
         m0_3 = _mm_mul_sd(x0, y3);
         a0_3 = _mm_add_sd(a0_3, m0_3);
         _mm_store_sd(A+i+0+lda3, a0_3);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_pd(X+i+0);
         a0_0 = _mm_load_pd(A+i+0);
         m0_0 = _mm_mul_pd(x0, y0);
         a0_0 = _mm_add_pd(a0_0, m0_0);
         _mm_store_pd(A+i+0, a0_0);
         x2 = _mm_load_pd(X+i+2);
         a2_0 = _mm_load_pd(A+i+2);
         m2_0 = _mm_mul_pd(x2, y0);
         a2_0 = _mm_add_pd(a2_0, m2_0);
         _mm_store_pd(A+i+2, a2_0);
         x4 = _mm_load_pd(X+i+4);
         a4_0 = _mm_load_pd(A+i+4);
         m4_0 = _mm_mul_pd(x4, y0);
         a4_0 = _mm_add_pd(a4_0, m4_0);
         _mm_store_pd(A+i+4, a4_0);
         x6 = _mm_load_pd(X+i+6);
         a6_0 = _mm_load_pd(A+i+6);
         m6_0 = _mm_mul_pd(x6, y0);
         a6_0 = _mm_add_pd(a6_0, m6_0);
         _mm_store_pd(A+i+6, a6_0);
         a0_1 = _mm_load_pd(A+i+0+lda1);
         m0_1 = _mm_mul_pd(x0, y1);
         a0_1 = _mm_add_pd(a0_1, m0_1);
         _mm_store_pd(A+i+0+lda1, a0_1);
         a2_1 = _mm_load_pd(A+i+2+lda1);
         m2_1 = _mm_mul_pd(x2, y1);
         a2_1 = _mm_add_pd(a2_1, m2_1);
         _mm_store_pd(A+i+2+lda1, a2_1);
         a4_1 = _mm_load_pd(A+i+4+lda1);
         m4_1 = _mm_mul_pd(x4, y1);
         a4_1 = _mm_add_pd(a4_1, m4_1);
         _mm_store_pd(A+i+4+lda1, a4_1);
         a6_1 = _mm_load_pd(A+i+6+lda1);
         m6_1 = _mm_mul_pd(x6, y1);
         a6_1 = _mm_add_pd(a6_1, m6_1);
         _mm_store_pd(A+i+6+lda1, a6_1);
         a0_2 = _mm_load_pd(A+i+0+lda2);
         m0_2 = _mm_mul_pd(x0, y2);
         a0_2 = _mm_add_pd(a0_2, m0_2);
         _mm_store_pd(A+i+0+lda2, a0_2);
         a2_2 = _mm_load_pd(A+i+2+lda2);
         m2_2 = _mm_mul_pd(x2, y2);
         a2_2 = _mm_add_pd(a2_2, m2_2);
         _mm_store_pd(A+i+2+lda2, a2_2);
         a4_2 = _mm_load_pd(A+i+4+lda2);
         m4_2 = _mm_mul_pd(x4, y2);
         a4_2 = _mm_add_pd(a4_2, m4_2);
         _mm_store_pd(A+i+4+lda2, a4_2);
         a6_2 = _mm_load_pd(A+i+6+lda2);
         m6_2 = _mm_mul_pd(x6, y2);
         a6_2 = _mm_add_pd(a6_2, m6_2);
         _mm_store_pd(A+i+6+lda2, a6_2);
         a0_3 = _mm_load_pd(A+i+0+lda3);
         m0_3 = _mm_mul_pd(x0, y3);
         a0_3 = _mm_add_pd(a0_3, m0_3);
         _mm_store_pd(A+i+0+lda3, a0_3);
         a2_3 = _mm_load_pd(A+i+2+lda3);
         m2_3 = _mm_mul_pd(x2, y3);
         a2_3 = _mm_add_pd(a2_3, m2_3);
         _mm_store_pd(A+i+2+lda3, a2_3);
         a4_3 = _mm_load_pd(A+i+4+lda3);
         m4_3 = _mm_mul_pd(x4, y3);
         a4_3 = _mm_add_pd(a4_3, m4_3);
         _mm_store_pd(A+i+4+lda3, a4_3);
         a6_3 = _mm_load_pd(A+i+6+lda3);
         m6_3 = _mm_mul_pd(x6, y3);
         a6_3 = _mm_add_pd(a6_3, m6_3);
         _mm_store_pd(A+i+6+lda3, a6_3);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */

         for (i=M8; i < M2; i += 2)
         {/* ----- BEGIN M-LOOP BODY ----- */
            /* --- BEGIN MUxNU UNROLL 0 --- */
            x0 = _mm_load_pd(X+i+0);
            a0_0 = _mm_load_pd(A+i+0);
            m0_0 = _mm_mul_pd(x0, y0);
            a0_0 = _mm_add_pd(a0_0, m0_0);
            _mm_store_pd(A+i+0, a0_0);
            a0_1 = _mm_load_pd(A+i+0+lda1);
            m0_1 = _mm_mul_pd(x0, y1);
            a0_1 = _mm_add_pd(a0_1, m0_1);
            _mm_store_pd(A+i+0+lda1, a0_1);
            a0_2 = _mm_load_pd(A+i+0+lda2);
            m0_2 = _mm_mul_pd(x0, y2);
            a0_2 = _mm_add_pd(a0_2, m0_2);
            _mm_store_pd(A+i+0+lda2, a0_2);
            a0_3 = _mm_load_pd(A+i+0+lda3);
            m0_3 = _mm_mul_pd(x0, y3);
            a0_3 = _mm_add_pd(a0_3, m0_3);
            _mm_store_pd(A+i+0+lda3, a0_3);
            /* --- END MUxNU UNROLL 0 --- */
         }/* ----- END M-LOOP BODY ----- */
         if (M != M2)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
            x0 = _mm_load_sd(X+i+0);
            a0_0 = _mm_load_sd(A+i+0);
            m0_0 = _mm_mul_sd(x0, y0);
            a0_0 = _mm_add_sd(a0_0, m0_0);
            _mm_store_sd(A+i+0, a0_0);
            a0_1 = _mm_load_sd(A+i+0+lda1);
            m0_1 = _mm_mul_sd(x0, y1);
            a0_1 = _mm_add_sd(a0_1, m0_1);
            _mm_store_sd(A+i+0+lda1, a0_1);
            a0_2 = _mm_load_sd(A+i+0+lda2);
            m0_2 = _mm_mul_sd(x0, y2);
            a0_2 = _mm_add_sd(a0_2, m0_2);
            _mm_store_sd(A+i+0+lda2, a0_2);
            a0_3 = _mm_load_sd(A+i+0+lda3);
            m0_3 = _mm_mul_sd(x0, y3);
            a0_3 = _mm_add_sd(a0_3, m0_3);
            _mm_store_sd(A+i+0+lda3, a0_3);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=4 */

   for (j=N4; j < N; j += 1, A += lda1, Y += incY1)
   {/* BEGIN N-LOOP UR=1 */
      y0 = _mm_load1_pd(Y);
      if (MAp)
      {/* peel to force X/A alignment */
         i=0;                                /* bug fix. */
         x0 = _mm_load_sd(X+i+0);
         a0_0 = _mm_load_sd(A+i+0);
         m0_0 = _mm_mul_sd(x0, y0);
         a0_0 = _mm_add_sd(a0_0, m0_0);
         _mm_store_sd(A+i+0, a0_0);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_pd(X+i+0);
         a0_0 = _mm_load_pd(A+i+0);
         m0_0 = _mm_mul_pd(x0, y0);
         a0_0 = _mm_add_pd(a0_0, m0_0);
         _mm_store_pd(A+i+0, a0_0);
         x2 = _mm_load_pd(X+i+2);
         a2_0 = _mm_load_pd(A+i+2);
         m2_0 = _mm_mul_pd(x2, y0);
         a2_0 = _mm_add_pd(a2_0, m2_0);
         _mm_store_pd(A+i+2, a2_0);
         x4 = _mm_load_pd(X+i+4);
         a4_0 = _mm_load_pd(A+i+4);
         m4_0 = _mm_mul_pd(x4, y0);
         a4_0 = _mm_add_pd(a4_0, m4_0);
         _mm_store_pd(A+i+4, a4_0);
         x6 = _mm_load_pd(X+i+6);
         a6_0 = _mm_load_pd(A+i+6);
         m6_0 = _mm_mul_pd(x6, y0);
         a6_0 = _mm_add_pd(a6_0, m6_0);
         _mm_store_pd(A+i+6, a6_0);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */

         for (i=M8; i < M2; i += 2)
         {/* ----- BEGIN M-LOOP BODY ----- */
            /* --- BEGIN MUxNU UNROLL 0 --- */
            x0 = _mm_load_pd(X+i+0);
            a0_0 = _mm_load_pd(A+i+0);
            m0_0 = _mm_mul_pd(x0, y0);
            a0_0 = _mm_add_pd(a0_0, m0_0);
            _mm_store_pd(A+i+0, a0_0);
            /* --- END MUxNU UNROLL 0 --- */
         }/* ----- END M-LOOP BODY ----- */
         if (M != M2)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
            x0 = _mm_load_sd(X+i+0);
            a0_0 = _mm_load_sd(A+i+0);
            m0_0 = _mm_mul_sd(x0, y0);
            a0_0 = _mm_add_sd(a0_0, m0_0);
            _mm_store_sd(A+i+0, a0_0);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=1 */
}/* END GER: nMU=1, MU=8, NU=4 */
#ifdef MA
   #undef MA
#endif
#ifdef MAp
   #undef MAp
#endif
@ROUT ATL_threadMM
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
@skip #include Mstr(Mjoin(atlas_,Mjoin(Mjoin(Mjoin(PRE,tXover_),ATL_NCPU),p.h)))

#ifdef DEBUG
#define T2c(ta_) ((ta_) == AtlasNoTrans) ? 'N' : 'T'
#endif
#ifndef ATL_TXOVER_H
int Mjoin(PATL,threadMM)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                         size_t M, size_t N, size_t K)
/*
 * This dummy routine used when crossover is not tuned 
 */
{
#if 0
   size_t minD, maxD;

   minD = Mmin(M,N);
   minD = Mmin(minD,K);
   maxD = Mmax(M,N);
   maxD = Mmax(maxD,K);
   if (M >= (NB<<(ATL_NTHRPOW2+2)))
      return(2);
   else if (minD >= 8 && maxD >= 2*NB)
      return(1);
   return(0);
#else
   int Mjoin(PATL,GemmWillThread)(ATL_CINT M, ATL_CINT N, ATL_CINT K);
   return(Mjoin(PATL,GemmWillThread)(M, N, K));
#endif
}
#else
int Mjoin(PATL,threadMM)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                         size_t M, size_t N, size_t K)
/*
 * RETURNS: number of threads matmul should use to paralellize the problem
 */
{
   size_t i, j, smp2, bip2, xo, xom, D;
   const int *xop;
   int k;
   if (M < 256 && N < 256 && K < 256)   /* small matrix */
   {
/*
 *    For really small problems, table lookups too expensive, so do a quick
 *    return
 */
      j = Mmax(M,N);
      i = Mmin(M,N);
      i = Mmin(i,K);
      if (j <= NB+NB || i < NB)
         return(1);    /* quick return */
/*
 *    Make choice based on most restricted dimension
 */
      if (M < N && M < K)   /* M most restricted dim */
         goto SMALLM;
      else if (K < M && K < N)  /* K most restricted dim */
         goto SMALLK;
      else if (M == N && M == K)
         goto SQUARE;
      else  /* N is most restricted dim */
         goto SMALLN;
   }
/*
 * The following three shapes model recursive factorizations where
 * two dimensions are cut during the recursion, and a third remains large
 */
   else if (N <= 256 && K <= 256)  /* recursive shape that doesn't cut M */
   {                               /* LU uses this shape */
      i = Mmin(N, K);
      j = Mmax(N, K);
      if (i >= NB)
         i = (i+j)>>1;
      else if (i >= 8)
         i = (i+i+i+j)>>2;  /* 3/4 MIN, 1/4 MAX */
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      D = M;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SnkLm_XO : ATL_tmmNT_SnkLm_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SnkLm_XO : ATL_tmmTT_SnkLm_XO;
      #ifdef DEBUG
         printf("sNKlM_%c%c, M=%d, N=%d, K=%d rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (M <= 256 && N <= 256)  /* recursive shape that doesn't cut K */
   {                               /* QR uses, maybe in LARFT? */
      i = Mmin(M, N);
      j = Mmax(M, N);
      if (i >= NB)
         i = (i+j)>>1;
      else if (i >= 8)
         i = (i+i+i+j)>>2;  /* 3/4 MIN, 1/4 MAX */
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      D = K;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SmnLk_XO : ATL_tmmNT_SmnLk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SmnLk_XO : ATL_tmmTT_SmnLk_XO;
      #ifdef DEBUG
         printf("sMNlK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (M <= 256 && K <= 256) /* recursive shape that doesn't cut N */
   {                              /* UNCONFIRMED: QR variant uses */
      i = Mmin(M, K);
      j = Mmax(M, K);
      if (i >= NB)
         i = (i+j)>>1;
      else if (i >= 8)
         i = (i+i+i+j)>>2;  /* 3/4 MIN, 1/4 MAX */
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      D = N;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SmkLn_XO : ATL_tmmNT_SmkLn_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SmkLn_XO : ATL_tmmTT_SmkLn_XO;
      #ifdef DEBUG
         printf("sNlMK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
/*
 * The three following shapes model static blocking, where two dimensions
 * are full, and the third is blocked
 */
   else if (K <= 256)           /* K dim small, as in right-looking LU/QR */
   {
SMALLK:
      D = Mmin(M,N);
      if (D >= NB+NB)
         D = (M+N)>>1;
      i = K;
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SkLmn_XO : ATL_tmmNT_SkLmn_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SkLmn_XO : ATL_tmmTT_SkLmn_XO;
      #ifdef DEBUG
         printf("sKlMN_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                 T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (M <= 256)          /* M dim small */
   {
SMALLM:
      D = Mmin(N,K);
      if (D >= NB+NB)
         D = (N+K)>>1;
      i = M;
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SmLnk_XO : ATL_tmmNT_SmLnk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SmLnk_XO : ATL_tmmTT_SmLnk_XO;
      #ifdef DEBUG
         printf("sMlNK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (N <= 256)          /* N dim small */
   {                           /* QR uses this */
SMALLN:
      D = Mmin(M,K);
      if (D >= NB+NB)
         D = (M+K)>>1;
      i = N;
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SnLmk_XO : ATL_tmmNT_SnLmk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SnLmk_XO : ATL_tmmTT_SnLmk_XO;
      #ifdef DEBUG
         printf("sNlMK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else                        /* all dim > 256, call it square */
   {
SQUARE:   /* near-square shape, N <= 256 if jumped here */
      D = (M+N+K+1)/3;
      j = 0;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SQmnk_XO : ATL_tmmNT_SQmnk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SQmnk_XO : ATL_tmmTT_SQmnk_XO;
      #ifdef DEBUG
         printf("SQ_%c%c, M=%d, N=%d, K=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, D);
      #endif
   }

   xop += j*ATL_PDIM;
   for (k=ATL_PDIM-1; k >= 0; k--)
      if (xop[k] && D >= xop[k])
         return((k == ATL_PDIM-1) ? ATL_NTHREADS : (2<<k));
   return(1);
}
#endif
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np ATL_tgemm_rkK ATL_tgemm_bigMN_Kp
#include "atlas_misc.h"
#include "atlas_tcacheedge.h"
#ifndef CacheEdge
   #include "atlas_cacheedge.h"
   #ifndef CacheEdge
      #define CacheEdge 524288
   #endif
#endif
@ROUT ATL_tgemm_rkK ATL_tgemm_bigMN_Kp `#include "atlas_tlvl3.h"`
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np `#include "atlas_lvl3.h"`
#include "atlas_threads.h"
#include "atlas_tsumm.h"

@ROUT ATL_tgemm_MKp
static int ATL_NTHR = ATL_NTHREADS;
typedef struct
{
   void *aBcnt;           /* counter on partitions of B used in B copy */
   void *aAcnt;           /* count on the partitions of A */
   void *aCcnt;           /* count on columns of C */
   volatile int *chkin;   /* NTHR-len checkin array */
   TYPE **Aws;            /* preallocated thread copy areas */
   TYPE *Bw;              /* workspace for common B */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;          
   SCALAR beta;
   ATL_INT Kp, nKp, klast;
   ATL_INT Mp, nMp, mlast;
   ATL_INT M, N, K, lda, ldb, ldc, nNb;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_MKP_t;
@ROUT ATL_tgemm_rkK_Np
typedef struct
{
   void *aNcnt;           /* count on col-panels of C */
   void *aMcnt;           /* count row-panels of A */
   volatile int *chkin;   /* ATL_NTHREAD-len checkin array */
   TYPE **Bws;            /* preallocated thread copy areas */
   TYPE *Aw;              /* workspace for common A */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;          
   SCALAR beta;
   ATL_INT nKb, kr, kr8;
   ATL_INT nMb, mr, nNb, nr, nlblks, nrblks;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_RKK_NP_t;
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np ATL_tgemm_rkK

/*
 * Matmul driver, loops over pre-copied A & B, operands are preblocked
 * so a column panel of B, the entire A and needed portion of C fit in the L2,
 * Therefore, use all of A against a single column panel of B, 
 * and thus do NMK loop order.
 */

#define genmm Mjoin(PATL,pKBmm)     /* cleans up any combin. of partial blks */
#define PKBmm Mjoin(PATL,pKBmm_b1)  /* cleans up full MB,NB, partial KB */
#define PNBmm Mjoin(PATL,pNBmm_b1)  /* cleans up full MB,KB, partial NB */
#define PMBmm Mjoin(PATL,pMBmm_b1)  /* cleans up full NB,KB, partial MB */

/*
 * This routine is a driver routine that makes all the appropriate calls
 * to the gemm kernel assuming both A & B are already copied to block-major
 * format and that you all you need is one traversal of the K loop.
 */
static void DoMM_K
(
   ATL_CINT mb,         /* # of rows in C <= MB */
   ATL_CINT nb,         /* # of cols in C <= NB */
   ATL_CINT nfKblks,    /* # of full blocks of K */
   ATL_CINT kr,         /* partial remainder block on K */
   const TYPE *A,       /* block-major A in (nfKblks*KB+kr) x NB panel */
   const TYPE *B,       /* block-major B in (nfKblks*KB+kr) x NB panel */
   const SCALAR beta,   /* scale C by this value */
   TYPE *C,             /* ldcxnb array for result */
   ATL_CINT ldc         /* stride betweent elts in a row of C */
)
{
#ifdef TREAL
   ATL_INT k;
   ATL_CINT incA = mb*KB, incB = KB*nb;
   NBMM0 mmk, mmk_kr=genmm, mmk_bX;

   ATL_assert(mb <= MB && nb <= NB);
   if (!nfKblks)  /* only partial block to do! */
   {
      if (mb != MB || nb != NB)
      {
         if (SCALAR_IS_ZERO(beta))
            Mjoin(PATL,gezero)(mb, nb, C, ldc);
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, C, ldc);
      }
      else
      {
         if (SCALAR_IS_ONE(beta))
            Mjoin(PATL,pKBmm_b1)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, 
                                 C, ldc);
         else if (SCALAR_IS_ZERO(beta))
            Mjoin(PATL,pKBmm_b0)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, 
                                 C, ldc);
         else
            Mjoin(PATL,pKBmm_bX)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, 
                                 C, ldc);
      }
      return;
   }
   if (mb != NB && nb != NB)
      mmk_bX = mmk = genmm;
   else if (mb != NB)
   {
      mmk = PMBmm;
      if (SCALAR_IS_ONE(beta))
         mmk_bX = Mjoin(PATL,pMBmm_b1);
      else if (SCALAR_IS_ZERO(beta))
         mmk_bX = Mjoin(PATL,pMBmm_b0);
      else
         mmk_bX = Mjoin(PATL,pMBmm_bX);
   }
   else if (nb != NB)
   {
      mmk = PNBmm;
      if (SCALAR_IS_ONE(beta))
         mmk_bX = Mjoin(PATL,pNBmm_b1);
      else if (SCALAR_IS_ZERO(beta))
         mmk_bX = Mjoin(PATL,pNBmm_b0);
      else
         mmk_bX = Mjoin(PATL,pNBmm_bX);
   }
   else
   {
      mmk = NBmm;
      mmk_kr = PKBmm;
      if (SCALAR_IS_ONE(beta))
         mmk_bX = NBmm_b1;
      else if (SCALAR_IS_ZERO(beta))
         mmk_bX = NBmm_b0;
      else
         mmk_bX = NBmm_bX;
   }
   mmk_bX(mb, nb, KB, ATL_rone, A, KB, B, KB, beta, C, ldc);  /* apply beta */
   A += incA; B += incB;
   for (k=1; k < nfKblks; k++, A += incA, B += incB)  /* full blocks */
      mmk(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
   if (kr)  /* partial remainder */
      mmk_kr(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone, C, ldc);
#else  /* complex code */
   ATL_INT k;
   ATL_CINT incA = mb*KB*2, incB = 2*KB*nb;
   const TYPE one[2] = {ATL_rone, ATL_rzero};
   const TYPE *bet = beta;
   NBMM0 mmk, mmk_kr=genmm, mmk_bX;

   ATL_assert(mb <= MB && nb <= NB);
   if (!nfKblks)  /* only partial block to do! */
   {
      if (beta[1] == ATL_rzero && beta[0] != ATL_rzero)
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, *beta, C, ldc);
      else
      {
         if (*beta == ATL_rzero && beta[1] == ATL_rzero)
            Mjoin(PATL,gezero)(mb, nb, C, ldc);
         else
            Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone,C, ldc);
      }
      return;
   }
   if (mb == MB && nb == NB)
   {
      if (beta[1] == ATL_rzero)  /* real scalar */
      {
         if (*beta == ATL_rone)
         {
            NBmm_b1(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
         }
         else if (*beta == ATL_rzero)
         {
            NBmm_b0(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rzero, C, ldc);
         }
         else
         {
            NBmm_bX(mb, nb, KB, ATL_rone, A, KB, B, KB, *beta, C, ldc);
         }
      }
      else /* must scale for complex beta */
      {
         Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
         NBmm_b1(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
      }
      A += incA; B += incB;
      for (k=1; k < nfKblks; k++, A += incA, B += incB)  /* full blocks */
         NBmm_b1(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
      if (kr)  /* partial remainder */
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone,C, ldc);
      return;
   }
   if (mb != MB)
   {
      if (nb != NB)  /* both blocks partial */
      {
         if (*beta == ATL_rzero && beta[1] == ATL_rzero)
         {
            Mjoin(PATL,gezero)(mb, nb, C, ldc);
            bet = one;
         }
         else if (beta[1] != ATL_rzero)
         {
            Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
            bet = one;
         }
         mmk = mmk_bX =  Mjoin(PATL,pKBmm);
      }
      else  /* only M block is partial */
      {
         if (beta[1] != ATL_rzero)
         {
            Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
            bet = one;
         }
         mmk = Mjoin(PATL,pMBmm_b1);
         if (*bet == ATL_rone)
            mmk_bX = Mjoin(PATL,pMBmm_b1);
         else if (*bet == ATL_rzero)
            mmk_bX = Mjoin(PATL,pMBmm_b0);
         else
            mmk_bX = Mjoin(PATL,pMBmm_bX);
      }
   }
   else if (nb != MB)  /* only N block is partial */
   {
      if (beta[1] != ATL_rzero)
      {
         Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
         bet = one;
      }
      mmk = Mjoin(PATL,pNBmm_b1);
      if (*bet == ATL_rone)
         mmk_bX = Mjoin(PATL,pNBmm_b1);
      else if (*bet == ATL_rzero)
         mmk_bX = Mjoin(PATL,pNBmm_b0);
      else
         mmk_bX = Mjoin(PATL,pNBmm_bX);
   }
   mmk_bX(mb, nb, KB, ATL_rone, A, KB, B, KB, *bet, C, ldc);  /* apply beta */
   A += incA; B += incB;
   for (k=1; k < nfKblks; k++, A += incA, B += incB)  /* full blocks */
      mmk(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
   if (kr)  /* partial remainder */
      Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone, C, ldc);
#endif
}

@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np
/*
 * This routine is a driver routine that makes all the appropriate calls
 * to the gemm kernel assuming both A & B are already copied to block-major
 * format.
 */
static void DoMM_NMK
(
   ATL_CINT nfMblks,    /* # of full blocks of M */
   ATL_CINT mr,         /* partial remainder block on M */
   ATL_CINT nfNblks,    /* # of full blocks of N */
   ATL_CINT nr,         /* partial remainder block on N */
   ATL_CINT nfKblks,    /* # of full blocks of K */
   ATL_CINT kr,         /* partial remainder block on K */
   const TYPE *A,       /* block-major A in (nfKblks*KB+kr)xNB panels */
   const TYPE *B,       /* block-major B in (nfKblks*KB+kr)xNB panels */
   TYPE *C,             /* ldaxN array for result */
   ATL_CINT ldc         /* stride betweent elts in a row of C */
)
{
   ATL_INT j;
   ATL_CINT K = nfKblks*NB + kr;

   for (j=0; j < nfNblks; j++)
   {
      const TYPE *Bc = B + K*NB*j;
      TYPE *Cc = C + j*NB*ldc;
      ATL_INT i;

      for (i=0; i < nfMblks; i++)
      {
         const TYPE *a = A + K*NB*i;
         const TYPE *b = Bc;
         TYPE *c = Cc + i*NB;
         ATL_INT k;

         for (k=0; k < nfKblks; k++)
         {
            NBmm(MB, NB, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += NBNB;
            b += NBNB;
         }
         if (kr)  /* partial KB requires PKBmm */
            PKBmm(MB, NB, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
      if (mr) /* can use PMBmm for all full blocks of N & K */
      {
         const TYPE *b = Bc;
         const TYPE *a = A + K*NB*nfMblks;
         TYPE *c = Cc + nfMblks*NB;
         ATL_INT k;
         const int mrNB = mr*NB;

         for (k=0; k < nfKblks; k++)
         {
            PMBmm(mr, NB, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += mrNB;
            b += NBNB;
         }
         if (kr)  /* partial KB requires genmm */
            genmm(mr, NB, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
   }
   if (nr) /* can use PNBmm for all full blks of M/K, must use genmm rest */
   {
      const TYPE *Bc = B + K*nfNblks*NB;
      TYPE *Cc = C + nfNblks*NB*ldc;
      ATL_CINT nrNB = nr * NB;
      ATL_INT i;

      for (i=0; i < nfMblks; i++)
      {
         const TYPE *a = A + K*NB*i, *b=Bc;
         TYPE *c = Cc + i*NB;
         ATL_INT k;

         for (k=0; k < nfKblks; k++)
         {
            PNBmm(MB, nr, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += NBNB;
            b += nrNB;
         }
         if (kr)  /* partial KB&NB requires genmm */
            genmm(MB, nr, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
      if (mr) /* must use genmm for two or more partial dim */
      {
         const TYPE *a = A + K*NB*nfMblks, *b = Bc;
         TYPE *c = Cc + nfMblks*NB;
         const int mrNB = mr*NB;
         ATL_INT k;

         for (k=0; k < nfKblks; k++)
         {
            genmm(mr, nr, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += mrNB;
            b += nrNB;
         }
         if (kr)  /* partial KB requires genmm */
            genmm(mr, nr, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
   }
}
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np ATL_tgemm_rkK

/* 
 * Takes a MxN block and expands it to an ldaxM block wt zero padding in-place
 */
#ifdef TREAL
static void ExpandBlock
(
   ATL_CINT M, /* the number of rows that should be expanded to lda */
   ATL_CINT N, /* number of columns in block */
   TYPE *A,    /* in: MxN block, out: ldaxN blk, wt zero-padding in lda-M gap */
   ATL_INT lda /* desired stride between columns */
)
{
   TYPE *a, *c;
   ATL_CINT gap = lda - M;
   ATL_INT j;

   if (gap < 1)   /* already done if lda == M */
      return;

//fprintf(stderr, "ExpandBlock, M=%d, N=%d, A=%p, lda=%d\n", M, N, A, lda);
   a = A + N*M - 1;
   c = A + N*lda - 1;
   for (j=N; j; j--)
   {
      TYPE *stop = c - gap;
      do 
         *c-- = ATL_rzero;
      while (c != stop);
      stop =  c - M;
      do
         *c-- = *a--;
      while (c != stop);
   }
}
#endif

@ROUT ATL_tgemm_rkK_Np
/*
 * This routine assumes A has already been copied to block-major, row-panel
 * format and had ALPHA applied to it, and threads should now cooperate to:
 * (a) Copy an K8xNB piece of B to their cache in their workspace
 * (b) Use that copied piece against all of A to update a col-panel of C
 * (c) Repeat until all column-panels of B have been applied
 * They determine what col panel of B to operate on using the aNcnt variable.
 */
static void ATL_tloopN
(
@skip   void **aNcnts,       /* NTHR Atomic counters on col panels of C */
   void *aNcnt,         /* Atomic counter on col panels of C */
   int iam,             /* my rank; */
   ATL_CINT nfMblks,    /* # of full NB blocks in Mp */
   int mr,              /* M%NB */
   ATL_CINT nfNblks,    /* # of full NB blocks along N */
   int nr,              /* N%NB */
@skip   int nlblks,          /* nfNblks / ATL_NTHREADS */
@skip   int nrblks,          /* nfNblks % ATL_NTHREADS */
   ATL_CINT nfKblks,    /* # of full NB blocks along K8 */
   int kr,              /* K%NB */
   int krpad,           /* # of entries to add to K-dim to make mul of 8 */
   const TYPE *A,       /* A in blk-major, row-panel (K8xMB) format */
   enum ATLAS_TRANS TB, /* Transpose setting of B */
   const TYPE *B,       /* B in original column-major format */
   ATL_CINT ldb,        /* leading dim of B */
   TYPE *pB,            /* my private workspace to copy col-panels into */
   TYPE *C,             /* original C matrix */
   ATL_CINT ldc         /* leading dim of C */
)
{
   ATL_CINT M = nfMblks*NB+mr, N = nfNblks*NB+nr, K = nfKblks*NB+kr, K8=K+krpad;
   ATL_CINT kr8 = kr+krpad, nnblks = nr ? nfNblks+1 : nfNblks;
   ATL_CINT kr8f = (kr8 >= NB) ? 0 : kr8;
   ATL_CINT nfKblksf = (kr8 >= NB) ? nfKblks+1 : nfKblks;
   ATL_INT jblk, p;
   ATL_CINT BNOTRANS = (TB == AtlasNoTrans);
   ATL_CINT bmul = (BNOTRANS) ? ldb : 1, nNblks=nfNblks+1;

   while (jblk = ATL_DecGlobalAtomicCount(aNcnt, iam))
   {
      ATL_INT nb, nnb, nbr;
      size_t j;

      if (jblk != nNblks)
      {
         nnb = 1;
         nbr = 0;
         nb = NB;
      }
      else
      {
         nnb = 0;
         nbr = nr;
         nb = nr;
      }
      j = (jblk-1)*(NB SHIFT);
/*
 *    Copy the specified col-panel to my private workspace
 */
      if (BNOTRANS)
         Mjoin(PATL,col2blk_a1)(K, nb, B+j*ldb, ldb, pB, ATL_rone);
      else
         Mjoin(PATL,row2blkT_a1)(K, nb, B+j, ldb, pB, ATL_rone);
/*
 *    If kr is nonzero and not a multiple of 8, pad K with K8-K zeros
 *    This allows us to cause less K-cleanup kernels to be loaded, as
 *    well as ensuring we keep things aligned for vectorized cleanup kernels
 *    This will tend to depress perf when kr is small, and improve it when
 *    kr is large and not a multiple of the vector length
 */
      if (krpad)
         ExpandBlock(kr, nb, pB+nfKblks*NB*nb, kr8);
/* 
 *    Perform the rank-K8 update of the column panel of C
 */
      DoMM_NMK(nfMblks, mr, nnb, nbr, nfKblksf, kr8f, A, pB, C+j*ldc, ldc);
   }
}

@ROUT ATL_tgemm_rkK_Np
void Mjoin(PATL,DoWork_rkK_Np)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/* 
 * This routine has everyone cooperate to copy row-panels of A, and then
 * calls tloopN to perform the rank-K update
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_RKK_NP_t *pd=lp->vp;
   ATL_CINT iam = tp->rank;
   volatile int *chkin = pd->chkin;
   TYPE *Bw=pd->Bws[iam], *Aw=pd->Aw;
   void *aMcnt=pd->aMcnt;
   ATL_CINT K = pd->K;
   ATL_CINT BNOTRANS = (pd->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pd->TA == AtlasNoTrans);
   const TYPE *A=pd->A, *B=pd->B;
   TYPE *C = pd->C;
   SCALAR alpha = pd->alpha;
   ATL_CINT nMb=pd->nMb, mr=pd->mr, nKb=pd->nKb, kr=pd->kr, kr8=pd->kr8;
   ATL_CINT nablks = (mr) ? nMb+1 : nMb, incA = (nKb*NB + kr8)*NB;
   ATL_CINT lda=pd->lda, amul = (ANOTRANS) ? (1 SHIFT) : (lda SHIFT);
   ATL_INT iblk;
   MAT2BLK A2blk;
   int k;

   if (ANOTRANS)
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,row2blkT_a1) : Mjoin(PATL,row2blkT_aX);
   else
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);
/*
 * Use the AtomicCounter aMcnt to copy the nMb full row panels of A, 
 * and possibly one partial mr-wide row panel.  Since K is not long, copying
 * them a rowpanel at a time shouldn't kill us on the TLB.
 */
   while(iblk = ATL_DecGlobalAtomicCount(aMcnt, iam))
   {
      int mb;
      size_t i, ia;

      mb = (iblk-- == nablks && mr) ? mr : NB;
      i = iblk*NB;
      i *= amul;
      A2blk(K, mb, A+i, lda, Aw+iblk*incA, alpha);
      if (kr8 != kr)
         ExpandBlock(kr, mb, Aw+iblk*incA+nKb*NB*mb, kr8);
   }
/*
 * Tell everyone I have finished copying A, and then loop until everyone
 * else signals they've copied their pieces
 */
   chkin[iam] = 1;
   for (k=0; k < ATL_NTHREADS; k++)
      while(!chkin[k]) ATL_POLL;
/*
 * Perform the rank-K update with a fully-copied A
 */
   ATL_tloopN(pd->aNcnt, iam, nMb, mr, pd->nNb, pd->nr, 
              nKb, kr, kr8-kr, Aw, pd->TB, pd->B, pd->ldb, pd->Bws[iam], 
              pd->C, pd->ldc);
}
@ROUT ATL_tgemm_rkK
void Mjoin(PATL,DoWork_rkK)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/* 
 * This routine has everyone cooperate to copy row-panels of A, and then
 * loops over atomic counters on N & M to perform the rank-K update
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_RKK_t *pd=lp->opstruct;
   ATL_CINT iam = tp->rank;
   volatile int *chkin = pd->chkin;
   TYPE *Bw=pd->Bws[iam], *Aw=pd->Aw;
   void *aMcnt=pd->aMcnt, *aNcnt=pd->aNcnt, **aMcnts = pd->aMcnts;
   ATL_CINT K = pd->K;
   ATL_CINT BNOTRANS = (pd->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pd->TA == AtlasNoTrans);
   const TYPE *A=pd->A, *B=pd->B;
   TYPE *C = pd->C;
   #ifdef TREAL
      TYPE alpha = pd->alpha;
      TYPE beta  = pd->beta;
   #else
      const SCALAR alpha = pd->alpha;
      const SCALAR beta  = pd->beta;
   #endif
   ATL_CINT nMb=pd->nMb, mr=pd->mr, nNb=pd->nNb, nr=pd->nr;
   ATL_CINT nKb=pd->nKb, kr=pd->kr, kr8=pd->kr8, krpad = kr8-kr;
   size_t incA = (nKb*KB + kr8)*(MB SHIFT), ldc = pd->ldc;
   size_t lda=pd->lda, amul = (ANOTRANS) ? (1 SHIFT) : (lda SHIFT);
   size_t ldb=pd->ldb, P=ATL_NTHREADS;
   ATL_CINT tnMblks = (mr) ? nMb+1 : nMb, tnNblks = (nr) ? nNb+1 : nNb;
   ATL_CINT kr8f = (kr8 >= NB) ? 0 : kr8;
   ATL_CINT nfKblks = (kr8 >= NB) ? nKb+1 : nKb;
   ATL_CINT bmul = (BNOTRANS) ? (ldb SHIFT) : (1 SHIFT);
   ATL_INT iblk, jblk;
/*   #define PRINTTOTALS */
   #ifdef PRINTTOTALS
      int myblks = 0, hisblks=0;
      static int myarr[ATL_NTHREADS], hisarr[ATL_NTHREADS];
   #endif
   MAT2BLK A2blk;
   #ifdef TREAL
      MAT2BLK B2blk = (BNOTRANS) ? Mjoin(PATL,col2blk_a1) : 
                                   Mjoin(PATL,row2blkT_a1);
   #else
      MAT2BLK B2blk = (BNOTRANS) ? Mjoin(PATL,col2blk_a1) : 
                                  (pd->TB == AtlasTrans) ?
                                   Mjoin(PATL,row2blkT_a1):
                                   Mjoin(PATL,row2blkC_a1);
   #endif
   int k;
   #ifdef TCPLX
      const TYPE one[2] = {ATL_rone, ATL_rzero};
   #else
      #define one ATL_rone
   #endif
#ifdef TREAL
   if (ANOTRANS)
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,row2blkT_a1) : Mjoin(PATL,row2blkT_aX);
   else
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);
#else
   if (ANOTRANS)
   {
      if (alpha[1] == ATL_rzero)
      {
         if (*alpha == ATL_rone)
            A2blk = Mjoin(PATL,row2blkT_a1);
         else
            A2blk = Mjoin(PATL,row2blkT_aXi0);
      }
      else
         A2blk = Mjoin(PATL,row2blkT_aX);
   }
   else if (pd->TA == AtlasConjTrans)
   {
      if (alpha[1] == ATL_rzero)
      {
         if (*alpha == ATL_rone)
            A2blk = Mjoin(PATL,col2blkConj_a1);
         else
            A2blk = Mjoin(PATL,col2blkConj_aXi0);
      }
      else
         A2blk = Mjoin(PATL,col2blkConj_aX);
   }
   else  /* TA == AtlasTrans */
   {
      if (alpha[1] == ATL_rzero)
      {
         if (*alpha == ATL_rone)
            A2blk = Mjoin(PATL,col2blk_a1);
         else
            A2blk = Mjoin(PATL,col2blk_aXi0);
      }
      else
         A2blk = Mjoin(PATL,col2blk_aX);
   }
#endif
/*
 * Use the AtomicCounter aMcnt to copy the nMb full row panels of A, 
 * and possibly one partial mr-wide row panel.  Since K is not long, copying
 * them a rowpanel at a time shouldn't kill us on the TLB.
 */
   while(iblk = ATL_DecGlobalAtomicCount(aMcnt, iam))
   {
      int mb;
      size_t i, ia;

      mb = (iblk-- == tnMblks && mr) ? mr : MB;
      i = iblk*NB;
      i *= amul;
      A2blk(K, mb, A+i, lda, Aw+iblk*incA, alpha);
      #ifdef TREAL
         if (kr8 != kr)
            ExpandBlock(kr, mb, Aw+iblk*incA+nKb*KB*mb, kr8);
      #endif
   }
/*
 * Tell everyone I have finished copying A, and then loop until everyone
 * else signals they've copied their pieces
 */
   if (iam == 0)
   {
      for (k=1; k < P; k++)
         while(!chkin[k]) 
            ATL_POLL;
      chkin[0] = 1;
   }
   else
   {
      chkin[iam] = 1;
      while (!chkin[0]);
   }
/*
 * Perform the rank-K update with a fully-copied A by looping over column-panels
 * of C in reverse order
 */
   ATL_mutex_lock(pd->Mlocks[iam]);
   while (jblk = ATL_DecGlobalAtomicCount(aNcnt, iam))
   {
      ATL_INT nb, pL;
      size_t j;
      void *aCrow;
      TYPE *c;

      nb = (jblk != tnNblks || !nr) ? NB : nr;
      pd->Js[iam] = j = (jblk-1)*NB;
      c = C + j*(ldc SHIFT);
/*
 *    Copy the specified column-panel of B to my private workspace
 */
      B2blk(K, nb, B+j*bmul, ldb, Bw, one);
/*
 *    If kr is nonzero and not a multiple of 8, pad K with K8-K zeros
 *    This allows us to cause less K-cleanup kernels to be loaded, as
 *    well as ensuring we keep things aligned for vectorized cleanup kernels
 *    This will tend to depress perf when kr is small, and improve it when
 *    kr is large and not a multiple of the vector length
 */
      #ifdef TREAL
      if (krpad)
         ExpandBlock(kr, nb, Bw+nKb*KB*nb, kr8);
      #endif
/*
 *    Given that I'm working on col-panel j, determine the percentage of its
 *    blocks that I reserve for myself based on how many columns are left.
 *    If there are plenty of columns left, do all local counters which will
 *    reduce the counter cost by something like a factor of 10.  If we are
 *    getting close to running out of col-panels, reserve less and less of
 *    the problem for my exclusive use.
 */
      if (jblk >= P+P)
         pL =  100;
      else if (jblk <= 2)
         pL = 0;
      else
         pL = (jblk > P) ? 50 : 100/P;
      aCrow = aMcnts[iam];
      ATL_ResetGlobalAtomicCount(aCrow, tnMblks, pL);
      ATL_mutex_unlock(pd->Mlocks[iam]);
      while (iblk = ATL_DecGlobalAtomicCount(aCrow, 0))
      {
         const int mb = (!mr || iblk != 1) ? MB : mr;
         const size_t i = (tnMblks-iblk)*(NB SHIFT);
         #ifdef PRINTTOTALS
            myblks++;
         #endif

         iblk = tnMblks - iblk;
         DoMM_K(mb, nb, nfKblks, kr8f, Aw+iblk*incA, Bw, beta, c+i, ldc);
      }
      ATL_mutex_lock(pd->Mlocks[iam]);
   }
   ATL_mutex_unlock(pd->Mlocks[iam]);
   chkin[iam] = -3;  /* let everyone know I've finished my columns */
/*
 * When no more col-panels of C are available, it is time to see if I can
 * help other workers finish their columns; As long as someone hasn't
 * signaled his completion (negative # in chkin), continue trying to steal.
 */
   do
   {
/*
 *    If anyone is still working, continue looking to steal his work
 */
      for (k=0; k < P && chkin[k] <= 0; k++);
      if (k == P)
         break;    /* everyone done, quit */
      for (; k < P; k++)
      {
         const int rk = k;
         void *aCrow = aMcnts[rk];
    
         Bw = pd->Bws[rk];
         ATL_mutex_lock(pd->Mlocks[rk]);
         if (ATL_GetGlobalAtomicCount(aCrow, 1))
         {
            TYPE *c = C + pd->Js[rk]*(((size_t)ldc)SHIFT);
            int nb;
   
            nb = pd->N - pd->Js[rk];
            nb = Mmin(NB, nb);
            while (iblk = ATL_DecGlobalAtomicCount(aCrow, 1))
            {
               const int mb = (iblk != 1 || !mr) ? MB : mr;
               const size_t i = (tnMblks-iblk)*(NB SHIFT);
      
               #ifdef PRINTTOTALS
                  hisblks++;
               #endif
               iblk = tnMblks - iblk;
               DoMM_K(mb, nb, nfKblks, kr8f, Aw+(iblk*incA), Bw, beta,
                      c+i, ldc);
            }
         }
         ATL_mutex_unlock(pd->Mlocks[rk]);
      }
   }
   while(1);
/*
 * Master process is waiting on thread 0, so 0 stays here until all nodes
 * complete their operations
 */
   chkin[iam] = -2;
   #ifdef PRINTTOTALS
      myarr[iam] = myblks;
      hisarr[iam] = hisblks;
   #endif
   if (pd->Sync0 && !iam)
   {
      #ifdef PRINTTOTALS
         int lblks, rblks;
      #endif
      for (k=1; k < P; k++)
         while(chkin[k] != -2)
            ATL_POLL;
      #ifdef PRINTTOTALS
          printf(" myblks : %4d", myarr[0]);
          lblks = myarr[0];
          for (k=1; k < P; k++)
          {
             lblks += myarr[k];
             printf(",%4d", myarr[k]);
          }
          printf(" = %d\n", lblks);
          printf("hisblks : %4d", hisarr[0]);
          rblks = hisarr[0];
          for (k=1; k < P; k++)
          {
             rblks += hisarr[k];
             printf(",%4d", hisarr[k]);
          }
          printf(" = %d\n", rblks);
          printf("Total = %d, expected=%d\n", lblks+rblks, 
                 ((pd->M+NB-1)/NB)*((pd->N+NB-1)/NB));
      #endif
   }
}
@ROUT  ATL_tgemm_bigMN_Kp
void Mjoin(PATL,DoWork_bigMN_Kp)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp = vp;
   const int iam = tp->rank, P = tp->P;
   ATL_TGEMM_RKK_t *pd = lp->opstruct;
   volatile int *chkin = pd->chkin+P, *hischk = pd->chkin;
   ATL_CINT K = pd->K, Kp = pd->kr, nkb = Kp / NB;
   ATL_CINT nnb = (pd->nr) ? pd->nNb+1 : pd->nNb;
   ATL_CINT nmb = (pd->mr) ? pd->nMb+1 : pd->nMb;
   const size_t incA = (pd->TA == AtlasNoTrans) ? (pd->lda SHIFT) : (1 SHIFT); 
   const size_t incB = (pd->TB == AtlasNoTrans) ? (1 SHIFT) : (pd->ldb SHIFT);
   ATL_INT k, kb;
   int i, n;
   const TYPE *A = pd->A, *B = pd->B;
   void Mjoin(PATL,DoWork_rkK)(ATL_LAUNCHSTRUCT_t *lp, void *vp);
   #ifdef TCPLX
      const TYPE one[2] = {ATL_rone, ATL_rzero};
   #else
      #define one ATL_rone
   #endif

   for (k=0; k < K; k += Kp)
   {
      kb = K - k;
      kb = Mmin(kb, Kp);
/*
 *    To avoid race conditions, thread 0 sets everything up, and rest of
 *    threads await his signal to begin
 */
      if (iam == 0)
      {
         n = chkin[0] + 1;
         for (i=1; i < P; i++)
            while(chkin[i] < n)
               ATL_POLL;
         for (i=0; i < P; i++)
            hischk[i] = 0;
         pd->beta = (k) ? one : pd->beta;
         pd->A = A + k*incA;
         pd->B = B + k*incB;
         ATL_ResetGlobalAtomicCount(pd->aNcnt, nnb, 0);
         ATL_ResetGlobalAtomicCount(pd->aMcnt, nmb, 0);
         pd->K = kb;
         if (kb == Kp)
         {
            pd->kr = pd->kr8 = 0;
            pd->nKb = nkb;
         }
         else
         {
            pd->nKb = kb / KB;
            pd->kr = kb - pd->nKb * KB;
            #ifdef TREAL
               pd->kr = kb - pd->nKb * KB;
               pd->kr8 = ((pd->kr+7)>>3)<<3;
               if (pd->kr8 > KB)
                  pd->kr8 = KB;
            #else
               pd->kr8 = pd->kr = kb - pd->nKb * KB;
            #endif
         }
         chkin[0] = n;
      }
      else
      {
         n = ++chkin[iam];
         while (chkin[0] < n)
            ATL_POLL;
      }
      Mjoin(PATL,DoWork_rkK)(lp, vp);  /* do rank-K update */
   }
/*
 * Master process waiting on thread 0, who therefore must block until everyone
 * signals completion
 */
   n = ++chkin[iam];
   if (!iam)
      for (i=1; i < P; i++)
         while (chkin[i] < n)
            ATL_POLL;
}
#ifdef TREAL
   #undef one
#endif
   

int Mjoin(PATL,tgemm_bigMN_Kp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
/*
 * This routine handles the asymtotic case where A & B are too large to be
 * copied at once, and we loop over CacheEdge-sized partitions of K.
 * Implements very large gemm as a series of synchronized rank-Kp updates.
 */
{
   ATL_TGEMM_RKK_t pd;   /* problem definition */
   size_t sz, Kp;
   void *vp;
   int i;
   #ifdef FindingCE
      extern int FoundCE, CompCE;
      #define MY_CE FoundCE
   #else
      #define MY_CE CacheEdge
   #endif

   #ifdef TREAL
      Kp = (ATL_DivBySize(MY_CE) - MB*NB)/(MB + 2*NB);
   #else
      Kp = (ATL_DivBySize(MY_CE) - 2*MB*NB)/(2*MB + 4*NB);
   #endif
   Kp = (Kp/KB)*KB;
   #ifdef FindingCE
      if (MY_CE == 0)
         Kp = K;
      if (Kp < KB)
         Kp = KB;
      if (CompCE)
      {
         CompCE = Kp;
         return;
      }
   #else
      if (Kp < KB)
         return(1);  /* not going to be efficient */
   #endif

   pd.kr = Kp;
   pd.TA = TA;
   pd.TB = TB;
   pd.M = M;
   pd.N = N;
   pd.K = K;
   pd.alpha = alpha;
   pd.A = A;
   pd.lda = lda;
   pd.B = B;
   pd.ldb = ldb;
   pd.beta = beta;
   pd.C = C;
   pd.ldc = ldc;
   pd.nMb = M / MB;
   pd.mr = M - pd.nMb*MB;
   pd.nNb = N / NB;
   pd.nr = N - pd.nNb*NB;

   sz = ATL_MulBySize(Kp)*M + ATL_Cachelen;  /* sizeof A workspace */
   sz += ATL_NTHREADS*(Kp*ATL_MulBySize(NB)+ATL_Cachelen);  /* B workspaces */
   sz += ATL_NTHREADS*3*sizeof(int);  /* chkin1/2 & Js arrays */
   sz += ATL_NTHREADS*sizeof(TYPE*);  /* Bws array */
   sz += ATL_NTHREADS*2*sizeof(void*); /* aMcnts & Mlocks arrays */
   if (sz > ATL_NTHREADS*ATL_PTMAXMALLOC+ATL_MaxMalloc)
      return(2);  /* can't allocate space */
   pd.Bws = malloc(sz);
   if (!pd.Bws)
      return(3);
   pd.aMcnts = (void**)(pd.Bws+ATL_NTHREADS);
   pd.Mlocks = (pd.aMcnts+ATL_NTHREADS);
   pd.chkin = (volatile int*)(pd.Mlocks+ATL_NTHREADS);
   pd.Js = (int*)(pd.chkin + 2*ATL_NTHREADS);
   pd.Aw = (TYPE *) (pd.Js + ATL_NTHREADS);
   pd.Aw = ATL_AlignPtr(pd.Aw);
   pd.Sync0 = 0;
   vp = pd.Aw + M * (Kp SHIFT); 
   pd.Bws[0] = ATL_AlignPtr(vp);
   for (i=1; i < ATL_NTHREADS; i++)
   {
      vp = pd.Bws[i-1] + (NB SHIFT)*Kp;
      pd.Bws[i] = ATL_AlignPtr(vp);
   }
   for (i=0; i < ATL_NTHREADS; i++)
   {
      pd.Mlocks[i] = ATL_mutex_init();
      pd.aMcnts[i] = ATL_SetGlobalAtomicCount(1, 0, 0);
      pd.chkin[i] = pd.chkin[ATL_NTHREADS+i] = 0;
      pd.Js[i] = 0;
   }
   pd.aMcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, 1, 0);
   pd.aNcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, 1, 0);

   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_bigMN_Kp), &pd, NULL);
/*
 * Free allocated resources
 */
   ATL_FreeGlobalAtomicCount(pd.aMcnt);
   ATL_FreeGlobalAtomicCount(pd.aNcnt);
   for (i=0; i < ATL_NTHREADS; i++)
   {
      ATL_mutex_free(pd.Mlocks[i]);
      ATL_FreeGlobalAtomicCount(pd.aMcnts[i]);
   }
   free(pd.Bws);
   return(0);
}
@ROUT ATL_tgemm_rkK_Np 
   @define sf @_Np@
@ROUT ATL_tgemm_rkK
   @define sf @@
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
int Mjoin(PATL,tgemm_rkK@(sf))
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
/*
 * Does a rank-K update on dynamically scheduled column panels of C
 */
{
   ATL_TGEMM_RKK@up@(sf)_t pd;   /* problem definition */
   size_t sz;
   volatile int *chkin;
   TYPE **Bws, *Aw;
   ATL_CINT nKb = ATL_DivByNB(K), kr = K - ATL_MulByNB(nKb);
   ATL_CINT K8 = ATL_MulByNB(nKb) + (((kr+7)>>3)<<3);
   ATL_INT nlblks, nrblks;
   int i, nDb, dr;
   void **acnts;

@ROUT ATL_tgemm_rkK_Np
   sz = ATL_MulBySize(K8)*(M + NB*ATL_NTHREADS) + ATL_Cachelen +
        ATL_NTHREADS*(sizeof(TYPE*)+sizeof(int)+ATL_Cachelen);
@ROUT ATL_tgemm_rkK
   sz = ATL_MulBySize(K8)*(M + NB*ATL_NTHREADS) + ATL_Cachelen +
        ATL_NTHREADS*(sizeof(TYPE*)+2*sizeof(int)+ATL_Cachelen+2*sizeof(void*));
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   if (sz > ATL_NTHREADS*ATL_PTMAXMALLOC)
      return(1);
   Bws = malloc(sz);
   if (!Bws)
      return(2);
   chkin = (volatile int*) (Bws + ATL_NTHREADS);
@ROUT ATL_tgemm_rkK
   pd.Sync0 = 1;
   pd.Js = (int*) (chkin+ATL_NTHREADS);
   acnts = (void**) (pd.Js+ATL_NTHREADS);
   pd.Mlocks = acnts + ATL_NTHREADS;
   Aw = (TYPE*)(pd.Mlocks+ATL_NTHREADS);
@ROUT ATL_tgemm_rkK_Np
   Aw = (TYPE*) (chkin+ATL_NTHREADS);
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   Aw = ATL_AlignPtr(Aw);
   Bws[0] = Aw + K8*(M SHIFT);
   Bws[0] = ATL_AlignPtr(Bws[0]);
@ROUT ATL_tgemm_rkK `   pd.Js[0] = 0;`
   chkin[0] = 0;
   for (i=1; i < ATL_NTHREADS; i++)
   {
      Bws[i] = Bws[i-1] + K8*(NB SHIFT);
      Bws[i] = ATL_AlignPtr(Bws[i]);
      chkin[i] = 0;
@ROUT ATL_tgemm_rkK `      pd.Js[i] = 0;`
   }
   pd.chkin = chkin;
   pd.Aw = Aw;
   pd.Bws = Bws;
   pd.nMb = nDb = ATL_DivByNB(M);
   pd.mr = dr = M - ATL_MulByNB(nDb);
   nDb = (dr) ? nDb+1 : nDb;
   pd.aMcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, nDb, 0);
@ROUT ATL_tgemm_rkK
   for (i=0; i < ATL_NTHREADS; i++)
   {
      pd.Mlocks[i] = ATL_mutex_init();
      acnts[i] = ATL_SetGlobalAtomicCount(1, 0, 0);
   }
   pd.aMcnts = acnts;
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   pd.nNb = nDb = ATL_DivByNB(N);
   pd.nr = dr = N - ATL_MulByNB(nDb);
   pd.aNcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, dr ? nDb+1 : nDb, 0);
   pd.nKb = nKb; pd.kr = kr; 
   #ifdef TREAL
      pd.kr8 = ((kr+7)>>3)<<3;
      if (pd.kr8 > KB)
         pd.kr8 = KB;
   #else
      pd.kr8 = kr;
   #endif
   pd.A = A; pd.B = B; pd.C = C;
   pd.lda = lda; pd.ldb = ldb; pd.ldc = ldc;
   pd.M = M; pd.N = N; pd.K = K;
   pd.TA = TA; pd.TB = TB;
   pd.alpha = alpha; pd.beta = beta;

   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_rkK@(sf)), &pd, NULL);

   ATL_FreeGlobalAtomicCount(pd.aMcnt);
   ATL_FreeGlobalAtomicCount(pd.aNcnt);
@ROUT ATL_tgemm_rkK
   for (i=0; i < ATL_NTHREADS; i++)
   {
      ATL_FreeGlobalAtomicCount(acnts[i]);
      ATL_mutex_free(pd.Mlocks[i]);
   }
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   free(Bws);
   return(0);
}
@ROUT ATL_tgemm_MKp
/*
 * This routine assumes B has already been copied to block-major, column-panel
 * format and had ALPHA applied to it, and threads should now cooperate to:
 * (a) Copy a piece of A to their cache
 * (b) Use that copied piece against all of B to update a row-panel of C
 * (c) Repeat until all rows of A have been applied
 * They determine what row panel of A to operate on using the aAcnt variable.
 */
static void ATL_tloopA
(
   void *aAcnt,         /* Atomic counter on row panels of A */
   int iam,             /* my rank; */
   int nMp,             /* how many M partitions are there? */
   int Mp,              /* size of all but last M partition */
   int mlast,           /* how many elts in last M partition? */
   int nfMblks,         /* # of full NB blocks in Mp */
   int nfNblks,         /* # of full NB blocks along N */
   int nr,              /* N%NB */
   int nfKblks,         /* # of full NB blocks along Kp */
   int kr,              /* K%NB */
   enum ATLAS_TRANS TA,
   const TYPE *A,       /* original A matrix */
   ATL_CINT lda,        /* leading dim of A */
   TYPE *pA,            /* my private workspace to copy row-panels into */
   const TYPE *B,       /* B in blk-major, column-panel format */
   TYPE *C,             /* original C matrix */
   ATL_CINT ldc         /* leading dim of C */
)
{
   ATL_CINT M = ((nMp-1)*Mp)+mlast, N = nfNblks*NB + nr, K = nfKblks*NB + kr;
   ATL_INT iblk;
   size_t i;

   while (iblk = ATL_DecGlobalAtomicCount(aAcnt, iam))
   {
      ATL_INT mb, mymr, mymblks;
      if (iblk-- == nMp)
      {
         mb = mlast;
         mymblks = (mlast/NB);
         mymr = mlast - mymblks*NB;
      }
      else
      {
         mb = Mp;
         mymr = 0;
         mymblks = nfMblks;
      }
      i = iblk*Mp;
/*
 *    Copy the specified row-panel (may be multiple MB-wide row-panels) 
 *    to my private workspace
 */
      if (TA == AtlasNoTrans)
         Mjoin(PATL,row2blkT2_a1)(mb, K, A+i, lda, pA, ATL_rone);
      else
         Mjoin(PATL,col2blk_a1)(K, mb, A+i*lda, lda, pA, ATL_rone);
      DoMM_NMK(mymblks, mymr, nfNblks, nr, nfKblks, kr, pA, B, C+i, ldc);
   }
}

void Mjoin(PATL,DoWork_MKp)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/* 
 * This routine loops over Kp-sized chunks of K
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_MKP_t *pbdef=lp->opstruct;
   ATL_CINT iam = tp->rank;
   ATL_INT k, kb;
   volatile int *chkin = pbdef->chkin;
   TYPE *Bw=pbdef->Bw, *Aw=pbdef->Aws[iam];
   void *aAcnt=pbdef->aAcnt, *aBcnt=pbdef->aBcnt;
   ATL_CINT BNOTRANS = (pbdef->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pbdef->TA == AtlasNoTrans);
   const TYPE *A=pbdef->A, *B=pbdef->B;
   TYPE *C = pbdef->C;
   ATL_CINT M = pbdef->M, N = pbdef->N, K = pbdef->K, klast = pbdef->klast;
   ATL_CINT nNb = pbdef->nNb,  Kp = pbdef->Kp;
   ATL_CINT lda = pbdef->lda, ldb = pbdef->ldb, ldc = pbdef->ldc;
   ATL_CINT nfMblks = pbdef->Mp/NB, nfNblks = N/NB;
   ATL_CINT nr = pbdef->N-nfNblks*NB; 
   ATL_CINT nMp=pbdef->nMp, Mp=pbdef->Mp, mlast=pbdef->mlast;
   ATL_CINT nfKblks0 = Kp / NB;
   ATL_INT nfKblks, kr, i;

/*
 * Threads cooperate to scale C if beta is not one; the unit of work is 1 col
 * After this block, we can always use BETA=1 for all GEMM calls
 */
   if (SCALAR_IS_ZERO(pbdef->beta))
   {
      register int j;
      void *aCcnt = pbdef->aCcnt;
      while (j = ATL_DecGlobalAtomicCount(aCcnt, iam))
      {
         j--;
         Mjoin(PATL,zero)(M, C+ldc*j, 1);
      }
   }
   else if (pbdef->aCcnt)   /* cheap test for BETA != 1.0 */
   {
      register int j;
      const SCALAR beta = pbdef->beta;
      void *aCcnt = pbdef->aCcnt;

      while (j = ATL_DecGlobalAtomicCount(aCcnt, iam))
      {
         j--;
         Mjoin(PATL,scal)(M, beta, C+ldc*j, 1);
      }
   }
/*
 * Loop over partitions of K that have been chosen to fit operands in the
 * cache so that A can be reused across all of B;  go from last to first.
 * This will result in CEIL(K/Kp) writes of C
 */
   kb = klast;
   nfKblks = kb / NB;
   kr = kb - nfKblks * NB;
   if (K > klast)
      ATL_assert(nfKblks0*NB == Kp);
   k = K-klast;
   B += (BNOTRANS) ?  k : k*ldb;
   A += (ANOTRANS) ? k*lda : k;

   for (; k >= 0; k -= Kp)
   {
      int jblk;
/*
 *    If we need row-major access on B, measly parallel speedup not worth
 *    doing row-access, so first node to arrive here just does the entire copy
 */
      if (!BNOTRANS)  /* transpose case */
      {
         if (ATL_DecGlobalAtomicCount(pbdef->aBcnt, iam) == nNb)
         {
            const SCALAR alpha = pbdef->alpha;
            if (SCALAR_IS_ONE(alpha))
               Mjoin(PATL,row2blkT2_a1)(N, kb, B, ldb, Bw, alpha);
            else
               Mjoin(PATL,row2blkT2_aX)(N, kb, B, ldb, Bw, alpha);
         }
         B -= Kp*ldb;
      }
/*
 *    Copy nNb column panels of B in parallel using the atomic counter aBcnt
 */
      else  /* No-Tranpose case */
      {
         const SCALAR alpha = pbdef->alpha;
         MAT2BLK B2blk = (SCALAR_IS_ONE(alpha)) ? 
                         Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);

         while(jblk = ATL_DecGlobalAtomicCount(pbdef->aBcnt, iam))
         {
            ATL_INT nn, j;
            jblk--;
            j = jblk * NB;
            nn = N - j;
            nn = Mmin(nn, NB);
            B2blk(kb, nn, B+j*ldb, ldb, Bw+(size_t)jblk*kb*NB, alpha);
         }
         B -= Kp;
      } 
/*
 *    Node P-1 makes sure everyone has finished, then he sets our counters
 *    (safe since everyone else waiting below) before signaling his completion
 */
      if (iam == ATL_NTHREADS-1)
      {
         const int mycnt = chkin[iam];
         for (i=0; i < ATL_NTHREADS-1; i++)
            while(chkin[i] <= mycnt);
         ATL_ResetGlobalAtomicCount(pbdef->aBcnt, nNb, 0);/* for next k-it */
         ATL_ResetGlobalAtomicCount(pbdef->aAcnt, nMp, 0);/* starting A again */
         chkin[iam]++;
      }
      else
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
      ATL_tloopA(aAcnt, iam, nMp, Mp, mlast, nfMblks, nfNblks, nr, 
                 nfKblks, kr, pbdef->TA, A, lda, Aw, Bw, C, ldc);
      A -= (ANOTRANS) ? Kp*lda : Kp;
      kb = Kp;
      nfKblks = nfKblks0;
      kr = 0;
      if (k)  /* must sync before copying B at top of loop */
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
   }  
@beginskip
   if (0)
   {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
   }
@endskip
}

int Mjoin(PATL,tgemm_MKp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M, 
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   TYPE *Bw, *p;
   int *chkin;
   void *amcnt, *vp;
   TYPE **Aws;
   size_t wrksz;
   ATL_INT Kp, Mp, i, k, m, klast, mlast, nKp, nMp, nNb;
   ATL_TGEMM_MKP_t pbdef;

@beginskip
   if (0)
   {
      TYPE *pB, *pA;
      void *vp;

      vp = malloc(M*K*sizeof(TYPE) + N*K*sizeof(TYPE) + 2*ATL_Cachelen);
      ATL_assert(vp);
      pB = ATL_AlignPtr(vp);
      pA = pB + N*K;
      pA = ATL_AlignPtr(pA);
      if (TB == AtlasNoTrans)
      {
         if (SCALAR_IS_ONE(alpha))
            Mjoin(PATL,col2blk_a1)(K, N, B, ldb, pB, alpha);
         else
            Mjoin(PATL,col2blk_aX)(K, N, B, ldb, pB, alpha);
      }
      else if (SCALAR_IS_ONE(alpha))
         Mjoin(PATL,row2blkT2_a1)(N, K, B, ldb, pB, alpha);
      else
         Mjoin(PATL,row2blkT2_aX)(N, K, B, ldb, pB, alpha);
      if (TA == AtlasNoTrans)
         Mjoin(PATL,row2blkT2_a1)(M, K, A, lda, pA, ATL_rone);
      else
         Mjoin(PATL,col2blk_a1)(K, M, A, lda, pA, ATL_rone);
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,gescal)(M, N, beta, C, ldc);
      DoMM_NMK(M/NB, M-(M/NB)*NB, N/NB, N-(N/NB)*NB, K/KB, K-(K/KB)*KB,
               pA, pB, C, ldc);
      free(vp);
      return(0);
   }
@endskip
/*
 * =====================================================================
 * Compute the Kp and Mp partitionings.  We want all our operands to fit
 * in cache, assuming algorithmic movement and LRU replacement, so:
 *    Mp*Kp + 2*(NB*Kp + Mp*NB) = CE
 * =====================================================================
 */
/*
 * See if we must cut K in order to fit in CE; if so, Mp=MB=NB, and:
 *    MB*Kp + 2*(NB*Kp + MB*NB) = CE  -> Kp = (CE - MB*NB)/(MB + 2*NB)
 *      A          B       C
 */
   if (MB*K + 2*NB*K + MB*NB > CacheEdge)  /* Mp=MB; solve for Kp */
   {
      Kp = (CacheEdge - MB*NB)/(MB + 2*NB);
      Kp = (Kp > KB) ? (Kp/KB)*KB : KB;
      Mp = MB;
   }
/*
 * Otherwise, Kp = K, and we solve for Mp
 *    Mp*K + 2*(NB*K + Mp*NB) = CE  -> Mp = (CE - 2*NB*K) / (K+NB)
 *     A         B       C
 */
   else  /* Kp = K, solve for Mp */
   {
      Mp = (CacheEdge - 2*NB*K) / (K+NB);
      if (Mp > MB)
      {
         while (M/Mp < 3*ATL_NTHREADS)
            Mp >>= 1;
         Mp = (Mp > MB) ? (Mp/MB)*MB : MB;
      }
      else
         Mp = MB;
      Kp = K;
   }

/*
 * Find size of partial block at end.  If it is <= NB, absorb it into previous
 * block and have no partial block for both M & K
 */
   nKp = K / Kp;          /* floor(K/Kp) */
   klast = K - nKp*Kp;
   if (klast)
   {
      if (klast < NB)
         klast += Kp;
      else
         nKp++;         /* nKp now includes klast */
   }
   else
      klast = Kp;  /* K is even multiple of Kp */
   nMp = M / Mp;        /* floor (K/Kp) */
   mlast = M - Mp*nMp;
   if (mlast)
   {
      if (mlast < MB)
         mlast += Mp;   /* absorb partial block into last part */
      else
         nMp++;         /* nMp now includes all partitions */
   }
   else
      mlast = Mp;    /* M even multiple of Mp */
   nNb = (N+NB-1)/NB;  /* ceil(N/NB) */
/* 
 * Worksize: let k = MAX(Kp,CEIL(klast/NB)*NB), then:
 * All threads share the N*k B workspace, which is split into
 * nNb column panels; we round the B workspace up as if N were a multiple of NB
 * Then all threads need a private k*NB piece of A, and finally we need
 * nthr-len integer checkin array and a nthr-len array of pointers, which
 * we use to pass all the A work pointers to the worker nodes.
 */
   k = Mmax(Kp, klast);
   m = Mmax(Mp, mlast);
   wrksz = (2+ATL_NTHR)*ATL_Cachelen + ATL_MulBySize(NB)*nNb*k + 
           ATL_NTHR*(sizeof(int)+sizeof(TYPE*) + ATL_MulBySize(m)*k);
   if (ATL_NTHR*ATL_PTMAXMALLOC < wrksz)
      return(1);
   vp = malloc(wrksz);
   if (!vp)
      return(2);
   Aws = vp;                            /* A work ptrs first array */
   chkin = (int*)(Aws+ATL_NTHR);        /* then checkin array */
   Bw = (TYPE*)(chkin+ATL_NTHR);        /* then workspace for B */
   Bw = ATL_AlignPtr(Bw);               /* B must be aligned */
   chkin[0] = 0;
   p = Bw + k*N;                        /* first A wrkspc after B wrkspc */
   Aws[0] = ATL_AlignPtr(p);            /* A wrkspcs must be aligned */
   for (i=1; i < ATL_NTHR; i++)         /* init rest of nthr-len arrays */
   {
      chkin[i] = 0;
      p = Aws[i-1] + m*k;
      Aws[i] = ATL_AlignPtr(p);
   }
   pbdef.chkin = chkin;
   pbdef.alpha = alpha; pbdef.beta = beta;
   pbdef.A = A; pbdef.B = B; pbdef.C = C;
   pbdef.M = M; pbdef.N = N; pbdef.K = K;
   pbdef.lda = lda; pbdef.ldb = ldb; pbdef.ldc = ldc;
   pbdef.Kp = Kp; pbdef.klast = klast; pbdef.nKp = nKp;
   pbdef.Mp = Mp; pbdef.mlast = mlast; pbdef.nMp = nMp;
   pbdef.nNb = nNb;
   pbdef.TA = TA; pbdef.TB = TB;
   pbdef.Bw = Bw; pbdef.Aws = Aws;
   pbdef.aBcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, nNb, 0);
   pbdef.aAcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, nMp, 0);
   if (SCALAR_IS_ONE(beta))
      pbdef.aCcnt = NULL;
   else
      pbdef.aCcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, N, 0);
//   printf("Mp=%d, nMp=%d, mlast=%d, Kp=%d, nKp=%d, klast=%d\n", 
//          Mp, nMp, mlast, Kp, nKp, klast);
   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_MKp), &pbdef, NULL);
   free(vp);
   return(0);
}
@ROUT ATL_tgemm2
#include "atlas_misc.h"
#include "atlas_lvl3.h"
#include "atlas_threads.h"
void Mjoin(PATL,tgemm2)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M, 
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
/*
 * See if it looks like a rank-K problem that can be split on N */
 */
   if (K <= 4*NB && N > ATL_NTHREADS*NB && M < N+N) /* rank-K split on N */
   {
      if (!Mjoin(PATL,tgemm_Np)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * Is it a problem that can really only be split on K?
 */
   if ( (K > 4*ATL_NTHREADS*NB && M < 2*NB && N < 2*NB) || 
        (K >= 4*M && K >= 4*N && M*(size_t)N*K*sizeof(TYPE) < ATL_PTMAXMALLOC) )
   {
      if (!Mjoin(PATL,tgemm_Kp)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * Is it a problem that can easily be split solely on M
 */
   if (M > N && M > 4*ATL_NTHREADS*NB)
   {
      if (!Mjoin(PATL,tgemm_Mp)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * Is it a problem that can easily be split solely on N
 */
   if (N > M && N > 4*ATL_NTHREADS*NB)
   {
      if (!Mjoin(PATL,tgemm_Np)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * For low-rank problems, use 3-d distribution with minimal copying.  This
 * should be a specialized routine where we know we can copy the entire
 * problem up front; no-copy routs should be handled by 1-D distros
 */
/*
 * Large-rank problems using 3-D with work stealing.
 */

/*
 * Otherwise, call serial gemm; this will later be replaced by _MpNp, which
 * is not allowed to fail (return non-zero).  Can we use work-stealing?
 */
   Mjoin(PATL,gemm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
}
@ROUT ATL_tgemm_Kp
   @define D @K@
   @define I @k@
@ROUT ATL_tgemm_Np
   @define D @N@
   @define I @j@
@ROUT ATL_tgemm_Mp
   @define D @M@
   @define I @i@
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
@define d @@low@(D)@
#include "atlas_misc.h"
#include "atlas_lvl3.h"
#include "atlas_threads.h"
typedef struct
{
@ROUT ATL_tgemm_Kp
   TYPE **Cws;            /* P-len array of C work areas, Cws[0] = C */
   void *chklck;          /* 0/1 ctr serves as lock for chkin array */
   volatile int *chkin;   /* P-len checkin array, init to 0 */
   ATL_INT ldw;           /* leading dim of workspaces */
   void *lockC;           /* lock around C combine data structs */
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
   void *a@(D)cnt;           /* which @(D) blk are we doing? */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;
   SCALAR beta;
   ATL_INT @(D)p, n@(D)p, @(d)last;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_@(D)PART_t;

@ROUT ATL_tgemm_Kp
static void DoCombine
(
   int iam,               /* rank of this thread */
   ATL_INT M,             /* # of rows in C */
   ATL_INT N,             /* # of cols in C */
   ATL_INT ldc,           /* leading dim of C (>=M) */
   ATL_INT ldw,           /* leading dim of all workspaces */
   volatile int *chkin,   /* check-in array initialized to 0 */
   void *lockC,           /* lock protecting the chkin array */
   TYPE **Cws             /* array of C workspaces to combine */
)
{
   #ifdef TCPLX
      TYPE one[2] = {ATL_rone, ATL_rzero};
   #else
      #define one ATL_rone
   #endif
   int i, combrank;
   ATL_CINT myldc = (iam) ? ldw : ldc;
/*
 * See if there are any C arrays I can combine with mine
 */
   do
   {
      combrank = -1;
      for (i=0; i < ATL_NTHREADS && chkin[i] != 1; i++);
/*
 *    If there are no arrays available, lock the structure and signal that
 *    I have left the building by storing a 1 in my structure
 */
      if (i == ATL_NTHREADS)
      {
         if (iam == 0)  /* rank=zero loops until all matrices are combined */
         {
             for (i=1; i < ATL_NTHREADS && chkin[i] == 3; i++);
             if (i != ATL_NTHREADS)
                continue;
             else
                return;
         }
/*
 *       non-zero nodes simply leave if there are no matrices to combine
 */
         ATL_mutex_lock(lockC);
         for (i=0; i < ATL_NTHREADS && chkin[i] != 1; i++);
         if (i == ATL_NTHREADS)
         {
            chkin[iam] = 1;   /* signal that my array is available to combine */
            ATL_mutex_unlock(lockC);
            return;
         }
         else
         {
            combrank = i;
            chkin[i] = 2;
         }
         ATL_mutex_unlock(lockC);
      }
/*
 *    I found an uncombined array, see if I can take it
 */
      else
      {
         ATL_mutex_lock(lockC);
         if (chkin[i] != 1)  /* if that one is gone, see if another is there */
         {
            for (i=0; i < ATL_NTHREADS && chkin[i] != 1; i++);
         }
         if (i < ATL_NTHREADS)
         {
            combrank = i;
            chkin[i] = 2;
         }
         else if (iam)
         {
            chkin[iam] = 1;   /* signal that my array is available to comb */
            ATL_mutex_unlock(lockC);
            return;
         }
         else   /* nothing to combine & I'm node zero */
         {
            ATL_mutex_unlock(lockC);
            continue; /* nothing more to do until someone checks in */
         }
      }
/*
 *    If I get to here, then combrank should be set, and I must do the combine
 */
      Mjoin(ATL,geadd)(M, N, one, Cws[combrank], ldw, one, Cws[iam], myldc);
      chkin[combrank] = 3;  /* safe since I can only do this after lockC */
   }                        /* and nobody writes to loc with 2 in it but me */
   while(1);
}
#ifndef TCPLX
   #undef one
#endif


@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
void Mjoin(PATL,DoWork_part@(D))(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_@(D)PART_t *pd=lp->vp;
   void *a@(D)cnt = pd->a@(D)cnt;
   const TYPE *A=pd->A, *B=pd->B;
   ATL_CINT iam = tp->rank, ANOTRANS = (pd->TA == AtlasNoTrans);
   ATL_CINT M=pd->M, N=pd->N, K=pd->K;
   ATL_CINT n@(D)p = pd->n@(D)p, @(D)p = pd->@(D)p, @(d)last=pd->@(d)last;
   ATL_CINT lda=pd->lda,  ldb=pd->ldb;
   const enum ATLAS_TRANS TA=pd->TA, TB=pd->TB;
   const SCALAR alpha=pd->alpha; 
@ROUT ATL_tgemm_Mp ATL_tgemm_Np
   TYPE *C=pd->C;
   ATL_CINT ldc = pd->ldc;
   const SCALAR beta=pd->beta;
@ROUT ATL_tgemm_Kp
   ATL_CINT ldc = (iam) ? pd->ldw : pd->ldc;
   TYPE *C = pd->Cws[iam];
   #ifdef TCPLX
      const TYPE one[2] = {ATL_rone, ATL_rzero};
      const TYPE zero[2] = {ATL_rzero, ATL_rzero};
      const TYPE *beta= (iam) ? zero : pd->beta;
   #else
      TYPE beta = (iam) ? ATL_rzero : pd->beta;
   #endif
   volatile int *chkin=pd->chkin;
   ATL_INT kblk;
   ATL_CINT amul = ((TA == AtlasNoTrans || TA == AtlasConj) ? lda : 1)SHIFT;
   ATL_CINT bmul = ((TB == AtlasNoTrans || TB == AtlasConj) ? 1 : ldb)SHIFT;

   while (kblk = ATL_DecAtomicCount(a@(D)cnt))
   {
      const TYPE *a;
      size_t ka, kb;
      ATL_INT i, k;

      if (kblk-- == nKp)
      {
         k = klast;
         i = pd->K - k;
      }
      else
      {
         i = kblk*Kp;
         k = Kp;
      }
      ka = i*amul;
      kb = i*bmul;
      Mjoin(PATL,gemm)(TA, TB, M, N, k, alpha, A+ka, lda, B+ka, ldb, beta, 
                       C, ldc);
      #ifdef TCPLX
         beta = one;
      #else
         beta = ATL_rone;
      #endif
   }
   DoCombine(iam, M, N, pd->ldc, pd->ldw, pd->chkin, pd->lockC, pd->Cws);
@ROUT ATL_tgemm_Np
   ATL_CINT bmul = (TB == AtlasNoTrans) ? (ldb SHIFT) : (1 SHIFT);
   ATL_INT jblk;

   while (jblk = ATL_DecAtomicCount(aNcnt))
   {
      ATL_INT n;
      size_t jb, jc;

      if (jblk-- == nNp)
      {
         n = nlast;
         jc = pd->N - n;
      }
      else
      {
         jc = jblk*Np;
         n = Np;
      }
      jb = jc*bmul;
      jc *= (ldc SHIFT);
      Mjoin(PATL,gemm)(TA, TB, M, n, K, alpha, A, lda, B+jb, ldb, beta, 
                       C+jc, ldc);
   }
@ROUT ATL_tgemm_Mp
   ATL_INT iblk;

   while (iblk = ATL_DecAtomicCount(a@(D)cnt))
   {
      const TYPE *a;
      ATL_INT i, m;

      if (iblk-- == nMp)
      {
         m = mlast;
         i = pd->M - m;
      }
      else
      {
         i = iblk*Mp;
         m = Mp;
      }
      a = (ANOTRANS) ? A+(i SHIFT) : A+i*(lda SHIFT);
      Mjoin(PATL,gemm)(TA, TB, m, N, K, alpha, a, lda, B, ldb, beta, 
                       C+(i SHIFT), ldc);
   }
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
}

int Mjoin(PATL,tgemm_@(D)p)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
/*
 * This routine divides @(D) into roughly 4*P chunks, and calls serial GEMM.
 */
{
   ATL_INT n@(D)p, @(D)p, @(d)last;
   ATL_TGEMM_@(D)PART_t pd;  /* problem definition */
@ROUT ATL_tgemm_Kp 
   ATL_INT ldw, i;
   size_t totsz;
   void *vp=NULL;
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp

   @(D)p = @(D) / (2*ATL_NTHREADS);
   if (@(D)p < NB)
      return(0);
   @(D)p = (@(D)p >= NB+NB) ? @(D)p>>1 : @(D)p;
   n@(D)p = @(D) / @(D)p;
   @(d)last = @(D) - n@(D)p*@(D)p;
   if (@(d)last)
   {
      if (@(d)last < @(D)B)
         @(d)last += @(D)p;   /* absorb partial block into last full partition */
      else
         n@(D)p++;
   }
   else
      @(d)last = @(D)p;       /* last part same size as all parts */
   pd.M = M; pd.N = N; pd.K = K;
   pd.lda = lda; pd.ldb = ldb; pd.ldc = ldc;
   pd.TA = TA; pd.TB = TB;
   #ifdef TCPLX
      pd.alpha = (TYPE*)alpha; pd.beta = (TYPE*)beta;
   #else
      pd.alpha = alpha; pd.beta = beta;
   #endif
   pd.A = A; pd.B = B; pd.C = C;
   pd.@(D)p = @(D)p; pd.n@(D)p = n@(D)p; pd.@(d)last = @(d)last;
   pd.a@(D)cnt = ATL_SetAtomicCount(n@(D)p);
@ROUT ATL_tgemm_Kp
  
   ldw = ((M+7)>>8)<<8;   /* make ldw a multiple of 8 */
   if (!(ldw & (ldw-1)))  /* if ldw is a power of 2 */
      ldw += 8;           /* change it so it is not */
   totsz = (ATL_MulBySize(ldw)*N+ATL_Cachelen) * (ATL_NTHREADS-1) + 
           ATL_NTHREADS*sizeof(TYPE*);
   if (totsz > ATL_PTMAXMALLOC*ATL_NTHREADS)
      return(1);
   pd.Cws = malloc(totsz);
   pd.Cws[0] = C;
   pd.ldw = ldw;
   pd.Cws[1] = (TYPE*) (pd.Cws + ATL_NTHREADS);
   pd.Cws[1] = ATL_AlignPtr(pd.Cws[1]);
   pd.chkin[0] = pd.chkin[1] = 0;  /* at least 2 threads, so safe */
   for (i=2; i < ATL_NTHREADS; i++)
   {
      void *vp;
      const size_t inc=ldw*N;
      vp = pd.Cws[i-1] + inc;
      pd.Cws[i] = ATL_AlignPtr(vp);
      pd.chkin[i] = 0;
   }
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_part@(D)), &pd, NULL);
   return(0);
}
@ROUT ATL_tgemm_MpNpKp
int Mjoin(PATL,tgemm_Mp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   ATL_INT nmblks, nnblks, nkblks, nblks, m, n, k;
   int cutKOK;

   nmblks = nnblks = nkblks = 1;
   m = M; n = N; k = K;
   do
   {
/*
 *    cost of operation : (2*M*N*K)/P
 *    worst case combine: (P*M*N)
 *    If this ratio is large, then we can afford to cut K
 */
      cutKOK = (k+k)/((nkblks+1)*(nkblks+1)) >= 200;
      if (cutKOK && k > m && k > n)
      {
         nkblks++;
         k = K / nkblks;
         if (k < KB)
         {
            nkblks--;
            break;
         }
      }
      else if (n > m)
      {
         nnblks++;
         n = N / nnblks;
         if (n < NB)
         {
            nnblks--;
            break;
         }
      }
      else /* M largest dim, cut it */
      {
         nmblks++;
         m = M / nmblks;
         if (m < MB)
         {
            nmblks--;
            break;
         }
      }
      nblks = nmblks * nnblks * nkblks;
   }
   while (nblks < 2*ATL_NTHREADS);
}
@ROUT ATL_tgemm_rKp
typedef struct
{
   void **aNcnts;         /* P counters on col-panels of C */
   void *aMcnt;           /* count row-panels of A */
   void *donecnt;         /* Counter used to detect if op is complete */
   volatile int *chkin;   /* ATL_NTHREAD-len checkin array */
   volatile int Riam;     /* 1st arriver on cooperative operation */
   volatile int DONE;     /* set by Riam when cooperative op is complete */
   TYPE **Bws;            /* preallocated thread copy areas */
   TYPE *Aw;              /* workspace for common A */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;
   SCALAR beta;
   ATL_INT Kp, nKp, klast;
   ATL_INT nMb, mr, nNb, nr, nlblks, nrblks;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_RKP_t;

void Mjoin(PATL,DoWork_NKp)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/*
 * This routine loops over Kp-sized chunks of K
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_RKP_t *pbdef=lp->vp;
   ATL_CINT iam = tp->rank;
   ATL_INT k, kb;
   volatile int *chkin = pbdef->chkin;
   TYPE *Aw=pbdef->Bw, *Bw=pbdef->Bws[iam];
   void *aAcnt=pbdef->aAcnt, *aBcnt=pbdef->aBcnt;
   ATL_CINT BNOTRANS = (pbdef->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pbdef->TA == AtlasNoTrans);
   const TYPE *A=pbdef->A, *B=pbdef->B;
   TYPE *C = pbdef->C;
   ATL_CINT M = pbdef->M, N = pbdef->N, K = pbdef->K, klast = pbdef->klast;
   ATL_CINT nNb = pbdef->nNb,  Kp = pbdef->Kp;
   ATL_CINT lda = pbdef->lda, ldb = pbdef->ldb, ldc = pbdef->ldc;
   ATL_CINT nfMblks = pbdef->nMb, nfNblks = nNb;
   ATL_CINT nr = pbdef->nr;
   ATL_CINT nfKblks0 = Kp / NB;
   ATL_INT nfKblks, kr, i;

/*
 * Threads cooperate to scale C if beta is not one; the unit of work is 1 col
 * After this block, we can always use BETA=1 for all GEMM calls
 */
   if (SCALAR_IS_ZERO(pbdef->beta))
   {
      register int j;
      int Ciam = iam+1;
      void *aCcnt = pbdef->aCcnt;

      j = ATL_DecAtomicCount(aCcnt);
      if (j == N)
      {
         pbdef.Riam  = iam;  /* I'm responsible for certifying C is finished */
      }
      if (j)
      {
         Mjoin(PATL,zero)(M, C+ldc*(j-1), 1);
         while (j = ATL_DecAtomicCount(aCcnt))
            Mjoin(PATL,zero)(M, C+ldc*(j-1), 1);
      }
      if (iam == pbdef.Riam)
      {
         pbdef.Riam = -1;
      }
      else
         while (!pbdef->DONE);
   }
   else if (pbdef->aCcnt)   /* cheap test for BETA != 1.0 */
   {
      register int j;
      const SCALAR beta = pbdef->beta;
      void *aCcnt = pbdef->aCcnt;

      while (j = ATL_DecAtomicCount(aCcnt))
      {
         j--;
         Mjoin(PATL,scal)(M, beta, C+ldc*j, 1);
      }
   }
/*
 * Loop over partitions of K that have been chosen to fit operands in the
 * cache so that A can be reused across all of B;  go from last to first.
 * This will result in CEIL(K/Kp) writes of C
 */
   kb = klast;
   nfKblks = kb / NB;
   kr = kb - nfKblks * NB;
   if (K > klast)
      ATL_assert(nfKblks0*NB == Kp);
   k = K-klast;
   B += (BNOTRANS) ?  k : k*ldb;
   A += (ANOTRANS) ? k*lda : k;

   for (; k >= 0; k -= Kp)
   {
      int jblk;
/*
 *    If we need row-major access on B, measly parallel speedup not worth
 *    doing row-access, so first node to arrive here just does the entire copy
 */
      if (!BNOTRANS)  /* transpose case */
      {
         if (ATL_DecAtomicCount(pbdef->aBcnt) == nNb)
         {
            const SCALAR alpha = pbdef->alpha;
            if (SCALAR_IS_ONE(alpha))
               Mjoin(PATL,row2blkT2_a1)(N, kb, B, ldb, Bw, alpha);
            else
               Mjoin(PATL,row2blkT2_aX)(N, kb, B, ldb, Bw, alpha);
         }
         B -= Kp*ldb;
      }
/*
 *    Copy nNb column panels of B in parallel using the atomic counter aBcnt
 */
      else  /* No-Tranpose case */
      {
         const SCALAR alpha = pbdef->alpha;
         MAT2BLK B2blk = (SCALAR_IS_ONE(alpha)) ?
                         Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);

         while(jblk = ATL_DecAtomicCount(pbdef->aBcnt))
         {
            ATL_INT nn, j;
            jblk--;
            j = jblk * NB;
            nn = N - j;
            nn = Mmin(nn, NB);
            B2blk(kb, nn, B+j*ldb, ldb, Bw+(size_t)jblk*kb*NB, alpha);
         }
         B -= Kp;
      }
/*
 *    Node P-1 makes sure everyone has finished, then he sets our counters
 *    (safe since everyone else waiting below) before signaling his completion
 */
      if (iam == ATL_NTHREADS-1)
      {
         const int mycnt = chkin[iam];
         for (i=0; i < ATL_NTHREADS-1; i++)
            while(chkin[i] <= mycnt);
         ATL_ResetAtomicCount(pbdef->aBcnt, nNb);  /* reset for next k-it */
         ATL_ResetAtomicCount(pbdef->aAcnt, nMp);  /* starting A again */
         chkin[iam]++;
      }
      else
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
      ATL_tloopA(aAcnt, iam, nMp, Mp, mlast, nfMblks, nfNblks, nr,
                 nfKblks, kr, pbdef->TA, A, lda, Aw, Bw, C, ldc);
      A -= (ANOTRANS) ? Kp*lda : Kp;
      kb = Kp;
      nfKblks = nfKblks0;
      kr = 0;
      if (k)  /* must sync before copying B at top of loop */
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
   }
}
int Mjoin(PATL,tgemm_rKp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   TYPE *Aw, *p;
   void **acnts;
   void *amcnt, *vp;
   TYPE **Bws;
   size_t wrksz;
   ATL_INT Kp, Mp, i, k, m, klast, mlast, nKp, nMp, nNb;
   ATL_TGEMM_RKP_t pbdef;

/*
 * =====================================================================
 * Compute the Kp partitioning.  We want all our operands to fit
 * in cache, assuming algorithmic movement and LRU replacement, so:
 *    M*Kp + 2*(NB*Kp + M*NB) = CE
 * =====================================================================
 */
/*
 * See if we must cut K in order to fit in CE; if so, Mp=MB=NB, and:
 *    MB*Kp + 2*(NB*Kp + MB*NB) = CE  -> Kp = (CE - MB*NB)/(MB + 2*NB)
 *      A          B       C
 */
   if (MB*K + 2*NB*K + MB*NB > CacheEdge)  /* Mp=MB; solve for Kp */
   {
      Kp = (CacheEdge - MB*NB)/(MB + 2*NB);
      Kp = (Kp > KB) ? (Kp/KB)*KB : KB;
   }
   else  /* otherwise, its a rank-K update, call special routine */
      return(Mjoin(PATL,tgemm_rkK_Np)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                      beta, C, ldc));
/*
 * Find size of partial block at end.  If it is <= NB, absorb it into previous
 * block and have no partial block 
 */
   nKp = K / Kp;        /* floor(K/Kp) */
   klast = K - nKp*Kp;
   if (klast)
   {
      if (klast < NB)
         klast += Kp;   /* absorb partial block into last K block */
      else
         nKp++;         /* nKp now includes klast, which is > NB */
   }
   else
      klast = Kp;       /* K is even multiple of Kp */
   nMb = M/NB;          /* floor(M/NB) */
   nNb = N/NB;          /* floor(N/NB) */
   if (nNb < ATL_NTHREADS*2)
      return(3);
/*
 * Worksize: let k = MAX(Kp,CEIL(klast/NB)*NB), then:
 * All threads share the N*k B workspace, which is split into
 * nNb column panels; we round the B workspace up as if N were a multiple of NB
 * Then all threads need a private k*NB piece of A, and finally we need
 * nthr-len integer checkin array and a nthr-len array of pointers, which
 * we use to pass all the A work pointers to the worker nodes.
 */
   k = ((klast+NB-1)/NB)*NB;
   k = Mmax(Kp, k);
   wrksz = (2+ATL_NTHREADS)*ATL_Cachelen + ATL_MulBySize(MB)*nMb*k +
           ATL_NTHREADS*(sizeof(void*)+sizeof(TYPE*) + ATL_MulBySize(NB)*k);
   if (ATL_NTHREADS*ATL_PTMAXMALLOC < wrksz)
      return(1);
   vp = malloc(wrksz);
   if (!vp)
      return(2);
   Bws = vp;                            /* B work ptrs first array */
   acnts = (void*)(Bws+ATL_NTHREADS);   /* then atomic count array */
   Aw = (TYPE*)(acnts+ATL_NTHREADS);    /* then workspace for common A */
   Aw = ATL_AlignPtr(Aw);               /* A must be aligned */
   p = Aw + M*k;                        /* first B wrkspc after A wrkspc */
   Bws[0] = ATL_AlignPtr(p);            /* B wrkspcs must be aligned */
   for (i=1; i < ATL_NTHR; i++)         /* init rest of nthr-len arrays */
   {
      p = Bws[i-1] + N*k;
      Bws[i] = ATL_AlignPtr(p);
   }
   pbdef.alpha = alpha; pbdef.beta = beta;
   pbdef.A = A; pbdef.B = B; pbdef.C = C;
   pbdef.M = M; pbdef.N = N; pbdef.K = K;
   pbdef.lda = lda; pbdef.ldb = ldb; pbdef.ldc = ldc;
   pbdef.Kp = Kp; pbdef.klast = klast; pbdef.nKp = nKp;
   pbdef.nMb = nMb; pbdef.nNb = nNb;
   pbdef.mr = M - nMb * NB; pbdef.nr = N - nNb * NB;
   pbdef.TA = TA; pbdef.TB = TB;
   pbdef.Aw = Aw; pbdef.Bws = Bws;
   i = nr ? nNb+1 : nNb;
   pbdef.aBcnt = ATL_SetAtomicCount(i);
   pbdef.donecnt = ATL_SetAtomicCount(i);
   if (SCALAR_IS_ONE(beta))
      pbdef.aCcnt = NULL;
   else
      pbdef.aCcnt = ATL_SetAtomicCount(N);
//   printf("Mp=%d, nMp=%d, mlast=%d, Kp=%d, nKp=%d, klast=%d\n",
//          Mp, nMp, mlast, Kp, nKp, klast);
   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_NKp), &pbdef, NULL);
   free(vp);
   return(0);
}
@ROUT ATL_gccstackfix
#include "atlas_asm.h"
/*
 * This routine necessary to fix gcc's 32-bit ABI violation:
 *    http://math-atlas.sourceforge.net/errata.html#gccCrazy
 * This assembly is only called on 32-bit Windows, where all windows calls
 * call this wrapper function, which takes a pointer to a void pointer array;
 * the first entry is a function pointer to the function usually threaded,
 * while the second is a pointer to its argument.  This assembly routine
 * manually aligns the stack so that gcc's ABI violation doesn't kill
 * the spawned thread.
 */

/*
 *                       4(%esp) 
 * void *ATL_gccstackfix(void *vp)
 */
        .text
.globl ATL_asmdecor(ATL_gccstackfix)
ALIGN16
ATL_asmdecor(ATL_gccstackfix):
#define FSIZE 12                /* (SP0, arg, func) */
ATL_gccstackfix:
   mov %esp, %eax               /* get unaligned stack ptr */
   and $0xF0, %al               /* align to 32 bytes */
   mov %esp, %edx               /* save original value of SP */
   lea -FSIZE(%eax), %esp       /* SP is aligned and has room for FSIZE */
   movl %edx, 4(%esp)           /* stack has original value of SP */
   movl 4(%edx), %eax           /* get ptr to func and arg ptrs */
   movl 4(%eax), %ecx           /* get arg ptr */
   movl %ecx, (%esp)            /* put arg as parameter for called func */
   movl 0(%eax), %ecx           /* get address of func to call */
   call *%ecx                   /* call func, pushing ret @ to stack */
   mov 4(%esp), %esp            /* restore stack ptr */
   ret
@ROUT ATL_tDistMemTouch
#include "atlas_misc.h"
#include "atlas_threads.h"

typedef struct
{
   int *mem;   /* ptr to memory location */
   size_t N;   /* length of mem in sizeof(int) chunks */
} ATL_dmt_t;

/*
 * This function will make it so that a first-touch allocation policy
 * still results in the pages of mem being allocated in cyclic fashion
 * amongst the nodes.  Without calling this function, all allocated memory
 * that is serially initialized will be owned by only one processor, which
 * can cause slowdown on parallel machines.  For AMD machines with HT-assist
 * putting all pages in one local memory essentially reduces the global
 * cache size to the local cache size, with a corresponding disastrous
 * drop in performance.
 * The first page is always assigned to rank=0; if you allocate a bunch of
 * small spaces, they will all be owned by rank=0!
 */
void ATL_dmt_DOWORK(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   const int iam = tp->rank;
   ATl_dmt_t *pd = lp->opstruct;
   const size_t pgsz = 4096 / sizeof(int), stride=pgsz * ATL_NTHREADS;
   const size_t N = pd->N;
   int *mp = pd->mem, *end = pd->mem + N;

   mp += iam*pgsz;
   while (mp < end)
   {
      *mp = iam+1;
      mp += stride;
   }
}

void ATL_tDistMemTouch  /* use first-touch policy to force cyclic page distro */
(
   size_t N,   /* length of allocated memory */
   void *vp    /* allocated memory */
)
{
   ATL_dmt_t pd;
   pd.N = N / sizeof(int);
   pd.mem = vp;
   ATL_goparallel(ATL_NTHREADS, ATL_dmt_DOWORK, &pd, NULL);
}
