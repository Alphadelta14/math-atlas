@ifdef ! topd
   @define topd @/home/whaley/atlas3.9/AtlasBase@
@endifdef
@ROUT ATL_dmm4x2x128_sse2 ATL_dmm3x3x128_sse2 ATL_dmm8x1x120_L1pf @\
      ATL_dmm2x2x2_sse2 ATL_dmm2x2x128_sse2 ATL_smm2x2x256_sse
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/gen.inc @(cw08) what=cw
#include "atlas_asm.h"
@ROUT ATL_dmm2x2x2_sse2 ATL_dmm2x2x128_sse2 ATL_smm2x2x256_sse
#ifndef ATL_SSE2
   #error "This routine requires SSE2!"
#endif
@ROUT ATL_dmm4x2x128_sse2 ATL_dmm3x3x128_sse2 ATL_dmm8x1x120_L1pf
#ifndef ATL_SSE3
   #error "This routine requires SSE3!"
#endif
@ROUT ATL_dmm4x2x128_sse2
/*
 * This computational kernel was created using a code fragment demonstrating a
 * Core2Duo-friendly 2-D x86 register block sent to me by Yevgen Voronenko 
 * of the CMU/SPIRAL group as a template.  Here is original the code fragment
 * that Yevgen sent me:
 *      movapd    (%rdi), %xmm9                
 *      movapd    %xmm9, %xmm6                                  
 *      movapd    48(%rdi), %xmm8                               
 *      mulpd     %xmm8, %xmm6                                  
 *      addpd     %xmm6, %xmm5                                  
 *      movapd    16(%rdi), %xmm10                              
 *      movapd    %xmm10, %xmm7                                 
 *      mulpd     %xmm8, %xmm7                                  
 *      addpd     %xmm7, %xmm4                                  
 *      movapd    32(%rdi), %xmm12                              
 *      mulpd     %xmm12, %xmm8                                 
 *      addpd     %xmm8, %xmm3                                  
 *      movapd    64(%rdi), %xmm11                              
 *      mulpd     %xmm11, %xmm9                                 
 *      mulpd     %xmm11, %xmm10                                
 *      mulpd     %xmm11, %xmm12                                
 *      addpd     %xmm9, %xmm2                                  
 *      addpd     %xmm10, %xmm1                                 
 *      addpd     %xmm12, %xmm0                                 
 */


#if !defined(ATL_GAS_x8664)
   #error "This kernel requires x86-64 assembly!"
#endif

#if !defined(KB) || (KB == 0)
   #error "KB must be a compile-time constant!"
#endif
#if KB > 128
   #error "KB can at most be 128!"
#endif

#ifdef DCPLX
   #define CMUL(arg_) 2*arg_
#else
   #define CMUL(arg_) arg_
#endif
/*
 *Integer register usage shown by these defines
 */
#define pA0     %rcx
#define lda     %rbx
#define lda3    %rbp
#define pfA     %rdi
#define pB0     %rax
#define ldb     %rsi
#define pfB     %rdx
#define incAn   %r8
#define incCn   %r9
#define pC0     %r10
#define MM      %r11
#define NN      %r12
#define MM0     %r13
#define ldc     %r14

#define rA0 	%xmm0
#define rA1 	%xmm1
#define rA2 	%xmm2
#define rA3 	%xmm3
#define rB0 	%xmm4
#define ra0 	%xmm5
#define ra1 	%xmm6
#define ra2 	%xmm7
#define rC00 	%xmm8
#define rC10 	%xmm9
#define rC20 	%xmm10
#define rC30 	%xmm11
#define rC01 	%xmm12
#define rC11 	%xmm13
#define rC21 	%xmm14
#define rC31 	%xmm15

/*
 * Save some inst space by using short version of instructions
 */
#define movapd movaps
#define movupd movups
#define movlpd movlps
#define movhpd movhps

@beginskip
#define ATL_Kiter_b0(off_) \
   movapd off_(pA0), rC00 ; \
   movapd off_(pB0), rB0 ; \
   movapd rC00, rC01 ; \
   mulpd  rB0, rC00 ; \
   movapd off_(pA0,lda), rC10 ; \
   movapd rC10, rC11 ; \
   mulpd  rB0, rC10 ; \
   movapd off_(pA0,lda,2), rC20 ; \
   movapd rC20, rC21 ; \
   mulpd  rB0, rC20 ; \
   movapd off_(pA0,lda3), rC30 ; \
   movapd rC30, rC31 ; \
   mulpd  rB0, rC30 ; \
   movapd off_(pB0,ldb), rB0 ; \
   mulpd  rB0, rC01 ; \
   mulpd  rB0, rC11 ; \
   mulpd  rB0, rC21 ; \
   mulpd  rB0, rC31 ;

#define ATL_Kiter(off_) \
   movapd off_(pA0), rA0 ; \
   movapd off_(pB0), rB0 ; \
   movapd rA0, ra0 ; \
   mulpd  rB0, rA0 ; \
   addpd  rA0, rC00 ; \
   movapd off_(pA0,lda), rA1 ; \
   movapd rA1, ra1 ; \
   mulpd  rB0, rA1 ; \
   addpd  rA1, rC10 ; \
   movapd off_(pA0,lda,2), rA2 ; \
   movapd rA2, ra2 ; \
   mulpd  rB0, rA2 ; \
   addpd  rA2, rC20 ; \
   movapd off_(pA0,lda3), rA3 ; \
   mulpd  rA3, rB0 ; \
   addpd  rB0, rC30 ; \
   movapd off_(pB0,ldb), rB0 ; \
   mulpd  rB0, ra0 ; \
   mulpd  rB0, ra1 ; \
   mulpd  rB0, ra2 ; \
   mulpd  rB0, rA3 ; \
   addpd  ra0, rC01 ; \
   addpd  ra1, rC11 ; \
   addpd  ra2, rC21 ; \
   addpd  rA3, rC31 ; 

/*
 * The last K iteration must load C, if BETA != 0
 */
#ifdef BETA0
   #define ATL_KiterL_b0 ATL_Kiter_b0
   #define ATL_KiterL ATL_Kiter
#elif !defined(DCPLX)
   #define ATL_KiterL_b0(off_) \
      movapd off_(pA0), rC00 ; \
      movapd off_(pB0), rB0 ; \
      movapd rC00, rC01 ; \
      mulpd  rB0, rC00 ; \
      movapd off_(pA0,lda), rC10 ; \
      movapd rC10, rC11 ; \
      mulpd  rB0, rC10 ; \
      movapd off_(pA0,lda,2), rC20 ; \
      movapd rC20, rC21 ; \
      mulpd  rB0, rC20 ; \
      movapd off_(pA0,lda3), rC30 ; \
      movapd rC30, rC31 ; \
      mulpd  rB0, rC30 ; \
      movapd off_(pB0,ldb), rB0 ; \
      mulpd  rB0, rC01 ; \
      MOVCPD (pC0), rA0 ; \
      mulpd  rB0, rC11 ; \
      MOVCPD 16(pC0), rA1 ; \
      mulpd  rB0, rC21 ; \
      MOVCPD (pC0,ldc), rA2 ; \
      mulpd  rB0, rC31 ; \
      MOVCPD 16(pC0,ldc), rA3 ; 

   #define ATL_KiterL(off_) \
      movapd off_(pA0), rA0 ; \
      movapd off_(pB0), rB0 ; \
      movapd rA0, ra0 ; \
      mulpd  rB0, rA0 ; \
      addpd  rA0, rC00 ; \
      movapd off_(pA0,lda), rA1 ; \
      movapd rA1, ra1 ; \
      mulpd  rB0, rA1 ; \
      addpd  rA1, rC10 ; \
      movapd off_(pA0,lda,2), rA2 ; \
      movapd rA2, ra2 ; \
      mulpd  rB0, rA2 ; \
      addpd  rA2, rC20 ; \
      movapd off_(pA0,lda3), rA3 ; \
      mulpd  rA3, rB0 ; \
      addpd  rB0, rC30 ; \
      movapd off_(pB0,ldb), rB0 ; \
      mulpd  rB0, ra0 ; \
      addpd  ra0, rC01 ; \
      MOVCPD (pC0), rA0 ; \
      mulpd  rB0, ra1 ; \
      addpd  ra1, rC11 ; \
      MOVCPD 16(pC0), rA1 ; \
      mulpd  rB0, ra2 ; \
      addpd  ra2, rC21 ; \
      MOVCPD (pC0,ldc), rA2 ; \
      mulpd  rB0, rA3 ; \
      addpd  rA3, rC31 ;  \
      MOVCPD 16(pC0,ldc), rA3 ; 
#else
   #define ATL_KiterL_b0(off_) \
      movapd off_(pA0), rC00 ; \
      movapd off_(pB0), rB0 ; \
      movapd rC00, rC01 ; \
      mulpd  rB0, rC00 ; \
      movapd off_(pA0,lda), rC10 ; \
      movapd rC10, rC11 ; \
      mulpd  rB0, rC10 ; \
      movapd off_(pA0,lda,2), rC20 ; \
      movapd rC20, rC21 ; \
      mulpd  rB0, rC20 ; \
      movapd off_(pA0,lda3), rC30 ; \
      movapd rC30, rC31 ; \
      mulpd  rB0, rC30 ; \
      movapd off_(pB0,ldb), rB0 ; \
      mulpd  rB0, rC01 ; \
      movlpd (pC0), rA0 ; \
      movhpd 16(pC0), rA0 ; \
      mulpd  rB0, rC11 ; \
      movlpd 32(pC0), rA1 ; \
      movhpd 48(pC0), rA1 ; \
      mulpd  rB0, rC21 ; \
      movlpd (pC0,ldc), rA2 ; \
      movhpd 16(pC0,ldc), rA2 ; \
      mulpd  rB0, rC31 ; \
      movlpd 32(pC0,ldc), rA3 ; \
      movhpd 48(pC0,ldc), rA3 ; 

   #define ATL_KiterL(off_) \
      movapd off_(pA0), rA0 ; \
      movapd off_(pB0), rB0 ; \
      movapd rA0, ra0 ; \
      mulpd  rB0, rA0 ; \
      addpd  rA0, rC00 ; \
      movapd off_(pA0,lda), rA1 ; \
      movapd rA1, ra1 ; \
      mulpd  rB0, rA1 ; \
      addpd  rA1, rC10 ; \
      movapd off_(pA0,lda,2), rA2 ; \
      movapd rA2, ra2 ; \
      mulpd  rB0, rA2 ; \
      addpd  rA2, rC20 ; \
      movapd off_(pA0,lda3), rA3 ; \
      mulpd  rA3, rB0 ; \
      addpd  rB0, rC30 ; \
      movapd off_(pB0,ldb), rB0 ; \
      mulpd  rB0, ra0 ; \
      addpd  ra0, rC01 ; \
      movlpd (pC0), rA0 ; \
      movhpd 16(pC0), rA0 ; \
      mulpd  rB0, ra1 ; \
      addpd  ra1, rC11 ; \
      movlpd 32(pC0), rA1 ; \
      movhpd 48(pC0), rA1 ; \
      mulpd  rB0, ra2 ; \
      addpd  ra2, rC21 ; \
      movlpd (pC0,ldc), rA2 ; \
      movhpd 16(pC0,ldc), rA2 ; \
      mulpd  rB0, rA3 ; \
      addpd  rA3, rC31 ;  \
      movlpd 32(pC0,ldc), rA3 ; \
      movhpd 48(pC0,ldc), rA3 ; 

#endif
@endskip
/*
                      %rdi/4       %rsi/8       %rdx/12          %xmm0/16
 void ATL_USERMM(const int M, const int N, const int K, const TYPE alpha,
                       %rcx/24         %r8/28         %r9/32           8/36
                 const TYPE *A, const int lda, const TYPE *B, const int ldb,
                        %xmm1/40    16/48          24/52
                 const TYPE beta, TYPE *C, const int ldc)
*/
#define MOVCPD movapd
        .text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 *      Save callee-saved iregs
 */
        movq    %rbp, -8(%rsp)
        movq    %rbx, -16(%rsp)
        movq    %r12, -24(%rsp)
        movq    %r13, -32(%rsp)
        movq    %r14, -40(%rsp)
                                        prefetcht0 (pA0)
/*        movq    %r15, -48(%rsp) */

/*
 *      Setup input parameters
 */
   #ifdef BETAX
      #define BETAOFF -56
        unpcklpd        %xmm1, %xmm1
        movapd  %xmm1, BETAOFF(%rsp)
   #endif
        movq    %rdi, MM0
        movq    %rsi, NN
        movq    %r8, lda
                                        prefetcht0      (pA0,lda)
        movq    %r9, pB0
                                        prefetcht0      (pB0)
        movslq  8(%rsp), ldb
                                        prefetcht0      (pA0,lda,2)
        movq    16(%rsp), pC0
        movslq  24(%rsp), incCn
	movq	incCn, ldc
                                        prefetcht0      KB*8(pA0,lda,2)
/*
 *      incCn = (2*ldc-M)*sizeof
 */
	shl	$1, incCn
        sub     MM0, incCn
#ifdef DCPLX
        shl     $4, incCn
        shl	$4, ldc
#else
        shl     $3, incCn
        shl	$3, ldc
#endif
/*
 *      pA0 += 128; pB0 += 128
 */
        sub     $-128, pA0
        sub     $-128, pB0
                                        prefetcht0      -64(pB0)
/*
 *      lda = lda*sizeof;  lda3 = lda*3
 */
        shl     $3, lda
                                                prefetcht0      (pB0)
        lea     (lda,lda,2), lda3
/*
 *      ldb = ldb*sizeof
 */
        shl     $3, ldb
                                                prefetcht0      64(pB0)
/*
 *      pfA = A + lda*M ; incAn = lda*M
 */
        movq    lda, pfA
                                                prefetcht0      128(pB0)
        imulq   MM0, pfA
        movq    pfA, incAn
        lea     -128(pA0, pfA), pfA
        movq    MM0, MM
        lea     -128(pB0,ldb,2), pfB
#ifndef DCPLX
        test    $15, ldc
        jnz UMNLOOP
        test    $15, pC0
        jnz     UMNLOOP
#endif
@ROUT ATL_dmm4x2x128_sse2 ATL_dmm4x2x128_sse2_UNAL
@ifdef ! MNLOOP
   @define MNLOOP @MNLOOP@
@endifdef
ALIGN16
@(MNLOOP):
/* MLOOP: */
/* KLOOP begin */
#if KB == 2
        movapd -128(pA0), rC00 
        movapd -128(pB0), rB0 
        movapd rC00, rC01 
        mulpd  rB0, rC00 
        movapd -128(pA0,lda), rC10 
        movapd rC10, rC11 
        mulpd  rB0, rC10 
        movapd -128(pA0,lda,2), rC20 
        movapd rC20, rC21 
        mulpd  rB0, rC20 
        movapd -128(pA0,lda3), rC30 
        movapd rC30, rC31 
        mulpd  rB0, rC30 
        movapd -128(pB0,ldb), rB0 
        mulpd  rB0, rC01 
        #ifndef BETA0
           #ifdef DCPLX
              movsd  (pC0), rA0 
              movhpd 16(pC0), rA0 
           #else
              MOVCPD (pC0), rA0 
           #endif
        #endif
        mulpd  rB0, rC11 
        #ifndef BETA0
           #ifdef DCPLX
              movsd  32(pC0), rA1 
              movhpd 48(pC0), rA1 
           #else
              MOVCPD 16(pC0), rA1 
           #endif
        #endif
        mulpd  rB0, rC21 
        #ifndef BETA0
           #ifdef DCPLX
              movsd  (pC0,ldc), rA2 
              movhpd 16(pC0,ldc), rA2 
           #else
              MOVCPD (pC0,ldc), rA2 
           #endif
        #endif
        mulpd  rB0, rC31 
        #ifndef BETA0
           #ifdef DCPLX
              movsd  32(pC0,ldc), rA3 
              movhpd 48(pC0,ldc), rA3
           #else
              MOVCPD 16(pC0,ldc), rA3
           #endif
        #endif
#else
        movapd -128(pA0), rC00 ; \
        movapd -128(pB0), rB0 ; \
        movapd rC00, rC01 ; \
        mulpd  rB0, rC00 ; \
        movapd -128(pA0,lda), rC10 ; \
        movapd rC10, rC11 ; \
        mulpd  rB0, rC10 ; \
        movapd -128(pA0,lda,2), rC20 ; \
        movapd rC20, rC21 ; \
        mulpd  rB0, rC20 ; \
        movapd -128(pA0,lda3), rC30 ; \
        movapd rC30, rC31 ; \
        mulpd  rB0, rC30 ; \
        movapd -128(pB0,ldb), ra0 ; \
        mulpd  ra0, rC01 ; \
        mulpd  ra0, rC11 ; \
           movapd 16-128(pB0), rB0 ; \
        mulpd  ra0, rC21 ; \
        mulpd  ra0, rC31 ;
#endif
        prefetcht0      (pfB)
        add     $64, pfB
@define i @4@
@define j @-112@
@iwhile i < 128
   #if KB > @(i)
      movapd @(j)(pA0), rA0 
      movapd rA0, ra0 
      mulpd  rB0, rA0 
      addpd  rA0, rC00 
      movapd @(j)(pA0,lda), rA1 
      movapd rA1, ra1 
      mulpd  rB0, rA1 
      addpd  rA1, rC10 
      movapd @(j)(pA0,lda,2), rA2 
      movapd rA2, ra2 
      mulpd  rB0, rA2 
      addpd  rA2, rC20 
      movapd @(j)(pA0,lda3), rA3 
      mulpd  rA3, rB0 
      addpd  rB0, rC30 
      movapd @(j)(pB0,ldb), rB0 
      mulpd  rB0, ra0 
      mulpd  rB0, ra1 
      mulpd  rB0, ra2 
      mulpd  rB0, rA3 
         movapd 16+@(j)(pB0), rB0 
      addpd  ra0, rC01 
      addpd  ra1, rC11 
      addpd  ra2, rC21 
      addpd  rA3, rC31 
   #elif KB == @(i)
      movapd @(j)(pA0), rA0 
      movapd @(j)(pB0), rB0 
      movapd rA0, ra0 
      mulpd  rB0, rA0 
      addpd  rA0, rC00 
      movapd @(j)(pA0,lda), rA1 
      movapd rA1, ra1 
      mulpd  rB0, rA1 
      addpd  rA1, rC10 
      movapd @(j)(pA0,lda,2), rA2 
      movapd rA2, ra2 
      mulpd  rB0, rA2 
      addpd  rA2, rC20 
      movapd @(j)(pA0,lda3), rA3 
      mulpd  rA3, rB0 
      addpd  rB0, rC30 
      movapd @(j)(pB0,ldb), rB0 
      mulpd  rB0, ra0 
      addpd  ra0, rC01 
      #ifndef BETA0
         #ifdef DCPLX
            movsd  (pC0), rA0
            movhpd 16(pC0), rA0
         #else
            MOVCPD (pC0), rA0 
         #endif
      #endif
      mulpd  rB0, ra1 
      addpd  ra1, rC11 
      #ifndef BETA0
         #ifdef DCPLX
            movsd  32(pC0), rA1
            movhpd 48(pC0), rA1
         #else
            MOVCPD 16(pC0), rA1 
         #endif
      #endif
      mulpd  rB0, ra2 
      addpd  ra2, rC21 
      #ifndef BETA0
         #ifdef DCPLX
            movsd  (pC0,ldc), rA2
            movhpd 16(pC0,ldc), rA2
         #else
            MOVCPD (pC0,ldc), rA2 
         #endif
      #endif
      mulpd  rB0, rA3 
      addpd  rA3, rC31 
      #ifndef BETA0
         #ifdef DCPLX
            movsd  32(pC0,ldc), rA3
            movhpd 48(pC0,ldc), rA3
         #else
            MOVCPD 16(pC0,ldc), rA3
         #endif
      #endif
   #endif
   @iexp i 2 @(i) +
   @iexp j 16 @(j) +
@endiwhile
@undef i
@undef j
/* KLOOP end */
        haddpd	rC10, rC00
#ifdef BETAX
        movapd  BETAOFF(%rsp), rB0
        mulpd   rB0, rA0
#endif
#ifndef BETA0
        addpd   rA0, rC00
#endif
#ifdef DCPLX
	movlpd	rC00, (pC0)
	movhpd	rC00, 16(pC0)
#else
	MOVCPD	rC00, (pC0)
#endif
        haddpd	rC30, rC20
#ifdef BETAX
        mulpd   rB0, rA1
#endif
#ifndef BETA0
        addpd   rA1, rC20
#endif
#ifdef DCPLX
	movlpd	rC20, 32(pC0)
	movhpd	rC20, 48(pC0)
#else
	MOVCPD	rC20, 16(pC0)
#endif
        haddpd	rC11, rC01
#ifdef BETAX
        mulpd   rB0, rA2
#endif
#ifndef BETA0
        addpd   rA2, rC01
#endif
#ifdef DCPLX
	movlpd	rC01, (pC0,ldc)
	movhpd	rC01, 16(pC0,ldc)
#else
	MOVCPD	rC01, (pC0,ldc)
#endif
        haddpd	rC31, rC21
#ifdef BETAX
        mulpd   rB0, rA3
#endif
#ifndef BETA0
        addpd   rA3, rC21
#endif
#ifdef DCPLX
	movlpd	rC21, 32(pC0,ldc)
	movhpd	rC21, 48(pC0,ldc)
#else
	MOVCPD	rC21, 16(pC0,ldc)
#endif

        add     $4*CMUL(8), pC0
          prefetcht1      (pfA)
          add     $64, pfA

        lea     0(pA0,lda,4), pA0
        sub     $4, MM
        jnz     @(MNLOOP)

        movq    MM0, MM
        sub     incAn, pA0
        add     incCn, pC0
	lea	(pB0, ldb, 2), pB0
        sub     $2, NN
        jnz     @(MNLOOP)

/* DONE: */
        movq    -8(%rsp), %rbp
        movq    -16(%rsp), %rbx
        movq    -24(%rsp), %r12
        movq    -32(%rsp), %r13
        movq    -40(%rsp), %r14
/*        movq    -48(%rsp), %r15  */
        ret
@ROUT ATL_dmm4x2x128_sse2 
/*
 * Same code as above, but assuming C is not aligned
 */
#ifndef DCPLX
   #undef  MOVCPD
   #define MOVCPD movupd
   @extract -b @(topd)/kernel/ClintWhaley/clint_sse.base rout=ATL_dmm4x2x128_sse2_UNAL -def MNLOOP "UMNLOOP"
#endif   /* end of repeated loops for unaligned C for real precision */
@ROUT ATL_dmm3x3x128_sse2
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
#if !defined(MB)
   #define MB 0
#endif
#if !defined(NB)
   #define NB 0
#endif
#if !defined(KB)
   #define KB 0
#endif
#if KB == 0
   #error "KB must be compile time constant!"
#endif
/*
 *Register usage
 */
#define pA0     %rcx
#define lda     %rbx
#define pfA     %rdi
#define pB0     %rax
#define ldb     %rsi
#define pfB     %rdx
#define incAn   %r8
#define incCn   %r9
#define pC0     %rbp
#define MM      %r11
#define NN      %r12
#define MM0     %r13
#define ldc     %r14
#define lda3    %r15

#define rA0     %xmm0
#define rA1     %xmm1
#define rA2     %xmm2
#define rB0     %xmm3
#define ra0     %xmm4
#define ra1     %xmm5
#define ra2     %xmm6
#define rC00    %xmm7
#define rC10    %xmm8
#define rC20    %xmm9
#define rC01    %xmm10
#define rC11    %xmm11
#define rC21    %xmm12
#define rC02    %xmm13
#define rC12    %xmm14
#define rC22    %xmm15

/*
 * Save some inst space by using short version of instructions
 */
#define movapd movaps
#define movupd movups
#define movlpd movlps
#define movhpd movhps
#ifdef DCPLX
   #define CMUL(arg_) 2*arg_
#else
   #define CMUL(arg_) arg_
#endif

/*
                      %rdi/4       %rsi/8       %rdx/12          %xmm0/16
 void ATL_USERMM(const int M, const int N, const int K, const TYPE alpha,
                       %rcx/24         %r8/28         %r9/32           8/36
                 const TYPE *A, const int lda, const TYPE *B, const int ldb,
                        %xmm1/40    16/48          24/52
                 const TYPE beta, TYPE *C, const int ldc)
*/
        .text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 *      Save callee-saved iregs
 */
        movq    %rbp, -8(%rsp)
        movq    %rbx, -16(%rsp)
        movq    %r12, -24(%rsp)
        movq    %r13, -32(%rsp)
        movq    %r14, -40(%rsp)
        movq    %r15, -48(%rsp)
                                        prefetcht0 (pA0)

/*
 *      Setup input parameters
 */
   #ifdef BETAX
      #define BETAOFF -72
        unpcklpd        %xmm1, %xmm1
        movapd  %xmm1, BETAOFF(%rsp)
   #endif
        movq    %rdi, MM0
        movq    %rsi, NN
        movq    %r8, lda
                                        prefetcht0      (pA0,lda)
        movq    %r9, pB0
                                        prefetcht0      (pB0)
        movslq  8(%rsp), ldb
                                        prefetcht0      (pA0,lda,2)
        movq    16(%rsp), pC0
        movslq  24(%rsp), incCn
	movq	incCn, ldc
                                        prefetcht0      KB*8(pA0,lda,2)
/*
 *      incCn = (3*ldc-M)*sizeof
 */
        lea     (incCn, incCn,2), incCn
        sub     MM0, incCn
#ifdef DCPLX
        shl     $4, incCn
        shl	$4, ldc
#else
        shl     $3, incCn
        shl	$3, ldc
#endif
/*
 *      pA0 += 128; pB0 += 128
 */
        sub     $-128, pA0
        sub     $-128, pB0
                                        prefetcht0      -64(pB0)
/*
 *      lda = lda*sizeof;  lda3 = lda*3
 */
        shl     $3, lda
                                                prefetcht0      (pB0)
        lea     (lda, lda,2), lda3
/*
 *      ldb = ldb*sizeof
 */
        shl     $3, ldb
                                                prefetcht0      64(pB0)
/*
 *      pfA = A + lda*M ; incAn = lda*M
 */
        movq    lda, pfA
                                                prefetcht0      128(pB0)
        imulq   MM0, pfA
        movq    pfA, incAn
        lea     -128(pA0, pfA), pfA
        movq    MM0, MM
        lea     -128(pB0,ldb,2), pfB
ALIGN16
MNLOOP:
/* MLOOP: */
/* KLOOP begin */
   #if KB == 2   /* last iteration loads C in rA0-rA3 */
      movapd -128(pA0), rC00
      movapd -128(pB0), rB0 
      movapd rC00, rC01
      mulpd  rB0, rC00
      movapd -128(pA0, lda), rC10
      movapd rC10, rC11
      mulpd  rB0, rC10
      movapd -128(pA0, lda,2), rC20
      movapd rC20, rC21
      mulpd  rB0, rC20
      movapd -128(pB0, ldb), rB0
      movapd rC01, rC02
      mulpd  rB0, rC01
      movapd rC11, rC12
      #ifndef BETA0
         #ifdef DCPLX                              /* ra0 = C10 C00 */
            movsd  (pC0), ra0
            movhpd 16(pC0), ra0
         #else
            movupd (pC0), ra0
         #endif
      #endif
      mulpd  rB0, rC11
      movapd rC21, rC22
      #ifndef BETA0
         movsd  CMUL(16)(pC0), ra1              
         movhpd CMUL(16)(pC0,ldc), ra1          /* ra1 = C21, C20 */
      #endif
      mulpd  rB0, rC21
      movapd -128(pB0, ldb,2), rB0
      #ifndef BETA0
         #ifdef DCPLX                              /* ra2 = C11, C01 */
            movsd  (pC0,ldc), ra2
            movhpd 16(pC0,ldc), ra2
         #else
            movupd (pC0,ldc), ra2
         #endif
      #endif

      mulpd  rB0, rC02
      #ifndef BETA0
         #ifdef DCPLX                              /* rA0 = C12, C02 */
            movsd  (pC0,ldc,2), rA0
            movhpd 16(pC0,ldc,2), rA0
         #else
            movupd (pC0,ldc,2), rA0
         #endif
      #endif
      mulpd  rB0, rC12
      #ifndef BETA0
         movsd  CMUL(16)(pC0,ldc,2), rA1        /* rA1 = 0, C22 */
      #endif
      mulpd  rB0, rC22
   #else
      movapd -128(pA0), rC00
      movapd -128(pB0), rB0 
      movapd rC00, rC01
      mulpd  rB0, rC00
      movapd -128(pA0, lda), rC10
      movapd rC10, rC11
      mulpd  rB0, rC10
      movapd -128(pA0, lda,2), rC20
      movapd rC20, rC21
      mulpd  rB0, rC20
      movapd -128(pB0, ldb), rB0
      movapd rC01, rC02
      mulpd  rB0, rC01
      movapd rC11, rC12
      mulpd  rB0, rC11
      movapd rC21, rC22
      mulpd  rB0, rC21
      movapd -128(pB0, ldb,2), ra0
      mulpd  ra0, rC02
         movapd 16-128(pB0), rB0
      mulpd  ra0, rC12
      mulpd  ra0, rC22
#endif
@define i @4@
@define j @-112@
@iwhile i < 128
   #if KB == @(i)   /* last iteration loads C in A/a regs */
/*
 * Last K iteration loads ra[0-2] & rA[0,1] such that:
 *                               ra0  = C10  C00
 *                               ra1  = C21  C20
 *                               ra2  = C11  C01
 *                               rA0  = C12, C02
 *                               rA1  = XXX, C22
 */
      movapd @(j)(pA0), rA0 
      movapd rA0, ra0
      mulpd  rB0, rA0
      addpd  rA0, rC00
      movapd @(j)(pA0, lda), rA1
      movapd rA1, ra1
      mulpd  rB0, rA1
      addpd  rA1, rC10
      movapd @(j)(pA0, lda,2), rA2
      mulpd  rA2, rB0
      addpd  rB0, rC20
      movapd @(j)(pB0, ldb), rB0
      movapd ra0, rA0
      mulpd  rB0, ra0
      addpd  ra0, rC01
      movapd ra1, rA1
      mulpd  rB0, ra1
      addpd  ra1, rC11
      mulpd  rA2, rB0
      addpd  rB0, rC21
      movapd @(j)(pB0, ldb,2), rB0
      mulpd rB0, rA0
      mulpd rB0, rA1
      mulpd rB0, rA2
      addpd rA0, rC02
      addpd rA1, rC12
      addpd rA2, rC22
   #elif KB > @(i)
      movapd @(j)(pA0), rA0 
      movapd @(j)(pB0), rB0 
      movapd rA0, ra0
      mulpd  rB0, rA0
      addpd  rA0, rC00
      movapd @(j)(pA0, lda), rA1
      movapd rA1, ra1
      mulpd  rB0, rA1
      addpd  rA1, rC10
      movapd @(j)(pA0, lda,2), rA2
      mulpd  rA2, rB0
      addpd  rB0, rC20
      movapd @(j)(pB0, ldb), rB0
      movapd ra0, rA0
      mulpd  rB0, ra0
      addpd  ra0, rC01
      movapd ra1, rA1
      mulpd  rB0, ra1
      addpd  ra1, rC11
      mulpd  rA2, rB0
      addpd  rB0, rC21
      movapd @(j)(pB0, ldb,2), rB0
      mulpd rB0, rA0
      mulpd rB0, rA1
      mulpd rB0, rA2
      addpd rA0, rC02
         movapd @(j)+16(pB0), rB0 
      addpd rA1, rC12
      addpd rA2, rC22
#endif
   @iexp i 2 @(i) +
   @iexp j 16 @(j) +
@endiwhile
@undef i
@undef j
/* KLOOP end */
/*
 * Last K iteration loads ra[0-2] & rA[0,1] such that:
 *                               ra0  = C10  C00
 *                               ra1  = C21  C20
 *                               ra2  = C11  C01
 *                               rA0  = C12, C02
 *                               rA1  =   0, C22
 */
        haddpd  rC10, rC00    /* rC00 = C10, C00 */
        #ifndef BETA0
           addpd   ra0, rC00
        #endif
        #ifdef DCPLX
           movlpd  rC00, (pC0)
           movhpd  rC00, 16(pC0)
        #else
           movupd  rC00, (pC0)
        #endif
        haddpd  rC21, rC20    /* rC20 = C21, C20 */
        #ifndef BETA0
           addpd   ra1, rC20
        #endif
           movlpd  rC20, CMUL(16)(pC0)
           movhpd  rC20, CMUL(16)(pC0,ldc)
        haddpd  rC11, rC01    /* rC01 = C11, C01 */
        #ifndef BETA0
           addpd   ra2, rC01
        #endif
        #ifdef DCPLX
           movlpd  rC01, (pC0,ldc)
           movhpd  rC01, 16(pC0,ldc)
        #else
           movupd  rC01, (pC0,ldc)
        #endif
        haddpd  rC12, rC02    /* rC02 = C12, C02 */
        #ifndef BETA0
           addpd   rA0, rC02
        #endif
        #ifdef DCPLX
           movlpd  rC02, (pC0,ldc,2)
           movhpd  rC02, 16(pC0,ldc,2)
        #else
           movupd  rC02, (pC0,ldc,2)
        #endif
        haddpd  rC22, rC22    /* rC22 = C22, C22 */
        #ifndef BETA0
           addpd   rA1, rC22 
        #endif
           movlpd  rC22, CMUL(16)(pC0,ldc,2)
/*
 *      pC0 += 3*sizeof, pA0 += 3*lda 
 */
        add     $3*CMUL(8), pC0
        prefetcht1      (pfA)
        add     $64, pfA
        lea     (pA0, lda3), pA0
        sub     $3, MM
        jnz     MNLOOP

        mov     MM0, MM
        sub     incAn, pA0
        add     incCn, pC0
        lea     (pB0, ldb, 2), pB0
        add     ldb, pB0
        sub     $3, NN
        jnz     MNLOOP
/* DONE */
        movq    -8(%rsp), %rbp
        movq    -16(%rsp), %rbx
        movq    -24(%rsp), %r12
        movq    -32(%rsp), %r13
        movq    -40(%rsp), %r14
        movq    -48(%rsp), %r15
        ret

@ROUT ATL_dmm8x1x120_L1pf
/*
 * This routine hastily adapted to the 3rd gen Opteron from the Core2 kernel,
 * ATL_dmm8x1x120.c.  Main difference is the prefetch strategy, which fetches
 * to the L1 on the Opt3rdGen, and the handling of loads, where MOVUPD
 * is preferred over MOVLPD/MOVHPD pair.  We also turn on unaligned mem 
 * MULPD/ADDPD.  This all makes the kernal about 8% faster, and allows us
 * to use a smaller block factor (for better application performance).
 * This kernel does not do well for complex.
 */
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
#if !defined(MB)
   #define MB 0
#endif
#if !defined(NB)
   #define NB 0
#endif
#if !defined(KB)
   #define KB 0
#endif
#if KB == 0
   #error "KB must be compile time constant!"
#endif
/*
 *Register usage
 */
#define pA0     %rcx
#define lda     %rbx
#define lda3    %rbp
#define lda5    %rdx
#define lda7    %rdi
#define pB0     %rax
#define pC0     %rsi
#define ldb     %r8
#define ldc     %r9
#define pfA     %r10
#define MM      %r11
#define NN      %r12
#define incAn   %r13
#define incCn   %r14
#define pfB     %r15

#define rA0     %xmm0
#define rB0     %xmm1
#define rC00    %xmm2
#define rC01    %xmm3
#define rC02    %xmm4
#define rC03    %xmm5
#define rC04    %xmm6
#define rC05    %xmm7
#define rC06    %xmm8
#define rC07    %xmm9
#define rC0     %xmm10
#define rC2     %xmm11
#define rC4     %xmm12
#define rC6     %xmm13
#define rb0     %xmm14
#define BETA    %xmm15

/*
 * Prefetch defines
 */
#if 1
   #define pref2(mem) prefetcht0        mem
   #define prefB(mem) prefetcht0        mem
   #define prefC(mem) prefetchw         mem
#else
   #define pref2(mem)
   #define prefB(mem)
   #define prefC(mem)
#endif

#define PFAINC -64
#define PFBINC -64

#ifdef DCPLX
   #define CMUL(arg_) 2*arg_
#else
   #define CMUL(arg_) arg_
#endif

#define ATL_Kiter(off_) \
	movapd	off_(pB0), rB0 ; \
	movapd	off_(pA0), rA0 ; \
	mulpd	rB0,rA0 ; \
	addpd	rA0,rC00 ; \
	movapd	off_(pA0,lda), rA0 ; \
	mulpd	rB0,rA0 ; \
	addpd	rA0,rC01 ; \
	movapd	off_(pA0,lda,2), rA0 ; \
	mulpd	rB0,rA0 ; \
	addpd	rA0,rC02 ; \
	movapd	off_(pA0,lda3), rA0 ; \
	mulpd	rB0,rA0 ; \
	addpd	rA0,rC03 ; \
	movapd	off_(pA0,lda,4), rA0 ; \
	mulpd	rB0,rA0 ; \
	addpd	rA0,rC04 ; \
	movapd	off_(pA0,lda5), rA0 ; \
	mulpd	rB0,rA0 ; \
	addpd	rA0,rC05 ; \
	movapd	off_(pA0,lda3,2), rA0 ; \
	mulpd	rB0,rA0 ; \
	addpd	rA0,rC06 ; \
	mulpd	off_(pA0,lda7), rB0 ; \
	addpd	rB0,rC07 ; \

/*
                      %rdi/4       %rsi/8       %rdx/12          %xmm0/16
 void ATL_USERMM(const int M, const int N, const int K, const TYPE alpha,
                       %rcx/24         %r8/28         %r9/32           8/36
                 const TYPE *A, const int lda, const TYPE *B, const int ldb,
                        %xmm1/40    16/48          24/52
                 const TYPE beta, TYPE *C, const int ldc)
*/
        .text
.global ATL_asmdecor(ATL_USERMM)
ALIGN64
ATL_asmdecor(ATL_USERMM):
/* 
 *      Enable use of unaligned memory addpd/mulpd instructions
 */
        stmxcsr -4(%rsp)
        orl     $131072, -4(%rsp)
        ldmxcsr -4(%rsp)

/*
 *      Save callee-saved iregs
 */
        movq    %rbp, -8(%rsp)
        movq    %rbx, -16(%rsp)
        movq    %r12, -24(%rsp)
        movq    %r13, -32(%rsp)
        movq    %r14, -40(%rsp)
        movq    %r15, -48(%rsp)
        #define MMOFF -56
#ifdef BETAX
        pshufd  $0x44, %xmm1, BETA
/*        pshufd  $0b01000100, %xmm1, BETA */
#endif
/*
 *      Setup input parameters
 */
        movq    %rdi, MM
        movq    %rsi, NN
        movq    %r9, pB0
        movq    %r8, lda
        movslq  8(%rsp), ldb
        movq    16(%rsp), pC0
        movslq  24(%rsp), incCn
/*
 *      ldx *= sizeof; lda3 = 3*lda, lda5=5*lda, lda7=7*lda
 */
#ifndef DCPLX
        movq    incCn, ldc
#endif
        shl     $3, lda
        shl     $3, ldb
        lea     (lda,lda,2), lda3
        lea     (lda,lda,4), lda5
        lea     (lda3,lda,4), lda7
/*
 *      pA3 += 128, pB0 += 128
 */
        sub     $-128, pA0
        sub     $-128, pB0
/*
 *      incAn = lda*M*sizeof, pfB = ldb*N*sizeof
 */
        movq    lda, incAn
        imulq   MM, incAn
        lea     (pA0,incAn,2), pfA
        movq    ldb, pfB
        imulq   NN, pfB
        lea     (pB0,pfB,2), pfB
/*
 *      incCn = (ldc - M)*sizeof
 */
        sub     MM, incCn
        movq    MM, MMOFF(%rsp)
#ifdef DCPLX
        shl     $4, incCn
#else
        shl     $3, incCn
#endif
ALIGN32
UNLOOP:
UMLOOP:
/*
 *      For complex, get C loaded to rCx regs; for real, put BETA in here, and
 *      we will use unaligned mulpd at end of loop, or unaligned addpd for BETA=1
 */
        #ifndef BETA0
           #ifdef DCPLX
              movsd    (pC0), rC0
              movsd    CMUL(16)(pC0), rC2
              movsd    CMUL(32)(pC0), rC4
              movsd    CMUL(48)(pC0), rC6
              movhpd    CMUL(8)(pC0), rC0
              movhpd    CMUL(24)(pC0), rC2
              movhpd    CMUL(40)(pC0), rC4
              movhpd    CMUL(56)(pC0), rC6
           #elif defined(BETAX)
              movapd    BETA, rC0
              movapd    BETA, rC2
              movapd    BETA, rC4
              movapd    BETA, rC6
           #endif
        #endif
/*KLOOP: */
	movapd	-128(pB0), rB0
                                        pref2((pfA))
                                        add     $PFAINC, pfA
	movapd	-128(pA0), rC00
	mulpd	rB0,rC00
	movapd	-128(pA0,lda), rC01
	mulpd	rB0,rC01
	movapd	-128(pA0,lda,2), rC02
	mulpd	rB0,rC02
	movapd	-128(pA0,lda3), rC03
	mulpd	rB0,rC03
	movapd	-128(pA0,lda,4), rC04
	mulpd	rB0,rC04
	movapd	-128(pA0,lda5), rC05
	mulpd	rB0,rC05
	movapd	-128(pA0,lda3,2), rC06
	mulpd	rB0,rC06
	movapd	-128(pA0,lda7), rC07
	mulpd	rB0,rC07

@define i @2@
@define j @-112@
@iwhile i < 128
   #if KB > @(i)
	ATL_Kiter(@(j))
   #endif
   @iexp i 2 @(i) +
   @iexp j 16 @(j) +
@endiwhile
@undef i
@undef j
/* KLOOP end */

/*       jne KLOOP */
/*
 *      pC[0-8] = rC[0-8]
 */

        #ifdef BETAX
           #ifdef DCPLX
              mulpd BETA, rC0
           #else
              mulpd (pC0), rC0
           #endif
       #endif
        haddpd  rC01,rC00
                                prefB((pfB))
                                add     $PFBINC, pfB
                                lea     (pA0,lda,8), pA0
        #if defined(BETA1) && !defined(DCPLX)
           addpd   (pC0), rC00
        #elif !defined(BETA0)
           addpd   rC0, rC00
        #endif
        movlpd  rC00, (pC0)
        movhpd  rC00, CMUL(8)(pC0)

        #ifdef BETAX
           #ifdef DCPLX
              mulpd BETA, rC2
           #else
              mulpd 16(pC0), rC2
           #endif
       #endif
        haddpd  rC03,rC02
        #if defined(BETA1) && !defined(DCPLX)
           addpd   16(pC0), rC02
        #elif !defined(BETA0)
           addpd   rC2, rC02
        #endif
        movlpd  rC02, CMUL(16)(pC0)
        movhpd  rC02, CMUL(24)(pC0)

        #ifdef BETAX
           #ifdef DCPLX
              mulpd BETA, rC4
           #else
              mulpd 32(pC0), rC4
           #endif
       #endif
        haddpd  rC05,rC04
        #if defined(BETA1) && !defined(DCPLX)
           addpd   32(pC0), rC04
        #elif !defined(BETA0)
           addpd   rC4, rC04
        #endif
        movlpd  rC04, CMUL(32)(pC0)
        movhpd  rC04, CMUL(40)(pC0)

        #ifdef BETAX
           #ifdef DCPLX
              mulpd BETA, rC6
           #else
              mulpd 48(pC0), rC6
           #endif
       #endif
        haddpd  rC07,rC06
        #if defined(BETA1) && !defined(DCPLX)
           addpd   48(pC0), rC06
        #elif !defined(BETA0)
           addpd   rC6, rC06
        #endif
        movlpd  rC06, CMUL(48)(pC0)
        movhpd  rC06, CMUL(56)(pC0)

        add     $8*CMUL(8), pC0
        sub     $8, MM
        jnz     UMLOOP

        movq    MMOFF(%rsp), MM
        sub     incAn, pA0
        add     incCn, pC0
        add     ldb, pB0
        sub     $1, NN

        jnz     UNLOOP

#UDONE:
        movq    -8(%rsp), %rbp
        movq    -16(%rsp), %rbx
        movq    -24(%rsp), %r12
        movq    -32(%rsp), %r13
        movq    -40(%rsp), %r14
        movq    -48(%rsp), %r15
        ret
@ROUT ATL_dmm2x2x2_sse2
#include "atlas_asm.h"
#if !defined(ATL_GAS_x8664) && !defined(ATL_GAS_x8632)
   #error "This kernel requires x86 assembly!"
#endif

#ifdef DCPLX
   #define CMUL(arg_) 2*arg_
   #define CSH 4
#else
   #define CMUL(arg_) arg_
   #define CSH 3
#endif
#ifdef ATL_GAS_x8632
   #define movq movl
   #define addq addl
   #define subq subl
   #define shrq shrl
   #define testq testl
   #define rsp  esp
   #ifdef BETAX
      #define BETAOFF 0
      #define BETASZ 16
   #else
      #define BETASZ 0
   #endif
   #define FSIZE 16*4+BETASZ
   #define KOFF    FSIZE-4
   #define ldcOFF  KOFF-4
   #define iAmOFF  ldcOFF-4
   #define PFAOFF  iAmOFF-4
   #define PFBOFF  PFAOFF-4
   #define IOFF    PFBOFF-4
   #define MOFF    IOFF-4
   #define iAnOFF  MOFF-4
   #define iBnOFF  iAnOFF-4
   #define iCnOFF  iBnOFF-4
   #define JOFF    iCnOFF-4
#endif
/*
 *Integer register usage shown by these defines
 */
#ifdef ATL_GAS_x8632
   #define pA0     %ecx
   #define pA1     %eax
   #define pB0     %ebx
   #define pB1     %edi
   #define pC0     %esi
   #define itmp    %edx
   #define KK      %ebp
   
   #define incAm   iAmOFF(%esp)
   #define incAn   iAnOFF(%esp)
   #define incBn   iBnOFF(%esp)
   #define incCn   iCnOFF(%esp)
   #define MM      IOFF(%esp)
   #define NN      JOFF(%esp)
   #define MM0     MOFF(%esp)
   #define PFA     PFAOFF(%esp)
   #define PFB     PFBOFF(%esp)
   #define ldc     ldcOFF(%esp)
   #define KK0     KOFF(%esp)
#else
   #define pA0     %rcx
   #define pA1     %rbx
   #define pB0     %rbp
   #define pB1     %rax
   #define pC0     %rdi
   #define KK      %rdx
   #define PFB     %rsi
   #define PFA     %r8
   #define ldc     %r9
   #define KK0     %r10
   #define incAm   %r11
   #define incAn   %r12
   #define incBn   %r13
   #define MM      %r14
   #define NN      %r15
   #define incCn   iCOFF(%rsp)
   #define MM0     iIOFF(%rsp)
   #define itmp    ldc
#endif
#define rA0     %xmm0
#define rA1     %xmm1
#define ra0     %xmm2
#define rB0     %xmm3
#define rC00    %xmm4
#define rC10    %xmm5
#define rC01    %xmm6
#define rC11    %xmm7
#ifdef ATL_GAS_x8664
   #define BETA %xmm8
   #define rC0  %xmm9
   #define rC1 %xmm10
#endif
/*
 * Define some macros for instruction selection
 *    VZERO: xorpd, xorps, pxor
 *    MOVAB: movapd,movaps or movupd/movups
 */
#define VZERO(reg_) xorps reg_, reg_
#define MOVAB  movaps
#define MOVAPD movaps
#define MOVUPD movups
#define PFAINC -64
#define PFBINC 32
#if 1
   #define pref2(mem) prefetcht1        mem
   #define prefB(mem) prefetcht0        mem
   #define prefC(mem) prefetcht0        mem
#else
   #define pref2(mem)
   #define prefB(mem)
   #define prefC(mem)
#endif
/*
                      %rdi/4       %rsi/8       %rdx/12          %xmm0/16
 void ATL_USERMM(const int M, const int N, const int K, const TYPE alpha,
                       %rcx/24         %r8/28         %r9/32           8/36
                 const TYPE *A, const int lda, const TYPE *B, const int ldb,
                        %xmm1/40    16/48          24/52
                 const TYPE beta, TYPE *C, const int ldc)
*/
        .text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 *      Save callee-saved iregs
 */
#ifdef ATL_GAS_x8632
        movl    %esp, %eax              /* save original stack ptr */
        sub     $FSIZE, %esp            /* allocate stack space */
        andw    $0xFFF0, %sp            /* SP now 16-byte aligned */
        movl    %ebp, BETASZ(%esp)
        movl    %ebx, BETASZ+4(%esp)
        movl    %esi, BETASZ+8(%esp)
        movl    %edi, BETASZ+12(%esp)
        movl    %eax, BETASZ+16(%esp)   /* original SP saved to new stack */
#else
        movq    %rbp, -8(%rsp)
        movq    %rbx, -16(%rsp)
        movq    %r12, -24(%rsp)
        movq    %r13, -32(%rsp)
        movq    %r14, -40(%rsp)
        movq    %r15, -48(%rsp)
   #define iCOFF -56
   #define iIOFF -64
#endif
        
/*
 *      Setup input parameters
 *      For x8632 %eax has old stack ptr; eax is pA1, so set this up late
 */
#ifdef ATL_GAS_x8632
        movl    4(%eax), itmp
        movl    itmp, MM0
        movl    8(%eax), itmp
        movl    itmp, NN
        movl    12(%eax), KK            /* load K */
        movl    24(%eax), pA0
        movl    32(%eax), pB0
        movl    36(%eax), itmp          /* itmp = ldb */
        lea     (pB0, itmp, 8), pB1      /* pB1 = pB0 + ldb*sizeof */
        shl     $4, itmp                /* itmp = 2*sizeof*ldb */
        movl    itmp, incBn             /* incBn = 2*sizeof*ldb */
        add     pB0, itmp
        movl    itmp, PFB
#ifdef BETAX
        movsd   40(%eax), rB0           /* load beta */
        unpcklpd rB0, rB0               /* rB0 = {beta, beta} */
        MOVAPD  rB0, BETAOFF(%esp)      /* store BETA to BETAOFF */
#endif
        movl    48(%eax), pC0
        movl    52(%eax), itmp          /* itmp = ldc */
        shl     $CSH, itmp              /* itmp = ldc*sizeof */
        movl    itmp, ldcOFF(%esp)      /* ldc = ldc*sizeof */
        shr     $CSH-1, itmp            /* itmp = 2*ldc */
        sub     MM0, itmp               /* itmp = 2*ldc - M */
        shl     $CSH, itmp              /* itmp = (2*ldc-M)*sizeof */
        movl    itmp, incCn             /* incCn = (2*ldc-M)*sizeof */
        movl    28(%eax), itmp          /* itmp = lda */
        lea     (pA0, itmp,8), pA1      /* just overwrote old SP in EAX */
        shl     $4, itmp                /* itmp = 2*sizeof*lda */
        movl    itmp, incAm             /* incAm = 2*sizeof*lda */

/*
 *      pfA = A + 2*lda*M; incAn = lda*M
 */
        movl    MM0, itmp       /* itmp = M */
        imull   incAm, itmp     /* itmp = 2*lda*M */
@skip        add     pA0, itmp       /* itmp = 2*lda*M + pA0 */
        lea     PFAINC(pA0, itmp), itmp /* pfA = pA0 + 2*lda*M - PFAINC */
        movl    itmp, PFA       /* pfA = 2*lda*M + pA0 - PFAINC */
        sub     pA0, itmp       /* itmp = 2*lda*M - PFAINC*/
        sub     $PFAINC, itmp   /* itmp = 2*lda*M */
        shr     $1, itmp        /* itmp = lda*M */
        movl    itmp, incAn     /* incAn = lda*M */
#else
/*
 *      Get parameters moves to correct registers
 */
        movq    %rdi, MM
        movq    %rsi, NN
        movq    %r8, pA1                /* pA1 = lda */
        movq    %r9, pB0                /* pB0 = B */
        movslq  8(%rsp), pB1            /* pB1 = ldb */
        unpcklpd        %xmm1, %xmm1    /* xmm1 = {beta, beta} */
        MOVAPD  %xmm1, BETA
        movq    16(%rsp), pC0           /* pC0 = C */
        movslq  24(%rsp), ldc           /* ldc = ldc */
/*
 *      ===================================================
 *      Compute rest of needed variables using these inputs
 *      ===================================================
 */
        shl     $3, pB1                 /* pB1 = ldb*sizeof */
        lea     (pB1, pB1), incBn       /* incBn = 2*ldb*sizeof */
        add     pB0, pB1                /* pB1 = pB0 + ldb*sizeof */
        lea     (ldc,ldc), PFA          /* PFA = 2*ldc */
        sub     MM, PFA                 /* PFA = 2*ldc - M */
        shl     $CSH, PFA               /* PFA = (2*ldc-M)*sizeof */
        movq    PFA, incCn              /* incCn = (2*ldc-M)*sizeof */
        shl     $CSH, ldc                 /* ldc *= sizeof */
        shl     $3, pA1                 /* pA1 = lda * sizeof */
        lea     (pA1, pA1), incAm       /* incAm = 2*lda*sizeof */
        mov     MM, PFA                 /* PFA = M */
        imulq   pA1, PFA                /* PFA = M * lda*sizeof */
        movq    PFA, incAn              /* incAn = M*lda*sizeof */
        lea     PFAINC(pA0,PFA,2),PFA   /* PFA = pA0+2*M*lda*sizeof - PFAINC */
        add     pA0, pA1                /* pA1 = pA0 + lda*sizeof */
        mov     pB0, PFB                /* PBF = pB0 */
        add     incBn, PFB              /* PFB = pB0 + 2*ldb*sizeof */
        movq    MM, MM0                 /* MM0 = MM */
#endif
        sub     $2, KK          /* must stop K it early to drain advance load */
        jz      K_IS_2
/*
 *      Have pA/B point to end of column, so we can run loop backwards
 */
        lea     (pA0, KK, 8), pA0       /* pA0 += K */
        lea     (pB0, KK, 8), pB0       /* pB0 += K */
        lea     (pA1, KK, 8), pA1       /* pA1 += K */
        lea     (pB1, KK, 8), pB1       /* pB1 += K */
        neg     KK                      /* KK = -K */
        add     $2, KK
        jz      K_IS_4
        movq    KK, KK0
#ifdef ATL_GAS_x8632
        movl    MM0, itmp
        movl    itmp, MM
#else
        movq    MM, MM0
#endif
#ifndef DCPLX
        test    $15, pC0
        jnz     UNALIGNED_C
        testq   $15, ldc
        jnz     UNALIGNED_C

@beginproc sse2sum
#ifndef ATL_SSE3
        MOVAPD          rC00, rA0       /* rA0  = c00a c00b */
        MOVAPD          rC01, rB0       /* rB0  = c01a c01b */
        unpcklpd        rC10, rC00      /* rC00 = c00a c10a */
        unpcklpd        rC11, rC01      /* rC01 = c01a c11a */
        unpckhpd        rC10, rA0       /* rA0  = c00b c10b */
        unpckhpd        rC11, rB0       /* rB0  = c01b c11b */
        addpd           rA0, rC00       /* rC00 = c00ab c10ab */
        addpd           rB0, rC01       /* rC01 = c01ab c11ab */
#endif
@endproc
ALIGN16
@beginproc LOOPS mnloop kloop movC
@(mnloop):
/*
 *      Peel 1st iteration of K to avoid need to zero rCxx
 */
        MOVAB   -16(pB0,KK,8), rA0
        MOVAB   -16(pA0,KK,8), rC00
        MOVAPD  rC00, rC01
        mulpd   rA0, rC00
        #ifdef ATL_GAS_x8632
           movq    PFB, itmp
        #else
           #ifndef BETA0
              #ifdef DCPLX
                 movsd (pC0), rC0
                 movhpd 16(pC0), rC0
              #else
                 @(movC)  (pC0), rC0
              #endif
           #else
               prefB((PFB))
           #endif
        #endif
        MOVAB   -16(pA1,KK,8), rC10
        MOVAPD  rC10, rC11
        mulpd   rA0, rC10
        #ifdef ATL_GAS_x8632
           prefB((itmp))
        #else
           #ifndef BETA0
              #ifdef DCPLX
                 movsd (pC0,ldc), rC1
                 movhpd 16(pC0,ldc), rC1
              #else
                 @(movC)  (pC0,ldc), rC1
              #endif
           #else
               add      $PFBINC, PFB
           #endif
        #endif
        MOVAB   -16(pB1,KK,8), rA0
        mulpd   rA0, rC01
           MOVAB   (pB0,KK,8), rB0
        mulpd   rA0, rC11
        #if !defined(ATL_GAS_x8632) && !defined(BETA0)
               prefB((PFB))
        #endif
ALIGN16
@(kloop):
        MOVAB   (pA0,KK,8), rA0
        MOVAPD  rA0, ra0
        mulpd   rB0, rA0
        addpd   rA0, rC00
        MOVAB   (pA1,KK,8), rA1
        mulpd   rA1, rB0
        addpd   rB0, rC10
        MOVAB   (pB1,KK,8), rB0
        mulpd   rB0, ra0
        mulpd   rB0, rA1
           MOVAB   16(pB0,KK,8), rB0
           add  $2, KK
        addpd   ra0, rC01
        addpd   rA1, rC11
        jnz     @(kloop)
/*
 *      Peel last iteration to stop forward fetch of B
 */
        MOVAB   (pA0), rA0
        #ifdef ATL_GAS_x8632
           add     $PFBINC, itmp
        #elif defined(BETAX)
           mulpd BETA, rC0
        #endif
        MOVAPD  rA0, ra0
        mulpd   rB0, rA0
        #ifdef ATL_GAS_x8632
           movl    itmp, PFB
        #else
           pref2((PFA))
        #endif
        addpd   rA0, rC00
        MOVAB   (pA1), rA1
        mulpd   rA1, rB0
        #ifdef ATL_GAS_x8632
           movq    PFA, itmp
        #elif defined(BETAX)
           mulpd  BETA, rC1
        #endif
        addpd   rB0, rC10
        MOVAB   (pB1), rB0
        mulpd   rB0, ra0
        #ifdef ATL_GAS_x8632
           prefetcht1      (itmp)
        #elif !defined(BETA0)
           add  $PFBINC, PFB
        #endif
        mulpd   rB0, rA1
        #ifdef ATL_GAS_x8632
           add     $PFAINC, itmp
        #else
           add     $PFAINC, PFA
        #endif
        addpd   ra0, rC01
        #ifdef ATL_GAS_x8632
           movl    itmp, PFA
        #endif
        addpd   rA1, rC11

@callproc sse2sum
#ifdef ATL_GAS_x8632
        movl    ldcOFF(%esp), itmp
@mif movC ! "MOVAPD
   #ifdef DCPLX
        #ifdef ATL_SSE3
           haddpd  rC10, rC00
        #endif
        #ifndef BETA0
           #ifdef BETAX
              movapd BETAOFF(%esp), rB0
           #endif
           movsd (pC0), rA0
           movhpd 16(pC0), rA0
           #ifdef BETAX
              mulpd  rB0, rA0
           #endif
           addpd  rA0, rC00
        #endif
        #ifdef ATL_SSE3
           haddpd  rC11, rC01
        #endif
        #ifndef BETA0
           movsd (pC0,itmp), rA1
           movhpd 16(pC0,itmp), rA1
           #ifdef BETAX
              mulpd  rB0, rA1
           #endif
           addpd  rA1, rC01
        #endif
   #else
@endmif
        #ifdef ATL_SSE3
           haddpd  rC10, rC00
        #endif
        #ifdef BETAX
           MOVAPD BETAOFF(%esp), rB0
           @(movC) (pC0), rA0
           mulpd  rB0, rA0
           addpd  rA0, rC00
        #endif
        #ifdef BETA1
        @mif movC = "MOVAPD
           addpd  (pC0), rC00
        @endmif
        @mif movC ! "MOVAPD
           @(movC) (pC0), rA0
           addpd   rA0, rC00
        @endmif
        #endif
        #ifdef ATL_SSE3
           haddpd  rC11, rC01
        #endif
        #ifdef BETAX
           @(movC) (pC0,itmp), rA1
           mulpd  rB0, rA1
           addpd  rA1, rC01
        #endif
        #ifdef BETA1
        @mif movC ! "MOVAPD
           @(movC) (pC0, itmp), rA1
           addpd  rA1, rC01
        @endmif
        @mif movC = "MOVAPD
           addpd  (pC0,itmp), rC01
        @endmif
        #endif
@mif movC ! "MOVAPD
   #endif
@endmif
#else
        #ifdef ATL_SSE3
           haddpd  rC10, rC00
        #endif
        #ifndef BETA0
           addpd  rC0, rC00
        #endif
        #ifdef ATL_SSE3
           haddpd  rC11, rC01
        #endif
        #ifndef BETA0
           addpd  rC1, rC01
        #endif
#endif
        add     $2*CMUL(8), pC0
        addq    incAm, pA0    /* pA0 += lda*sizeof*2 */
        addq    incAm, pA1    /* pA1 += lda*sizeof*2 */
        subq    $2, MM
        movq    KK0, KK
   #ifdef DCPLX
        movlpd  rC00, -32(pC0)
        movhpd  rC00, -16(pC0)
        movlpd  rC01, -32(pC0,itmp)
        movhpd  rC01, -16(pC0,itmp)
   #else
        @(movC)  rC00, -2*CMUL(8)(pC0)
        @(movC)  rC01, -2*CMUL(8)(pC0,itmp)
   #endif
        jnz     @(mnloop)

#ifdef ATL_GAS_x8632
        movl    MM0, itmp
        movl    itmp, MM
#else
        movq    MM0, MM
#endif
        movq    KK0, KK
        subq    incAn, pA0
        subq    incAn, pA1
        addq    incCn, pC0
        addq    incBn, pB0
        addq    incBn, pB1
        subq    $2, NN
        jnz     @(mnloop)
@endproc
@callproc LOOPS MNLOOP KLOOP MOVAPD
@beginproc DONE
#ifdef ATL_GAS_x8632
        movl    BETASZ(%esp), %ebp
        movl    BETASZ+4(%esp), %ebx
        movl    BETASZ+8(%esp), %esi
        movl    BETASZ+12(%esp), %edi
        movl    BETASZ+16(%esp), %esp    /* restore saved original SP */
#else
        movq    -8(%rsp), %rbp
        movq    -16(%rsp), %rbx
        movq    -24(%rsp), %r12
        movq    -32(%rsp), %r13
        movq    -40(%rsp), %r14
        movq    -48(%rsp), %r15
#endif
        ret
@endproc DONE
@callproc DONE
#endif  /* end of ifndef DCPLX -- CPLX must use unaligned loads to C */
ALIGN16
/*
 * Code specialized for when C or ldc is not aligned to 16-byte boundary, so
 * we must use unaligned loads.  This is a big cost on Core2 systems
 */
@callproc LOOPS UNALIGNED_C UKLOOP MOVUPD
@callproc DONE
/*
 * Code specialized for K == 2; pA0 & pAB pt to start of arrays
 * Assume C unaligned so we don't have to write this cleanup case twice.
 * This assumption costs you major perf. if you care about this case
 * (since load/store of C dominant cost when K=2).  This code is more for
 * correctness than perf. as presently written.
 */
K_IS_2:
#ifdef ATL_GAS_x8632
        movq    ldc, itmp
#endif
        movq    MM0, KK          /* KK is now M-loop counter */
ALIGN16
MNLOOP_K2:
   #ifdef BETA0
        MOVAB   (pB0), rA0
        MOVAB   (pA0), rC00
        MOVAPD  rC00, rC01
        mulpd   rA0, rC00
        MOVAB   (pA1), rC10
        MOVAPD  rC10, rC11
        mulpd   rA0, rC10
        MOVAB   (pB1), rA0
        mulpd   rA0, rC01
        mulpd   rA0, rC11
   #else
        movsd   (pC0), rC00
        movsd   CMUL(8)(pC0), rC10
        movsd   (pC0,itmp), rC01
        movsd   CMUL(8)(pC0,itmp), rC11
        #ifdef BETAX
           #ifdef ATL_GAS_x8632
              MOVAPD BETAOFF(%esp), rA1
              mulpd  rA1, rC00
              mulpd  rA1, rC10
              mulpd  rA1, rC01
              mulpd  rA1, rC11
           #else
              mulpd  BETA, rC00
              mulpd  BETA, rC10
              mulpd  BETA, rC01
              mulpd  BETA, rC11
           #endif
        #endif
        MOVAB   (pA0), rA0
        MOVAB   (pB0), rB0
        MOVAPD  rA0, ra0
        mulpd   rB0, rA0
        addpd   rA0, rC00
        MOVAB   (pA1), rA1
        mulpd   rA1, rB0
        addpd   rB0, rC10
        MOVAB   (pB1), rB0
        mulpd   rB0, ra0
        mulpd   rB0, rA1
        addpd   ra0, rC01
        addpd   rA1, rC11
   #endif
           add     $2*CMUL(8), pC0
@callproc sse2sum
        #ifdef ATL_SSE3
           haddpd  rC10, rC00
        #endif
           addq  incAm, pA0
        #ifdef ATL_SSE3
           haddpd  rC11, rC01
        #endif
           addq  incAm, pA1
           sub   $2, KK
   #ifdef DCPLX
        movlpd  rC00, -32(pC0)
        movhpd  rC00, -16(pC0)
        movlpd  rC01, -32(pC0,itmp)
        movhpd  rC01, -16(pC0,itmp)
   #else
        MOVUPD  rC00, -2*8(pC0)
        MOVUPD  rC01, -2*8(pC0,itmp)
   #endif
        jnz MNLOOP_K2                   /* end of M-loop */

        subq    incAn, pA0
        subq    incAn, pA1
        addq    incCn, pC0
        addq    incBn, pB0
        addq    incBn, pB1
        subq    $2, NN
        mov     MM0, KK
        jnz     MNLOOP_K2
@callproc DONE
/*
 * Code specialized for K == 4; pA0 & pAB pt to start of arrays + 16
 * Assume C unaligned so we don't have to write this cleanup case twice.
 * This assumption costs you major perf. if you care about this case
 * (since load/store of C dominant cost when K=4).  This code is more for
 * correctness than perf. as presently written.
 */
K_IS_4:
#ifdef ATL_GAS_x8632
        movq    ldc, itmp
#endif
        movq    MM0, KK          /* KK is now M-loop counter */
ALIGN16
MNLOOP_K4:
   #ifdef BETA0
        MOVAB   -16(pB0), rA0
        MOVAB   -16(pA0), rC00
        MOVAPD  rC00, rC01
        mulpd   rA0, rC00
        MOVAB   -16(pA1), rC10
        MOVAPD  rC10, rC11
        mulpd   rA0, rC10
        MOVAB   -16(pB1), rA0
        mulpd   rA0, rC01
        MOVAB   (pB0), rB0
        mulpd   rA0, rC11
   #else
        movsd   (pC0), rC00
        movsd   CMUL(8)(pC0), rC10
        movsd   (pC0,itmp), rC01
        movsd   CMUL(8)(pC0,itmp), rC11
        #ifdef BETAX
           #ifdef ATL_GAS_x8632
              MOVAPD BETAOFF(%esp), rA1
              mulpd  rA1, rC00
              mulpd  rA1, rC10
              mulpd  rA1, rC01
              mulpd  rA1, rC11
           #else
              mulpd  BETA, rC00
              mulpd  BETA, rC10
              mulpd  BETA, rC01
              mulpd  BETA, rC11
           #endif
        #endif
        MOVAB   -16(pA0), rA0
        MOVAB   -16(pB0), rB0
        MOVAPD  rA0, ra0
        mulpd   rB0, rA0
        addpd   rA0, rC00
        MOVAB   -16(pA1), rA1
        mulpd   rA1, rB0
        addpd   rB0, rC10
        MOVAB   -16(pB1), rB0
        mulpd   rB0, ra0
        mulpd   rB0, rA1
        MOVAB   (pB0), rB0
        addpd   ra0, rC01
        addpd   rA1, rC11
   #endif
        MOVAB   (pA0), rA0
        MOVAPD  rA0, ra0
        mulpd   rB0, rA0
        addpd   rA0, rC00
        MOVAB   (pA1), rA1
        mulpd   rA1, rB0
        addpd   rB0, rC10
        MOVAB   (pB1), rB0
        mulpd   rB0, ra0
        mulpd   rB0, rA1
        addpd   ra0, rC01
        addpd   rA1, rC11
           add     $2*CMUL(8), pC0
@callproc sse2sum
        #ifdef ATL_SSE3
           haddpd  rC10, rC00
        #endif
           addq  incAm, pA0
        #ifdef ATL_SSE3
           haddpd  rC11, rC01
        #endif
           addq  incAm, pA1
           sub   $2, KK
   #ifdef DCPLX
        movlpd  rC00, -32(pC0)
        movhpd  rC00, -16(pC0)
        movlpd  rC01, -32(pC0,itmp)
        movhpd  rC01, -16(pC0,itmp)
   #else
        MOVUPD  rC00, -2*CMUL(8)(pC0)
        MOVUPD  rC01, -2*CMUL(8)(pC0,itmp)
   #endif
        jnz MNLOOP_K4                   /* end of M-loop */

        subq    incAn, pA0
        subq    incAn, pA1
        addq    incCn, pC0
        addq    incBn, pB0
        addq    incBn, pB1
        subq    $2, NN
        movq    MM0, KK
        jnz     MNLOOP_K4
@callproc DONE
@ROUT ATL_dmm2x2x128_sse2
   @define Kmax @128@
   @define TSH @3@
   @define Kmul @2@
@ROUT ATL_smm2x2x256_sse
   @define Kmax @256@
   @define TSH @2@
   @define Kmul @4@
@ROUT ATL_dmm2x2x128_sse2 ATL_smm2x2x256_sse
#include "atlas_asm.h"
#if !defined(ATL_GAS_x8664) && !defined(ATL_GAS_x8632)
   #error "This kernel requires x86 assembly!"
#endif

#if !defined(KB) || (KB == 0)
   #error "KB must be a compile-time constant!"
#endif
#if KB > @(Kmax)
   #error "KB can at most be @(Kmax)!"
#endif
#if (KB/@(Kmul))*@(Kmul) != KB
   #error "KB must be a multiple of @(Kmul)
#endif

@ROUT ATL_smm2x2x256_sse
#ifdef SCPLX
   #define CMUL(arg_) 2*arg_
   #define CSH 3
#else
   #define CMUL(arg_) arg_
   #define CSH 2
#endif
@ROUT ATL_dmm2x2x128_sse2 
#ifdef DCPLX
   #define CMUL(arg_) 2*arg_
   #define CSH 4
#else
   #define CMUL(arg_) arg_
   #define CSH 3
#endif
@ROUT ATL_dmm2x2x128_sse2 ATL_smm2x2x256_sse
#ifdef ATL_GAS_x8632
   #define movq movl
   #define addq addl
   #define subq subl
   #define shrq shrl
   #define testq testl
   #define rsp  esp
   #ifdef BETAX
      #define BETAOFF 0
      #define BETASZ 16
   #else
      #define BETASZ 0
   #endif
   #define FSIZE 12*4+BETASZ
   #define PFAOFF  FSIZE-4
   #define PFBOFF  PFAOFF-4
   #define IOFF    PFBOFF-4
   #define MOFF    IOFF-4
   #define iAnOFF  MOFF-4
   #define iCnOFF  iAnOFF-4
   #define JOFF    iCnOFF-4
#endif
/*
 *Integer register usage shown by these defines
 */
#ifdef ATL_GAS_x8632
   #define pA0     %ecx
   #define lda     %eax
   #define pB0     %ebx
   #define ldb     %edi
   #define pC0     %esi
   #define itmp    %edx
   #define ldc     %ebp
   
@skip   #define incAm   iAmOFF(%esp)  /* not needed, just use lda*2 */
   #define incAn   iAnOFF(%esp)
@skip   #define incBn   iBnOFF(%esp)  /* not needed, just use ldb*2 */
   #define incCn   iCnOFF(%esp)
   #define MM      IOFF(%esp)
   #define NN      JOFF(%esp)
   #define MM0     MOFF(%esp)
   #define pfA     PFAOFF(%esp)
   #define pfB     PFBOFF(%esp)
   #define BETA    BETAOFF(%esp)
#else
   #define pA0     %rcx
   #define lda     %rbx
   #define pB0     %rbp
   #define ldb     %rax
   #define pC0     %rdi
   #define pfA     %rdx
   #define pfB     %rsi
   #define incCn   %r8
   #define ldc     %r9
   #define MM0     %r10
   #define incAn   %r11
   #define MM      %r12
   #define NN      %r13
   #define itmp    ldc
@skip   #define incAm   %r14
@skip   #define incBn   %r15
#endif
#define rA0     %xmm0
#define rA1     %xmm1
#define ra0     %xmm2
#define rB0     %xmm3
#define rC00    %xmm4
#define rC10    %xmm5
#define rC01    %xmm6
#define rC11    %xmm7
#ifdef ATL_GAS_x8664
   #define BETA %xmm8
@skip   #define rC0  %xmm9
@skip   #define rC1 %xmm10
#endif
/*
 * Define some macros for instruction selection
 *    VZERO: xorpd, xorps, pxor
 *    MOVAB: movapd,movaps or movupd/movups
 */
#define VZERO(reg_) xorps reg_, reg_
#define MOVAB  movaps
#define MOVAPD movaps
#define MOVUPD movups
#define PFAINC -64
#define PFBINC 32
#define movlpd movlps
#define movhpd movhps
#if 1
   #define pref2(mem) prefetcht1        mem
   #define prefB(mem) prefetcht0        mem
   #define prefC(mem) prefetcht0        mem
#else
   #define pref2(mem)
   #define prefB(mem)
   #define prefC(mem)
#endif
@ROUT ATL_dmm2x2x128_sse2
/*
                      %rdi/4       %rsi/8       %rdx/12          %xmm0/16
 void ATL_USERMM(const int M, const int N, const int K, const TYPE alpha,
                       %rcx/24         %r8/28         %r9/32           8/36
                 const TYPE *A, const int lda, const TYPE *B, const int ldb,
                        %xmm1/40    16/48          24/52
                 const TYPE beta, TYPE *C, const int ldc)
*/
@ROUT ATL_smm2x2x256_sse
/*
                      %rdi/4       %rsi/8       %rdx/12          %xmm0/16
 void ATL_AUSERMM(const int M, const int N, const int K, const TYPE alpha,
                       %rcx/20         %r8/24         %r9/28           8/32
                 const TYPE *A, const int lda, const TYPE *B, const int ldb,
                        %xmm1/36    16/40          24/44
                 const TYPE beta, TYPE *C, const int ldc)
*/
@ROUT ATL_dmm2x2x128_sse2 ATL_smm2x2x256_sse
        .text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 *      Save callee-saved iregs
 */
#ifdef ATL_GAS_x8632
        movl    %esp, %eax              /* save original stack ptr */
        sub     $FSIZE, %esp            /* allocate stack space */
        andw    $0xFFF0, %sp            /* SP now 16-byte aligned */
        movl    %ebp, BETASZ(%esp)
        movl    %ebx, BETASZ+4(%esp)
        movl    %esi, BETASZ+8(%esp)
        movl    %edi, BETASZ+12(%esp)
        movl    %eax, BETASZ+16(%esp)   /* original SP saved to new stack */
#else
        movq    %rbp, -8(%rsp)
        movq    %rbx, -16(%rsp)
        movq    %r12, -24(%rsp)
        movq    %r13, -32(%rsp)
@skip        movq    %r14, -40(%rsp)
@skip        movq    %r15, -48(%rsp)
#endif
        
/*
 *      Setup input parameters
 *      For x8632 %eax has old stack ptr; eax is lda, so set this up late
 */
#ifdef ATL_GAS_x8632
        movl    4(%eax), itmp
        movl    itmp, MM0
        movl    itmp, MM
        movl    8(%eax), itmp
        movl    itmp, NN
@ROUT ATL_smm2x2x256_sse
        movl    20(%eax), pA0
        movl    28(%eax), pB0
        movl    32(%eax), ldb           /* ldb = ldb */
@ROUT ATL_dmm2x2x128_sse2
        movl    24(%eax), pA0
        movl    32(%eax), pB0
        movl    36(%eax), ldb           /* ldb = ldb */
@ROUT ATL_dmm2x2x128_sse2 ATL_smm2x2x256_sse
        shl     $@(TSH), ldb                 /* ldb = ldb*sizeof */
        lea     (pB0, ldb, 2), itmp     /* itmp = pB0 + 2*ldb*sizeof */
        movl    itmp, pfB               /* pfB = pB0 + 2*ldb*sizeof */
#ifdef BETAX
@ROUT ATL_dmm2x2x128_sse2
        movsd   40(%eax), rB0           /* load beta */
        unpcklpd rB0, rB0               /* rB0 = {beta, beta} */
        MOVAPD  rB0, BETAOFF(%esp)      /* store BETA to BETAOFF */
#endif
        movl    48(%eax), pC0
        movl    52(%eax), ldc           /* ldc = ldc */
@ROUT ATL_smm2x2x256_sse
        movss   36(%eax), rB0           /* load beta */
        shufps  $0, rB0, rB0            /* rB0 = beta,beta,beta,beta */
        movaps  rB0, BETAOFF(%esp)      /* store BETA to stack */
#endif
        movl    40(%eax), pC0
        movl    44(%eax), ldc           /* ldc = ldc */
@ROUT ATL_dmm2x2x128_sse2 ATL_smm2x2x256_sse
        lea     (ldc,ldc), itmp         /* itmp = 2*ldc */
        shl     $CSH, ldc               /* ldc = ldc*sizeof */
        sub     MM0, itmp               /* itmp = 2*ldc - M */
        shl     $CSH, itmp              /* itmp = (2*ldc-M)*sizeof */
        movl    itmp, incCn             /* incCn = (2*ldc-M)*sizeof */
@ROUT ATL_dmm2x2x128_sse2 `        movl    28(%eax), lda           /* lda = lda; overwrote old SP in EAX */`
@ROUT ATL_smm2x2x256_sse `        movl    24(%eax), lda           /* lda = lda; overwrote old SP in EAX */`
        shl     $@(TSH), lda                 /* lda = lda*sizeof */
/*
 *      pfA = A + 2*lda*M; incAn = lda*M
 */
        lea     (lda,lda), itmp         /* itmp = 2*lda*sizeof */
        imull   MM0, itmp               /* itmp = 2*lda*M*sizeof */
        lea     PFAINC(pA0, itmp), itmp /* pfA = pA0 + 2*lda*M - PFAINC */
        movl    itmp, pfA               /* pfA = 2*lda*M + pA0 - PFAINC */
        sub     pA0, itmp               /* itmp = 2*lda*M - PFAINC*/
        sub     $PFAINC, itmp           /* itmp = 2*lda*M */
        shr     $1, itmp                /* itmp = lda*M */
        movl    itmp, incAn             /* incAn = lda*M */
        movl    MM0, itmp
        movl    itmp, MM
#else
/*
 *      Get parameters moves to correct registers
 */
        movq    %rdi, MM
        movq    %rsi, NN
        movq    %r8, lda                /* lda = lda */
        movq    %r9, pB0                /* pB0 = B */
        movslq  8(%rsp), ldb            /* ldb = ldb */
@ROUT ATL_dmm2x2x128_sse2 `        unpcklpd        %xmm1, %xmm1    /* xmm1 = beta, beta */`
@ROUT ATL_smm2x2x256_sse `        shufps  $0, %xmm1, %xmm1            /* rB0 = beta,beta,beta,beta */`
        MOVAPD  %xmm1, BETA             /* BETA = beta, beta */
        movq    16(%rsp), pC0           /* pC0 = C */
        movslq  24(%rsp), ldc           /* ldc = ldc */
/*
 *      ===================================================
 *      Compute rest of needed variables using these inputs
 *      ===================================================
 */
        shl     $@(TSH), ldb                 /* ldb = ldb*sizeof */
        lea     (ldc,ldc), incCn        /* incCn = 2*ldc */
        sub     MM, incCn               /* incCn = 2*ldc - M */
        shl     $CSH, incCn             /* incCn = (2*ldc-M)*sizeof */
        shl     $CSH, ldc               /* ldc *= sizeof */
        shl     $@(TSH), lda                 /* lda = lda * sizeof */
        mov     MM, incAn               /* incAn = M */
        imulq   lda, incAn              /* incAn = M * lda*sizeof */
        lea     PFAINC(pA0,incAn,2),pfA /* pfA = pA0+2*M*lda*sizeof - PFAINC */
        lea     (pB0, ldb, 2),pfB       /* pfB = pB0 + 2*ldb*sizeof */
        mov     MM, MM0                 /* MM0 = MM */
#endif
/*
 *      Advance A & B by 128 so we can use byte indexing as long as possible
 */
        sub     $-128, pA0              /* pA0 += 128 */
        sub     $-128, pB0              /* pB0 += 128 */
@ROUT ATL_dmm2x2x128_sse2
/*
 * ======================================================================
 * This set of loops assumes aligned C & ldc; CPLX code takes every other
 * elt of C, and thus can't use aligned C access in any case
 * ======================================================================
 */
#ifndef DCPLX
        test    $15, pC0
        jnz     UNALIGNED_C
        testq   $15, ldc
        jnz     UNALIGNED_C

@beginproc sse2sum
#ifndef ATL_SSE3
/*
 * After loop pC00 is in rA0, and pC01 is in ra0, so use pB0 & rA1 here
 */
        MOVAPD          rC00, rA1       /* rA1  = c00a c00b */
        MOVAPD          rC01, rB0       /* rB0  = c01a c01b */
        unpcklpd        rC10, rC00      /* rC00 = c00a c10a */
        unpcklpd        rC11, rC01      /* rC01 = c01a c11a */
        unpckhpd        rC10, rA1       /* rA1  = c00b c10b */
        unpckhpd        rC11, rB0       /* rB0  = c01b c11b */
        addpd           rA1, rC00       /* rC00 = c00ab c10ab */
        addpd           rB0, rC01       /* rC01 = c01ab c11ab */
#endif
@endproc
ALIGN16
@beginproc LOOPS mnloop movC
@(mnloop):
/*
 *      Peel 1st iteration of K to avoid need to zero rCxx
 */
        MOVAB   -128(pB0), rA0
        MOVAB   -128(pA0), rC00
        MOVAPD  rC00, rC01
        mulpd   rA0, rC00
        #ifdef ATL_GAS_x8632
           movq    pfB, itmp
        #else
           prefB((pfB))
        #endif
        MOVAB   -128(pA0,lda), rC10
        MOVAPD  rC10, rC11
        mulpd   rA0, rC10
        #ifdef ATL_GAS_x8632
           prefB((itmp))
        #else
           add      $PFBINC, pfB
        #endif
        MOVAB   -128(pB0,ldb), rA0
        mulpd   rA0, rC01
        #if KB > 2
           MOVAB   -112(pB0), rB0
        #endif
        mulpd   rA0, rC11
        #if KB == 2
           #ifdef DCPLX
              movsd  (pC0), rA0
              movhpd 16(pC0), rA0
              movsd  (pC0,ldc), ra0
              movhpd 16(pC0,ldc), ra0
           #else
              @(movC) (pC0), rA0
              @(movC) (pC0,ldc), ra0
           #endif
        #endif
        #ifdef ATL_GAS_x8632
           add  $PFBINC, pfB
        #endif

@define i @4@
@define j @-112@
@iwhile i < 128
   #if KB > @(i)
        MOVAB   @(j)(pA0), rA0
        MOVAPD  rA0, ra0
        mulpd   rB0, rA0
        addpd   rA0, rC00
        MOVAB   @(j)(pA0,lda), rA1
        mulpd   rA1, rB0
        addpd   rB0, rC10
        MOVAB   @(j)(pB0,ldb), rB0
        mulpd   rB0, ra0
        mulpd   rB0, rA1
           MOVAB   @(j)+16(pB0), rB0
        addpd   ra0, rC01
        addpd   rA1, rC11
   #elif KB == @(i)
        MOVAB   @(j)(pA0), rA0
        MOVAPD  rA0, ra0
        mulpd   rB0, rA0
        addpd   rA0, rC00
        MOVAB   @(j)(pA0,lda), rA1
        mulpd   rA1, rB0
        addpd   rB0, rC10
        MOVAB   @(j)(pB0,ldb), rB0
        mulpd   rB0, ra0
        #ifndef BETA0
           #ifdef DCPLX
              movsd  (pC0), rA0
           #else
              @(movC) (pC0), rA0
           #endif
        #endif
        mulpd   rB0, rA1
        #if !defined(BETA0) && defined(DCPLX)
           movhpd 16(pC0), rA0
        #endif
        addpd   ra0, rC01
        #ifndef BETA0
           #ifdef DCPLX
              movsd (pC0,ldc), ra0
           #else
              @(movC) (pC0,ldc), ra0
           #endif
        #endif
        addpd   rA1, rC11
        #if !defined(BETA0) && defined(DCPLX)
           movhpd 16(pC0,ldc), ra0
        #endif
   #endif
   @iexp i 2 @(i) +
   @iexp j 16 @(j) +
@endiwhile
@undef i
@undef j

        #ifdef ATL_GAS_x8632
           movl    pfA, itmp
        #else
           pref2((pfA))
        #endif
/* 
 * After loop pC00 is in rA0, and pC01 is in ra0
 */
@callproc sse2sum
        #ifdef ATL_SSE3
           haddpd  rC10, rC00
        #endif
        #ifdef ATL_GAS_x8632
           pref2((itmp))
        #else
           add  $PFAINC, pfA
        #endif
        #ifdef BETAX
           #ifdef ATL_GAS_x8632
              movapd       BETAOFF(%esp), rB0
              mulpd        rB0, rA0
           #else
              mulpd    BETA, rA0
           #endif
        #endif
        #ifndef BETA0
           addpd  rA0, rC00
        #endif
        #ifdef ATL_SSE3
           haddpd  rC11, rC01
        #endif
        #ifdef ATL_GAS_x8632
           add     $PFAINC, itmp
        #endif
        #ifdef BETAX
           #ifdef ATL_GAS_x8632
              mulpd        rB0, ra0
           #else
              mulpd    BETA, ra0
           #endif
        #endif
        #ifndef BETA0
           addpd  ra0, rC01
        #endif
        #ifdef ATL_GAS_x8632
           movl    itmp, pfA
        #endif

        add     $2*CMUL(8), pC0
        lea     (pA0, lda, 2), pA0
        subq    $2, MM
        #ifdef ATL_GAS_x8632
           movl    ldc, itmp
        #endif
   #ifdef DCPLX
        movlpd  rC00, -32(pC0)
        movhpd  rC00, -16(pC0)
        movlpd  rC01, -32(pC0,itmp)
        movhpd  rC01, -16(pC0,itmp)
   #else
        @(movC)  rC00, -2*CMUL(8)(pC0)
        @(movC)  rC01, -2*CMUL(8)(pC0,itmp)
   #endif
        jnz     @(mnloop)

        subq    incAn, pA0
        addq    incCn, pC0
        lea     (pB0, ldb, 2), pB0
        subq    $2, NN
#ifdef ATL_GAS_x8632
        movl    MM0, itmp
        movl    itmp, MM
#else
        movq    MM0, MM
#endif
        jnz     @(mnloop)
@endproc
@callproc LOOPS MNLOOP MOVAPD
@ROUT ATL_dmm2x2x128_sse2 ATL_smm2x2x256_sse
@beginproc DONE
#ifdef ATL_GAS_x8632
        movl    BETASZ(%esp), %ebp
        movl    BETASZ+4(%esp), %ebx
        movl    BETASZ+8(%esp), %esi
        movl    BETASZ+12(%esp), %edi
        movl    BETASZ+16(%esp), %esp    /* restore saved original SP */
#else
        movq    -8(%rsp), %rbp
        movq    -16(%rsp), %rbx
        movq    -24(%rsp), %r12
        movq    -32(%rsp), %r13
@skip        movq    -40(%rsp), %r14
@skip        movq    -48(%rsp), %r15
#endif
        ret
@endproc DONE
@ROUT ATL_dmm2x2x128_sse2
@callproc DONE
#endif  /* end of ifndef DCPLX -- CPLX must use unaligned loads to C */
ALIGN16
/*
 * Code specialized for when C or ldc is not aligned to 16-byte boundary, so
 * we must use unaligned loads.  This is a big cost on Core2 systems
 */
@callproc LOOPS UNALIGNED_C MOVUPD
@callproc DONE
@ROUT ATL_smm2x2x256_sse
MNLOOP:
/*
 * Peel first iteration of K to avoid need to zero rCxx
 */
   movaps       -128(pB0), rA0
   movaps       -128(pA0), rC00
   movaps       rC00, rC01
   mulps        rA0, rC00
   #ifdef ATL_GAS_x8632
      movl      pfB, itmp
   #else
      prefB((pfB))
   #endif
   movaps       -128(pA0,lda), rC10
   movaps       rC10, rC11
   mulps        rA0, rC10
   #ifdef ATL_GAS_x8632
      prefB((itmp))
   #else
      add       $PFBINC, pfB
   #endif
   movaps       -128(pB0,ldb), rA0
   mulps        rA0, rC01
   #if KB > 4
      movaps    -112(pB0), rB0
   #endif
   mulps        rA0, rC11
   #ifdef ATL_GAS_x8632
      addl      $PFBINC, pfB
   #endif
   #if KB == 4
      #ifdef SCPLX
         movss  8(pC0), rA0             /* rA0 = c10 XXX XXX XXX */
         movhps (pC0), rA0              /* rA0 = c10 XXX c00 XXX */
         shufps $0xE2, rA0, rA0         /* rA0 = c00 c10 XXX XXX */
         movss  8(pC0,ldc), rA1         /* rA1 = c11 XXX XXX XXX */
         movhps (pC0,ldc), rA1          /* rA1 = c11 XXX c01 XXX */
         shufps $0x24, rA1, rA0         /* rA0 = c00 c10 c01 c11 */
      #else
         movlps (pC0), rA0
         movhps (pC0,ldc), rA0
      #endif
   #endif
@define i @8@
@define j @-112@
@iwhile i < 256
   #if KB > @(i)
      movaps    @(j)(pA0), rA0
      movaps    rA0, ra0
      mulps     rB0, rA0
      addps     rA0, rC00
      movaps    @(j)(pA0,lda), rA1
      mulps     rA1, rB0
      addps     rB0, rC10
      movaps    @(j)(pB0,ldb), rB0
      mulps     rB0, ra0
      mulps     rB0, rA1
         movaps @(j)+16(pB0), rB0
      addps      ra0, rC01
      addps      rA1, rC11
   #elif KB == @(i)
      movaps    @(j)(pA0), rA0
      movaps    rA0, ra0
      mulps     rB0, rA0
      addps     rA0, rC00
      movaps    @(j)(pA0,lda), rA1
      mulps     rA1, rB0
      addps     rB0, rC10
      movaps    @(j)(pB0,ldb), rB0
      mulps     rB0, ra0
      #ifndef BETA0
         #ifdef SCPLX
            movsd  8(pC0), rA0             /* rA0 = c10 XXX XXX XXX */
            movhps (pC0), rA0              /* rA0 = c10 XXX c00 XXX */
            shufps $0xE2, rA0, rA0         /* rA0 = c00 c10 XXX XXX */
         #else
            movlps (pC0), rA0
         #endif
      #endif
      mulps     rB0, rA1
      addps      ra0, rC01
      #ifndef BETA0
         #ifdef SCPLX
            movsd  8(pC0,ldc), rB0         /* rB0 = c11 XXX XXX XXX */
            movhps (pC0,ldc), rB0          /* rB0 = c11 XXX c01 XXX */
            shufps $0x24, rB0, rA0         /* rA0 = c00 c11 c01 c11 */
         #else
            movhps (pC0,ldc), rA0
         #endif
      #endif
      addps      rA1, rC11
   #endif
   @iexp i 4 @(i) +
   @iexp j 16 @(j) +
@endiwhile
@undef i
@undef j
#ifndef ATL_SSE3
   #error "This kernel presently requires SSE3!"
                                /* rC00 = c00a    c00b    c00c    c00d    */
                                /* rC10 = c10a    c10b    c10c    c10d    */
                                /* rC01 = c01a    c01b    c01c    c01d    */
                                /* rC11 = c11a    c11b    c11c    c11d    */
   movaps       rC00, rB0       /* rB0  = c00a    c00b    c00c    c00d    */
   movlhps      rC10, rC00      /* rC00 = c00a    c00b    c10a    c10b    */
   shufps $0xBE, rC10, rB0      /* rB0  = c00c    c00d    c10c    c10d    */
   addps        rB0, rC00       /* rC00 = c00ac   c00bd   c10ac   c10bd   */

   movaps       rC01, rA1       /* rA1  = c01a    c01b    c01c    c01d    */
   movlhps      rC11, rC01      /* rC01 = c01a    c01b    c11a    c11b    */
   shufps $0xBE, rC11, rA1      /* rA1  = c01c    c01d    c11c    c11d    */
   addps        rA1, rC01       /* rC01 = c01ac   c01bd   c11ac   c11bd   */

   movaps       rC00, rB0       /* rB0  = c00ac   c00bd   c10ac   c10bd   */
   unpcklps     rC01, rC00      /* rC00 = c00ac   c01ac   c00bd   c01bd   */
   movaps       rC00, rC10      /* rC10 = c00ac   c01ac   c00bd   c01bd   */
   unpckhps     rB0, rC01       /* rC01 = c11ac   c10ac   c11bd   c10bd   */
   movaps       rC01, rC11      /* rC11 = c11ac   c10ac   c11bd   c10bd   */
#else
                                /* rC00 = c00a    c00b    c00c    c00d    */
                                /* rC10 = c10a    c10b    c10c    c10d    */
                                /* rC01 = c01a    c01b    c01c    c01d    */
                                /* rC11 = c11a    c11b    c11c    c11d    */
   #ifdef ATL_GAS_x8632
      movl pfA, itmp
   #endif
   haddps       rC10, rC00      /* rC00 = c00ab   c00cd   c01ab   c01cd   */
   #ifdef ATL_GAS_x8632
      pref2((itmp))
   #else
      pref2((pfA))
   #endif
   haddps       rC11, rC01      /* rC01 = c10ab   c10cd   c11ab   c11cd   */
      addq      $PFAINC, pfA
   haddps       rC01, rC00      /* rC00 = c00abcd c01abcd  c10abcd c11abcd*/
#endif
   #ifndef BETA0
      #ifdef BETAX
         mulps  BETA, rA0
      #endif
      addps     rA0, rC00
   #endif
   add  $2*CMUL(4), pC0
   lea  (pA0, lda, 2), pA0
   subq $2, MM
   #ifdef SCPLX
      movhlps rC00, rC01
      movss  rC00, -16(pC0)
      psrldq    $4, rC00
      movss  rC01, -16(pC0,ldc)
      psrldq    $4, rC01
      movss  rC00, -8(pC0)
      movss  rC01, -8(pC0,ldc)
   #else
      movlpd rC00, -8(pC0)
      movhpd rC00, -8(pC0,ldc)
   #endif
   jnz  MNLOOP

   subq incAn, pA0
   addq incCn, pC0
   lea  (pB0, ldb, 2), pB0
   subq $2, NN
   #ifdef ATL_GAS_x8632
      movl    MM0, itmp
      movl    itmp, MM
   #else
      movq   MM0, MM
   #endif
   jnz MNLOOP
@callproc DONE
@ROUT ATL_dmm4x2x256_avx
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2011
#include "atlas_asm.h"
#if !defined(ATL_GAS_x8664)
   #error "This kernel requires x86-64 assembly!"
#endif
#ifndef ATL_AVX
   #error "This routine requires AVX!"
#endif

#if !defined(KB) || (KB == 0)
   #error "KB must be a compile-time constant!"
#endif
#if ((KB/4)*4 != KB)
   #error "KB must be a multiple of 4!"
#endif
#if KB > 256
   #error "KB can at most be 256!"
#endif

#ifdef DCPLX
   #define CMUL(arg_) 2*arg_
#else
   #define CMUL(arg_) arg_
#endif
/*
 *Integer register usage shown by these defines
 */
#define pA0     %rcx
#define lda     %rbx
#define lda3    %rbp
#define pfA     %rdi
#define pB0     %rax
#define ldb     %rsi
#define pfB     %rdx
#define incAn   %r8
#define incCn   %r9
#define pC0     %r10
#define MM      %r11
#define NN      %r12
#define MM0     %r13
#define ldc     %r14

#define rA0 	%ymm0
   #define xA0  %xmm0
#define rA1 	%ymm1
   #define xA1  %xmm1
#define rA2 	%ymm2
   #define xA2  %xmm2
#define rA3 	%ymm3
   #define xA3  %xmm3
#define rB0 	%ymm4
#define rB1	%ymm5
#define m0 	%ymm6
#define rC00 	%ymm7
#define rC10 	%ymm8
#define rC20 	%ymm9
#define rC30 	%ymm10
#define rC01 	%ymm11
#define rC11 	%ymm12
#define rC21 	%ymm13
#define rC31 	%ymm14
#define rbeta   %ymm15
   #define xbeta   %xmm15

/*
 * Save some inst space by using short version of instructions
 */
#define movapd movaps
#define movupd movups
#define movlpd movlps
#define movhpd movhps

/*
                      %rdi/4       %rsi/8       %rdx/12          %xmm0/16
 void ATL_USERMM(const int M, const int N, const int K, const TYPE alpha,
                       %rcx/24         %r8/28         %r9/32           8/36
                 const TYPE *A, const int lda, const TYPE *B, const int ldb,
                        %xmm1/40    16/48          24/52
                 const TYPE beta, TYPE *C, const int ldc)
*/
#define MOVCPD movapd
        .text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 *      Save callee-saved iregs
 */
        movq    %rbp, -8(%rsp)
        movq    %rbx, -16(%rsp)
        movq    %r12, -24(%rsp)
        movq    %r13, -32(%rsp)
        movq    %r14, -40(%rsp)
                                        prefetcht0 (pA0)
/*        movq    %r15, -48(%rsp) */

/*
 *      Setup input parameters
 */
   #ifdef BETAX
        unpcklpd %xmm1, %xmm1
        vinsertf128 $0x01, %xmm1, %ymm1, rbeta
           /* dest, src1, src2, imm8 --> imm8, src2, src1, dest */
   #endif
        movq    %rdi, MM0
        movq    %rsi, NN
        movq    %r8, lda
                                        prefetcht0      (pA0,lda)
        movq    %r9, pB0
                                        prefetcht0      (pB0)
        movslq  8(%rsp), ldb
                                        prefetcht0      (pA0,lda,2)
        movq    16(%rsp), pC0
        movslq  24(%rsp), incCn
	movq	incCn, ldc
                                        prefetcht0      KB*8(pA0,lda,2)
/*
 *      incCn = (2*ldc-M)*sizeof
 */
	shl	$1, incCn
        sub     MM0, incCn
#ifdef DCPLX
        shl     $4, incCn
        shl	$4, ldc
#else
        shl     $3, incCn
        shl	$3, ldc
#endif
/*
 *      pA0 += 128; pB0 += 128
 */
        sub     $-128, pA0
        sub     $-128, pB0
                                        prefetcht0      -64(pB0)
/*
 *      lda = lda*sizeof;  lda3 = lda*3
 */
        shl     $3, lda
                                                prefetcht0      (pB0)
        lea     (lda,lda,2), lda3
/*
 *      ldb = ldb*sizeof
 */
        shl     $3, ldb
                                                prefetcht0      64(pB0)
/*
 *      pfA = A + lda*M ; incAn = lda*M
 */
        movq    lda, pfA
                                                prefetcht0      128(pB0)
        imulq   MM0, pfA
        movq    pfA, incAn
        lea     -128(pA0, pfA), pfA
        movq    MM0, MM
        lea     -128(pB0,ldb,2), pfB
#ifdef DCPLX00
        test    $15, ldc
        jnz UMNLOOP
        test    $15, pC0
        jnz     UMNLOOP
#endif
ALIGN16
MNLOOP:
/* MLOOP: */
/* KLOOP begin */
      vmovapd -128(pB0), rB0
      vmovapd -128(pA0), rA0
      vmulpd  rA0, rB0, rC00
      vmovapd -128(pA0,lda), rA1
      vmulpd  rA1, rB0, rC10
      vmovapd -128(pA0,lda,2), rA2
      vmulpd  rA2, rB0, rC20
      vmovapd -128(pA0,lda3), rA3
      vmulpd  rA3, rB0, rC30
      vmovapd -128(pB0,ldb), rB1

      #if KB == 4
         vmulpd  rA0, rB1, rC01
            #if defined(DCPLX) && !defined(BETA0)
               movlpd  32(pC0), xA0
            #elif !defined(BETA0)
               vmovupd (pC0), rA0
            #endif
         vmulpd  rA1, rB1, rC11
            #if defined(DCPLX) && !defined(BETA0)
               movhpd  48(pC0), xA0                   /* XXX XXX c30 c20 */
            #endif
         vmulpd  rA2, rB1, rC21
            #if defined(DCPLX) && !defined(BETA0)
               movlpd  32(pC0,ldc), xA1
            #elif !defined(BETA0)
               vmovupd (pC0,ldc), rA1
            #endif
         vmulpd  rA3, rB1, rC31
            #if defined(DCPLX) && !defined(BETA0)
               movhpd  48(pC0,ldc), xA1               /* XXX XXX c31, c21 */
            #endif

      #else
         vmulpd  rA0, rB1, rC01
            vmovapd -96(pB0), rB0
         vmulpd  rA1, rB1, rC11
            vmovapd -96(pA0), rA0
         vmulpd  rA2, rB1, rC21
            vmovapd -96(pA0,lda), rA1
         vmulpd  rA3, rB1, rC31
            vmovapd -96(pA0,lda,2), rA2
      #endif

   #if KB == 8
      vmulpd  rA0, rB0, m0
      vaddpd  rC00, m0, rC00
      vmovapd -96(pA0,lda3), rA3
      vmulpd  rA1, rB0, m0
      vaddpd  rC10, m0, rC10
      vmovapd -96(pB0,ldb), rB1
      vmulpd  rA2, rB0, m0
      vaddpd  rC20, m0, rC20
        prefetcht0      (pfB)
      vmulpd  rA3, rB0, m0
      vaddpd  rC30, m0, rC30

      vmulpd  rA0, rB1, m0
      vaddpd  rC01, m0, rC01
      #if defined(DCPLX) && !defined(BETA0)
         movlpd  32(pC0), xA0
      #elif !defined(BETA0)
         vmovupd (pC0), rA0
      #endif
      vmulpd  rA1, rB1, m0
      vaddpd  rC11, m0, rC11
      #if defined(DCPLX) && !defined(BETA0)
         movhpd  48(pC0), xA0                   /* XXX XXX c30 c20 */
      #endif
      vmulpd  rA2, rB1, m0
      vaddpd  rC21, m0, rC21
      #if defined(DCPLX) && !defined(BETA0)
         movlpd  32(pC0,ldc), xA1
      #elif !defined(BETA0)
         vmovupd (pC0,ldc), rA1
      #endif
      vmulpd  rA3, rB1, m0
      vaddpd  rC31, m0, rC31
      #if defined(DCPLX) && !defined(BETA0)
         movhpd  48(pC0,ldc), xA1               /* XXX XXX c31, c21 */
      #endif
   #elif KB > 8
      vmulpd  rA0, rB0, m0
      vaddpd  rC00, m0, rC00
      vmovapd -96(pA0,lda3), rA3
      vmulpd  rA1, rB0, m0
      vaddpd  rC10, m0, rC10
      vmovapd -96(pB0,ldb), rB1
      vmulpd  rA2, rB0, m0
      vaddpd  rC20, m0, rC20
        prefetcht0      (pfB)
      vmulpd  rA3, rB0, m0
      vaddpd  rC30, m0, rC30
         vmovapd -64(pB0), rB0

      vmulpd  rA0, rB1, m0
      vaddpd  rC01, m0, rC01
         vmovapd -64(pA0), rA0
      vmulpd  rA1, rB1, m0
      vaddpd  rC11, m0, rC11
         vmovapd -64(pA0,lda), rA1
      vmulpd  rA2, rB1, m0
      vaddpd  rC21, m0, rC21
         vmovapd -64(pA0,lda,2), rA2
      vmulpd  rA3, rB1, m0
      vaddpd  rC31, m0, rC31
         vmovapd -64(pA0,lda3), rA3
   #endif

   #if KB == 12
      vmulpd  rA0, rB0, m0
      vaddpd  rC00, m0, rC00
        add     $64, pfB
      vmovapd -64(pB0,ldb), rB1
      vmulpd  rA1, rB0, m0
      vaddpd  rC10, m0, rC10
          prefetcht0      (pfA)
      vmulpd  rA2, rB0, m0
      vaddpd  rC20, m0, rC20
          add     $64, pfA
      vmulpd  rA3, rB0, m0
      vaddpd  rC30, m0, rC30

      vmulpd  rA0, rB1, m0
      vaddpd  rC01, m0, rC01
      #if defined(DCPLX) && !defined(BETA0)
         movlpd  32(pC0), xA0
      #elif !defined(BETA0)
         vmovupd (pC0), rA0
      #endif
      vmulpd  rA1, rB1, m0
      vaddpd  rC11, m0, rC11
      #if defined(DCPLX) && !defined(BETA0)
         movhpd  48(pC0), xA0                   /* XXX XXX c30 c20 */
      #elif !defined(BETA0)
         vmovupd (pC0, ldc), rA1
      #endif
      vmulpd  rA2, rB1, m0
      vaddpd  rC21, m0, rC21
      #if defined(DCPLX) && !defined(BETA0)
         movlpd  32(pC0,ldc), xA1
      #endif
      vmulpd  rA3, rB1, m0
      vaddpd  rC31, m0, rC31
      #if defined(DCPLX) && !defined(BETA0)
         movhpd  48(pC0,ldc), xA1               /* XXX XXX c31, c21 */
      #endif
   #elif KB > 12
      vmulpd  rA0, rB0, m0
      vaddpd  rC00, m0, rC00
        add     $64, pfB
      vmovapd -64(pB0,ldb), rB1
      vmulpd  rA1, rB0, m0
      vaddpd  rC10, m0, rC10
          prefetcht0      (pfA)
      vmulpd  rA2, rB0, m0
      vaddpd  rC20, m0, rC20
          add     $64, pfA
      vmulpd  rA3, rB0, m0
      vaddpd  rC30, m0, rC30
         vmovapd -32(pB0), rB0

      vmulpd  rA0, rB1, m0
      vaddpd  rC01, m0, rC01
         vmovapd -32(pA0), rA0
      vmulpd  rA1, rB1, m0
      vaddpd  rC11, m0, rC11
         vmovapd -32(pA0,lda), rA1
      vmulpd  rA2, rB1, m0
      vaddpd  rC21, m0, rC21
         vmovapd -32(pA0,lda,2), rA2
      vmulpd  rA3, rB1, m0
      vaddpd  rC31, m0, rC31
         vmovapd -32(pA0,lda3), rA3
   #endif

@iexp k 16 0 +
@iwhile k < 256
   @iexp j @(k) -20 +
   @iexp j @(j) 8 *
   #if KB == @(k)
      vmulpd  rA0, rB0, m0
      vaddpd  rC00, m0, rC00
      vmovapd @(j)(pB0,ldb), rB1
      vmulpd  rA1, rB0, m0
      vaddpd  rC10, m0, rC10
      vmulpd  rA2, rB0, m0
      vaddpd  rC20, m0, rC20
      vmulpd  rA3, rB0, m0
      vaddpd  rC30, m0, rC30

      vmulpd  rA0, rB1, m0
      vaddpd  rC01, m0, rC01
      #if defined(DCPLX) && !defined(BETA0)
         movlpd  32(pC0), xA0
      #elif !defined(BETA0)
         vmovupd (pC0), rA0
      #endif
      vmulpd  rA1, rB1, m0
      vaddpd  rC11, m0, rC11
      #if defined(DCPLX) && !defined(BETA0)
         movhpd  48(pC0), xA0                   /* XXX XXX c30 c20 */
      #elif !defined(BETA0)
         vmovupd (pC0,ldc), rA1
      #endif
      vmulpd  rA2, rB1, m0
      vaddpd  rC21, m0, rC21
      #if defined(DCPLX) && !defined(BETA0)
         movlpd  32(pC0,ldc), xA1
      #endif
      vmulpd  rA3, rB1, m0
      vaddpd  rC31, m0, rC31
      #if defined(DCPLX) && !defined(BETA0)
         movhpd  48(pC0,ldc), xA1               /* XXX XXX c31, c21 */
      #endif
   #elif KB > @(k)
      vmulpd  rA0, rB0, m0
      vaddpd  rC00, m0, rC00
      vmovapd @(j)(pB0,ldb), rB1
   @iexp j @(j) 32 +
      vmulpd  rA1, rB0, m0
      vaddpd  rC10, m0, rC10
      vmulpd  rA2, rB0, m0
      vaddpd  rC20, m0, rC20
      vmulpd  rA3, rB0, m0
      vaddpd  rC30, m0, rC30
         vmovapd @(j)(pB0), rB0

      vmulpd  rA0, rB1, m0
      vaddpd  rC01, m0, rC01
         vmovapd @(j)(pA0), rA0
      vmulpd  rA1, rB1, m0
      vaddpd  rC11, m0, rC11
         vmovapd @(j)(pA0,lda), rA1
      vmulpd  rA2, rB1, m0
      vaddpd  rC21, m0, rC21
         vmovapd @(j)(pA0,lda,2), rA2
      vmulpd  rA3, rB1, m0
      vaddpd  rC31, m0, rC31
         vmovapd @(j)(pA0,lda3), rA3
   #endif
   @iexp k @(k) 4 +
@endiwhile
       /* dest, src1, src2, imm8 --> imm8, src2, src1, dest */
/*
 *  Reduce Cvecs and, if BETA != 0 add to actual C, then store
 */
    vhaddpd rC10, rC00, rC00                    /* c10cd  c00cd c10ab c00ab */
    #if defined(DCPLX) && (defined(BETA1) || defined(BETAX))
       movlpd (pC0), xA2                        /* XXX XXX XXX c00 */
    #endif
    vhaddpd rC30, rC20, rC20                    /* c30cd  c20cd c30ab c20ab */
    #if defined(DCPLX) && (defined(BETA1) || defined(BETAX))
       movhpd 16(pC0), xA2                      /* XXX XXX c10 c00 */
    #endif
    #if defined(BETAX) && !defined(DCPLX)
       vmulpd rA0, rbeta, rA0
    #endif
    vperm2f128 $0x20, rC20, rC00, rC10          /* c30ab  c20ab  c10ab c00ab */
    #if defined(DCPLX) && (defined(BETA1) || defined(BETAX))
       vperm2f128 $0x20, rA0, rA2, rA0
    #endif
    vhaddpd rC11, rC01, rC01
    #if defined(DCPLX) && (defined(BETA1) || defined(BETAX))
       movlpd (pC0,ldc), xA3                    /* XXX XXX XXX c01 */
    #endif
    #if defined(BETAX) && !defined(DCPLX)
       vmulpd rA1, rbeta, rA1
    #endif
    vperm2f128 $0x31, rC20, rC00, rC00          /* c30cd  c20cd  c10cd c00cd */
    vhaddpd rC31, rC21, rC21
    #if defined(DCPLX) && (defined(BETA1) || defined(BETAX))
       movhpd 16(pC0,ldc), xA3                  /* XXX XXX c11 c01 */
    #endif
    vperm2f128 $0x20, rC21, rC01, rC11          /* c31ab  c21ab  c11ab c01ab */
    vaddpd rC00, rC10, rC00                     /* c30    c20    c10   c00 */
    #if defined(DCPLX) && (defined(BETA1) || defined(BETAX))
       vperm2f128 $0x20, rA1, rA3, rA1
    #endif
    vperm2f128 $0x31, rC21, rC01, rC01          /* c31cd  c21cd  c11cd c01cd */
    vaddpd rC11, rC01, rC01                     /* c31    c21    c11   c01 */
    #if defined(BETAX) && defined(DCPLX)
       vmulpd rA0, rbeta, rA0
    #endif

    #ifndef BETA0
       vaddpd rC00, rA0, rC00
       #if defined(BETAX) && defined(DCPLX)
          vmulpd rA1, rbeta, rA1
       #endif
       vaddpd rC01, rA1, rC01
    #endif
    #ifdef DCPLX
       movsd %xmm7, (pC0)                       /* xmm7 is rC00 */
       movhpd %xmm7, 16(pC0)
       vextractf128 $1, rC00, %xmm7
       movsd %xmm11, (pC0,ldc)                  /* xmm11 is rC01 */
       movhpd %xmm11, 16(pC0,ldc)
       vextractf128 $1, rC01, %xmm11
       movsd %xmm7, 32(pC0)
       movhpd %xmm7, 48(pC0)
       movsd %xmm11, 32(pC0,ldc)
       movhpd %xmm11, 48(pC0,ldc)
    #else
       vmovupd rC00, (pC0)
       vmovupd rC01, (pC0,ldc)
    #endif
/* KLOOP end */
        add     $4*CMUL(8), pC0

        lea     0(pA0,lda,4), pA0
        sub     $4, MM
        jnz     MNLOOP

        movq    MM0, MM
        sub     incAn, pA0
        add     incCn, pC0
	lea	(pB0, ldb, 2), pB0
        sub     $2, NN
        jnz     MNLOOP

/* DONE: */
        movq    -8(%rsp), %rbp
        movq    -16(%rsp), %rbx
        movq    -24(%rsp), %r12
        movq    -32(%rsp), %r13
        movq    -40(%rsp), %r14
/*        movq    -48(%rsp), %r15  */
        ret
@ROUT ATL_smm4x2x256_avx
@extract -b @(topd)/gen.inc what=crsetup
@extract -b @(topd)/cw.inc lang=c -define cwdate 2011
#include "atlas_asm.h"
#if !defined(ATL_GAS_x8664)
   #error "This kernel requires x86-64 assembly!"
#endif
#ifndef ATL_AVX
   #error "This routine requires AVX!"
#endif


#if !defined(KB) || (KB == 0)
   #error "KB must be a compile-time constant!"
#endif
#if ((KB/8)*8 != KB)
   #error "KB must be a multiple of 8!"
#endif
#if KB > 256
   #error "KB can at most be 256!"
#endif

#ifdef SCPLX
   #define CMUL(arg_) 2*arg_
#else
   #define CMUL(arg_) arg_
#endif
/*
 *Integer register usage shown by these defines
 */
#define pA0     %rcx
#define lda     %rbx
#define lda3    %rbp
#define pfA     %rdi
#define pB0     %rax
#define ldb     %rsi
#define pfB     %rdx
#define incAn   %r8
#define incCn   %r9
#define pC0     %r10
#define MM      %r11
#define NN      %r12
#define MM0     %r13
#define ldc     %r14

#define rA0 	%ymm0
   #define xA0  %xmm0
#define rA1 	%ymm1
   #define xA1  %xmm1
#define rA2 	%ymm2
   #define xA2  %xmm2
#define rA3 	%ymm3
   #define xA3  %xmm3
#define rB0 	%ymm4
   #define xB0  %xmm4
#define rB1	%ymm5
   #define xB1  %xmm5
#define m0 	%ymm6
   #define xm0  %xmm6
#define rC00 	%ymm7
   #define xC00	%xmm7
#define rC10 	%ymm8
   #define xC10	%xmm8
#define rC20 	%ymm9
   #define xC20	%xmm9
#define rC30 	%ymm10
   #define xC30	%xmm10
#define rC01 	%ymm11
   #define xC01	%xmm11
#define rC11 	%ymm12
   #define xC11	%xmm12
#define rC21 	%ymm13
   #define xC21	%xmm13
#define rC31 	%ymm14
   #define xC31	%xmm14
#define rbeta   %ymm15
   #define xbeta   %xmm15

/*
                      %rdi/4       %rsi/8       %rdx/12          %xmm0/16
 void ATL_USERMM(const int M, const int N, const int K, const TYPE alpha,
                       %rcx/24         %r8/28         %r9/32           8/36
                 const TYPE *A, const int lda, const TYPE *B, const int ldb,
                        %xmm1/40    16/48          24/52
                 const TYPE beta, TYPE *C, const int ldc)
*/
#define MOVCPD movapd
        .text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 *      Save callee-saved iregs
 */
        movq    %rbp, -8(%rsp)
        movq    %rbx, -16(%rsp)
        movq    %r12, -24(%rsp)
        movq    %r13, -32(%rsp)
        movq    %r14, -40(%rsp)
                                        prefetcht0 (pA0)
/*        movq    %r15, -48(%rsp) */

/*
 *      Setup input parameters
 */
   #ifdef BETAX
        pshufd $0, %xmm1, %xmm1
        vinsertf128 $0x01, %xmm1, %ymm1, rbeta
           /* dest, src1, src2, imm8 --> imm8, src2, src1, dest */
   #endif
        movq    %rdi, MM0
        movq    %rsi, NN
        movq    %r8, lda
                                        prefetcht0      (pA0,lda)
        movq    %r9, pB0
                                        prefetcht0      (pB0)
        movslq  8(%rsp), ldb
                                        prefetcht0      (pA0,lda,2)
        movq    16(%rsp), pC0
        movslq  24(%rsp), incCn
	movq	incCn, ldc
                                        prefetcht0      KB*8(pA0,lda,2)
/*
 *      incCn = (2*ldc-M)*sizeof
 */
	shl	$1, incCn
        sub     MM0, incCn
#ifdef SCPLX
        shl     $3, incCn
        shl	$3, ldc
#else
        shl     $2, incCn
        shl	$2, ldc
#endif
/*
 *      pA0 += 128; pB0 += 128
 */
        sub     $-128, pA0
        sub     $-128, pB0
                                        prefetcht0      -64(pB0)
/*
 *      lda = lda*sizeof;  lda3 = lda*3
 */
        shl     $2, lda
                                                prefetcht0      (pB0)
        lea     (lda,lda,2), lda3
/*
 *      ldb = ldb*sizeof
 */
        shl     $2, ldb
                                                prefetcht0      64(pB0)
/*
 *      pfA = A + lda*M ; incAn = lda*M
 */
        movq    lda, pfA
                                                prefetcht0      128(pB0)
        imulq   MM0, pfA
        movq    pfA, incAn
        lea     -128(pA0, pfA), pfA
        movq    MM0, MM
        lea     -128(pB0,ldb,2), pfB
ALIGN16
MNLOOP:
/* MLOOP: */
/* KLOOP begin */
      vmovaps -128(pB0), rB0
      vmovaps -128(pA0), rA0
      vmulps  rA0, rB0, rC00
      vmovaps -128(pA0,lda), rA1
      vmulps  rA1, rB0, rC10
      vmovaps -128(pA0,lda,2), rA2
      vmulps  rA2, rB0, rC20
      vmovaps -128(pA0,lda3), rA3
      vmulps  rA3, rB0, rC30
      vmovaps -128(pB0,ldb), rB1

      #if KB == 8
         vmulps  rA0, rB1, rC01
         vmulps  rA1, rB1, rC11
         vmulps  rA2, rB1, rC21
         vmulps  rA3, rB1, rC31
      #else
         vmulps  rA0, rB1, rC01
            vmovaps -96(pB0), rB0
         vmulps  rA1, rB1, rC11
            vmovaps -96(pA0), rA0
         vmulps  rA2, rB1, rC21
            vmovaps -96(pA0,lda), rA1
         vmulps  rA3, rB1, rC31
            vmovaps -96(pA0,lda,2), rA2
      #endif

   #if KB == 16
      vmulps  rA0, rB0, m0
      vaddps  rC00, m0, rC00
      vmovaps -96(pA0,lda3), rA3
      vmulps  rA1, rB0, m0
      vaddps  rC10, m0, rC10
      vmovaps -96(pB0,ldb), rB1
      vmulps  rA2, rB0, m0
      vaddps  rC20, m0, rC20
        prefetcht0      (pfB)
      vmulps  rA3, rB0, m0
      vaddps  rC30, m0, rC30

      vmulps  rA0, rB1, m0
      vaddps  rC01, m0, rC01
      vmulps  rA1, rB1, m0
      vaddps  rC11, m0, rC11
      vmulps  rA2, rB1, m0
      vaddps  rC21, m0, rC21
      vmulps  rA3, rB1, m0
      vaddps  rC31, m0, rC31
   #elif KB > 16
      vmulps  rA0, rB0, m0
      vaddps  rC00, m0, rC00
      vmovaps -96(pA0,lda3), rA3
      vmulps  rA1, rB0, m0
      vaddps  rC10, m0, rC10
      vmovaps -96(pB0,ldb), rB1
      vmulps  rA2, rB0, m0
      vaddps  rC20, m0, rC20
        prefetcht0      (pfB)
      vmulps  rA3, rB0, m0
      vaddps  rC30, m0, rC30
         vmovaps -64(pB0), rB0

      vmulps  rA0, rB1, m0
      vaddps  rC01, m0, rC01
         vmovaps -64(pA0), rA0
      vmulps  rA1, rB1, m0
      vaddps  rC11, m0, rC11
         vmovaps -64(pA0,lda), rA1
      vmulps  rA2, rB1, m0
      vaddps  rC21, m0, rC21
         vmovaps -64(pA0,lda,2), rA2
      vmulps  rA3, rB1, m0
      vaddps  rC31, m0, rC31
         vmovaps -64(pA0,lda3), rA3
   #endif

   #if KB == 24
      vmulps  rA0, rB0, m0
      vaddps  rC00, m0, rC00
        add     $64, pfB
      vmovaps -64(pB0,ldb), rB1
      vmulps  rA1, rB0, m0
      vaddps  rC10, m0, rC10
          prefetcht0      (pfA)
      vmulps  rA2, rB0, m0
      vaddps  rC20, m0, rC20
          add     $64, pfA
      vmulps  rA3, rB0, m0
      vaddps  rC30, m0, rC30

      vmulps  rA0, rB1, m0
      vaddps  rC01, m0, rC01
      vmulps  rA1, rB1, m0
      vaddps  rC11, m0, rC11
      vmulps  rA2, rB1, m0
      vaddps  rC21, m0, rC21
      vmulps  rA3, rB1, m0
      vaddps  rC31, m0, rC31
   #elif KB > 24
      vmulps  rA0, rB0, m0
      vaddps  rC00, m0, rC00
        add     $64, pfB
      vmovaps -64(pB0,ldb), rB1
      vmulps  rA1, rB0, m0
      vaddps  rC10, m0, rC10
          prefetcht0      (pfA)
      vmulps  rA2, rB0, m0
      vaddps  rC20, m0, rC20
          add     $64, pfA
      vmulps  rA3, rB0, m0
      vaddps  rC30, m0, rC30
         vmovaps -32(pB0), rB0

      vmulps  rA0, rB1, m0
      vaddps  rC01, m0, rC01
         vmovaps -32(pA0), rA0
      vmulps  rA1, rB1, m0
      vaddps  rC11, m0, rC11
         vmovaps -32(pA0,lda), rA1
      vmulps  rA2, rB1, m0
      vaddps  rC21, m0, rC21
         vmovaps -32(pA0,lda,2), rA2
      vmulps  rA3, rB1, m0
      vaddps  rC31, m0, rC31
         vmovaps -32(pA0,lda3), rA3
   #endif

@iexp k 32 0 +
@iwhile k < 256
   @iexp j @(k) -40 +
   @iexp j @(j) 4 *
   #if KB == @(k)
      vmulps  rA0, rB0, m0
      vaddps  rC00, m0, rC00
      vmovaps @(j)(pB0,ldb), rB1
      vmulps  rA1, rB0, m0
      vaddps  rC10, m0, rC10
      vmulps  rA2, rB0, m0
      vaddps  rC20, m0, rC20
      vmulps  rA3, rB0, m0
      vaddps  rC30, m0, rC30

      vmulps  rA0, rB1, m0
      vaddps  rC01, m0, rC01
      vmulps  rA1, rB1, m0
      vaddps  rC11, m0, rC11
      vmulps  rA2, rB1, m0
      vaddps  rC21, m0, rC21
      vmulps  rA3, rB1, m0
      vaddps  rC31, m0, rC31
   #elif KB > @(k)
      vmulps  rA0, rB0, m0
      vaddps  rC00, m0, rC00
      vmovaps @(j)(pB0,ldb), rB1
   @iexp j @(j) 32 +
      vmulps  rA1, rB0, m0
      vaddps  rC10, m0, rC10
      vmulps  rA2, rB0, m0
      vaddps  rC20, m0, rC20
      vmulps  rA3, rB0, m0
      vaddps  rC30, m0, rC30
         vmovaps @(j)(pB0), rB0

      vmulps  rA0, rB1, m0
      vaddps  rC01, m0, rC01
         vmovaps @(j)(pA0), rA0
      vmulps  rA1, rB1, m0
      vaddps  rC11, m0, rC11
         vmovaps @(j)(pA0,lda), rA1
      vmulps  rA2, rB1, m0
      vaddps  rC21, m0, rC21
         vmovaps @(j)(pA0,lda,2), rA2
      vmulps  rA3, rB1, m0
      vaddps  rC31, m0, rC31
         vmovaps @(j)(pA0,lda3), rA3
   #endif
   @iexp k @(k) 8 +
@endiwhile
/* KLOOP end */
       /* dest, src1, src2, imm8 --> imm8, src2, src1, dest */
/*
 *  Reduce Cvecs and, if BETA != 0 add to actual C, then store
 */
        #if defined(SCPLX) && !defined(BETA0)
                          /* MEM = SEG c30 XXX c20 XXX c10 XXX c00 */
           movss  24(pC0), xA2                  /*   0   0   0 c30 */
           movhps 16(pC0), xA2                  /* XXX c20   0 c30 */
           movups (pC0), xA0                    /* XXX c10 XXX c00 */
           shufps $0x28, xA2, xA0               /* c30 c20 c10 c00 */

           movss  24(pC0,ldc), xA3              /*   0   0   0 c31 */
           movhps 16(pC0,ldc), xA3              /* XXX c21   0 c31 */
           movups (pC0,ldc), xA1                /* XXX c11 XXX c01 */
           shufps $0x28, xA3, xA1               /* c31 c21 c11 c01 */
        #elif !defined(BETA0)
            movups (pC0), xA0                   /* c30 c20 c10 c00 */
            movups (pC0,ldc), xA1               /* c31 c21 c11 c01 */
        #endif
                    /* rC00 */  /* c00h c00g c00f c00e c00d c00c c00b c00a */
                    /* rC10 */  /* c10h c10g c10f c10e c10d c10c c10b c10a */
                    /* rC20 */  /* c20h c20g c20f c20e c20d c20c c20b c20a */
                    /* rC30 */  /* c30h c30g c30f c30e c30d c30c c30b c30a */
        vhaddps rC10, rC00, rC00  
        #ifdef BETAX
           mulps xbeta, xA0
        #endif
        vhaddps rC11, rC01, rC01  
        #ifdef BETAX
           mulps xbeta, xA1
        #endif
           /* c10gh  c10ef  c00gh  c00ef  c10cd  c10ab  c00cd  c00ab */
        vhaddps rC30, rC20, rC20
        vhaddps rC31, rC21, rC21  
           /* c30gh  c30ef  c20gh  c20ef  c30cd  c30ab  c20cd  c20ab */
        vhaddps rC20, rC00, rC00
        vhaddps rC21, rC01, rC01
           /* c30e-h c20e-h c10e-h c00e-h c30a-d c20a-d c10a-d c00a-b */
        vextractf128 $1, rC00, xB0   /* c30e-h c20e-h c10e-h c00e-h */
        addps xB0, xC00              /* c30a-h c20a-h c10a-h c00a-h */
        vextractf128 $1, rC01, xB1   /* c31e-h c21e-h c11e-h c01e-h */
        addps xB1, xC01              /* c31a-h c21a-h c11a-h c01a-h */
/*
 *      Add in original C if BETA != 0
 */
        #ifndef BETA0
           addps xA0, xC00
           addps xA1, xC01
        #endif
/*
 *      Store answer to mem
 */
        #ifdef SCPLX
                          /* MEM = SEG c30 XXX c20 XXX c10 XXX c00 */
           movss xC00, (pC0)
           psrldq $4, xC00
           movss xC00, 8(pC0)
           psrldq $4, xC00
           movss xC00, 16(pC0)
           psrldq $4, xC00
           movss xC00, 24(pC0)

           movss xC01, (pC0,ldc)
           psrldq $4, xC01
           movss xC01, 8(pC0,ldc)
           psrldq $4, xC01
           movss xC01, 16(pC0,ldc)
           psrldq $4, xC01
           movss xC01, 24(pC0,ldc)
        #else
            movups xC00, (pC0)
            movups xC01, (pC0,ldc)
        #endif
       
        add     $4*CMUL(4), pC0

        lea     0(pA0,lda,4), pA0
        sub     $4, MM
        jnz     MNLOOP

        movq    MM0, MM
        sub     incAn, pA0
        add     incCn, pC0
	lea	(pB0, ldb, 2), pB0
        sub     $2, NN
        jnz     MNLOOP

/* DONE: */
        movq    -8(%rsp), %rbp
        movq    -16(%rsp), %rbx
        movq    -24(%rsp), %r12
        movq    -32(%rsp), %r13
        movq    -40(%rsp), %r14
/*        movq    -48(%rsp), %r15  */
        ret
@ROUT ATL_smm2x4x256_fma4 ATL_smm2x2x256_fma4
  @define sh @2@
  @define sz @4@
  @define shp1 @3@
  @define cplx @SCPLX@
@ROUT ATL_dmm2x4x256_fma4 ATL_dmm2x2x256_fma4
  @define sh @3@
  @define sz @8@
  @define shp1 @4@
  @define cplx @DCPLX@
@ROUT ATL_dmm2x4x256_fma4 ATL_smm2x4x256_fma4 ATL_dmm2x2x256_fma4 ATL_smm2x2x256_fma4
#include "atlas_asm.h"
#ifndef ATL_AVXFMA4
   #error "This kernel requires AMD's 4-operand FMAC (FM4)"
#endif
#ifdef @(cplx)
   #define CMUL(arg_) 2*arg_
#else
   #define CMUL(arg_) arg_
#endif
@ROUT ATL_dmm2x2x256_fma4 ATL_smm2x2x256_fma4
#define prefC(mem) prefetchw  mem
#define prefA(mem) prefetch   mem
#define prefB(mem) prefetch   mem
/*
 * Stack locations and frame size
 */
#define pAOFF 16
#define iCOFF 20
#define JOFF  24
#define MOFF  28
#define M0OFF 32
#define LDCOFF 36
#define BETOFF FSIZE+40
#define FSIZE 40
/*
 * Integer register usage shown by these defines
 */
#ifdef ATL_GAS_x8632
   #define pA0     %ecx
   #define lda     %ebx
   #define pB0     %eax
   #define pC0     %esi
   #define ldb     %edi
   #define pfA     %edx
   #define ldc     %ebp

   #define pA00    pAOFF(%esp)
   #define incCn   iCOFF(%esp)
   #define NN      JOFF(%esp)
   #define MM      MOFF(%esp)
   #define MM0     M0OFF(%esp)
#else
   #error "This kernel Requires IA32 assembler!"
   #define pA0     %rcx
   #define lda     %rbx
//   #define lda3    %rbp
//   #define pAE     %rdi
   #define pB0     %rax
   #define pC0     %rsi
//   #define pBE     %rdx
   #define incAn   %r8
   #define incCn   %r9
   #define ldb     %r10
   #define MM      %r11
   #define NN      %r12
   #define pfA     %r13
   #define MM0     %r14
#endif

#define rA0     %xmm0
#define rA1     %xmm1
#define rB0     %xmm2
#define rB1     %xmm3
#define rC00    %xmm4
#define rC10    %xmm5
#define rC01    %xmm6
#define rC11    %xmm7

#if defined(DCPLX) || defined(SCPLX)
   #define TCPLX 1
#endif
/*
 * Save some inst space by using short version of instructions
 */
#define movapd movaps
#define movupd movups
#define movlpd movlps
#define movhpd movhps
#define MOVCPD movups

/*
                      %rdi/4       %rsi/8       %rdx/12          %xmm0/16
 void ATL_USERMM(const int M, const int N, const int K, const TYPE alpha,
                       %rcx/24         %r8/28         %r9/32           8/36
                 const TYPE *A, const int lda, const TYPE *B, const int ldb,
                        %xmm1/40    16/48          24/52
                 const TYPE beta, TYPE *C, const int ldc)
*/
        .text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
#ifdef ATL_GAS_x8632
/*
 * Save callee-saved registers
 */
   subl $FSIZE, %esp
   movl %ebx, 0(%esp)
   movl %ebp, 4(%esp)
   movl %esi, 8(%esp)
   movl %edi, 12(%esp)


/* 
 * Load values from stack, and save some back.
 */
   movl FSIZE+24(%esp), pA0
   movl FSIZE+28(%esp), lda
   shl  $@(sh), lda
   movl FSIZE+36(%esp), ldb
   shl  $@(sh), ldb
   movl FSIZE+4(%esp), pfA
   movl pfA, MM0
   movl pfA, MM
   movl FSIZE+52(%esp), pfA           /* pfA = ldc */
   movl pfA, ldc
   #ifdef TCPLX
      shl $@(sh)+1, ldc              /* ldc *= sizeof */
   #else
      shl $@(sh), ldc                /* ldc *= sizeof */
   #endif
   movl ldc, LDCOFF(%esp)
   add pfA, pfA                 /* pfA = 2*ldc */
   sub MM, pfA                  /* pfA = 2*ldc - M */
   #ifdef TCPLX
      shl $@(sh)+1, pfA              /* pfA = (2*ldc-M)*sizeof */
   #else
      shl $@(sh), pfA                /* pfA = (2*ldc-M)*sizeof */
   #endif
   movl pfA, incCn              /* incCn = (2*ldc-M)*sizeof */
   movl FSIZE+8(%esp), pfA
   movl pfA, NN          /* JOFF(%esp) = N */
   movl FSIZE+32(%esp), pB0
   movl FSIZE+48(%esp), pC0
   lea NBNB*@(sz)(pA0), pfA
#endif
/*
 * pA0 += 128; pB0 += 128
 */
   sub $-128, pA0
   movl pA0, pAOFF(%esp)     /* save for restoration at end of N-loop */
   sub $-128, pB0
ALIGN16
   MNLOOP:
@ROUT ATL_dmm2x2x256_fma4
/*
 *       K=0,1, with rCxx starting at zero
 */
         movapd -128(pB0), rB0
         xorpd  rC11, rC11
         movapd -128(pA0), rA0
         vfmaddpd rC11, rA0, rB0, rC00
         movapd -128(pB0,ldb), rB1
         vfmaddpd rC11, rA0, rB1, rC01
         movapd -128(pA0,lda), rA1
         vfmaddpd rC11, rA1, rB0, rC10
         #if KB > 2
            movapd -112(pB0), rB0
         #elif KB == 2 && !defined(BETA0)
            #ifdef TREAL
               MOVCPD (pC0), rB0
            #else
               movlpd (pC0),rB0
               movhpd CMUL(8)(pC0),rB0
            #endif
         #endif
         vfmaddpd rC11, rA1, rB1, rC11
         #if KB > 2
            movapd -112(pA0), rA0
         #elif KB == 2 && !defined(BETA0)
            #ifdef TREAL
               MOVCPD (pC0,ldc), rB0
            #else
               movlpd (pC0,ldc),rA0
               movhpd CMUL(8)(pC0,ldc),rA0
            #endif
         #endif
/*
 *    K=2,3
 */
         #if KB > 2
            vfmaddpd rC00, rA0, rB0, rC00
            movapd -112(pA0,lda), rA1
            vfmaddpd rC10, rA1, rB0, rC10
            movapd -112(pB0,ldb), rB1
            vfmaddpd rC01, rA0, rB1, rC01
            #if KB > 4
               movapd -96(pB0), rB0
            #elif KB == 4 && !defined(BETA0)
               #ifdef TREAL
                  MOVCPD (pC0), rB0
               #else
                  movlpd (pC0),rB0
                  movhpd CMUL(8)(pC0),rB0
               #endif
            #endif
            vfmaddpd rC11, rA1, rB1, rC11
            #if KB > 4
               movapd -96(pA0), rA0
            #elif KB == 4 && !defined(BETA0)
               #ifdef TREAL
                  MOVCPD (pC0,ldc), rB0
               #else
                  movlpd (pC0,ldc),rA0
                  movhpd CMUL(8)(pC0,ldc),rA0
               #endif
            #endif
         #endif
   @define N @256@
   @iexp off -96 0 +
   @iexp i 4 0 +
   @iwhile i < @(N)
         #if KB > @(i)
      @iexp i @(i) 2 +
            vfmaddpd rC00, rA0, rB0, rC00
            movapd @(off)(pA0,lda), rA1
            vfmaddpd rC10, rA1, rB0, rC10
            movapd @(off)(pB0,ldb), rB1
      @iexp off @(off) 16 +
            vfmaddpd rC01, rA0, rB1, rC01
            #if KB > @(i)
               movapd @(off)(pB0), rB0
            #elif KB == @(i) && !defined(BETA0)
               #ifdef TREAL
                  MOVCPD (pC0), rB0
               #else
                  movlpd (pC0),rB0
                  movhpd CMUL(8)(pC0),rB0
               #endif
            #endif
            vfmaddpd rC11, rA1, rB1, rC11
            #if KB > @(i)
               movapd @(off)(pA0), rA0
            #elif KB == @(i) && !defined(BETA0)
               #ifdef TREAL
                  MOVCPD (pC0,ldc), rB0
               #else
                  movlpd (pC0,ldc),rA0
                  movhpd CMUL(8)(pC0,ldc),rA0
               #endif
            #endif
         #endif
   @endiwhile

/*
 *    K-loop finished, sum up vectors, write to C
 */
         #ifdef BETAX
            movddup BETOFF(%esp), rA1
         #endif
         haddpd rC10, rC00
         prefA((pfA))
         #ifdef BETAX
               mulpd rA1, rB0
         #endif
         haddpd rC11, rC01
         #ifdef BETAX
            mulpd rA1, rA0
         #endif
         #ifndef BETA0
            addpd rB0, rC00
            addpd rA0, rC01
         #endif
         add $64, pfA
         #ifndef TCPLX
            MOVCPD rC00, (pC0)
            MOVCPD rC01, (pC0,ldc)
         #else
            movlpd rC00, (pC0)
            movhpd rC00, 16(pC0)
            movlpd rC01, (pC0,ldc)
            movhpd rC01, 16(pC0,ldc)
         #endif

         lea (pA0, lda,2), pA0
         add $2*CMUL(8), pC0
      sub $2, MM
      jnz MNLOOP

      lea (pB0,ldb,2), pB0
      movl MM0, pA0
      prefetcht0 (pB0)
      movl pA0, MM
      prefetcht0 (pB0,ldb)
      movl pAOFF(%esp), pA0
      add incCn, pC0
   sub $2, NN
   jnz MNLOOP

/* DONE: */
   movl 0(%esp), %ebx
   movl 4(%esp), %ebp
   movl 8(%esp), %esi
   movl 12(%esp), %edi
   addl $FSIZE, %esp
   ret

@ROUT ATL_dmm2x4x256_fma4 ATL_smm2x4x256_fma4 
/*
 *Integer register usage shown by these defines
 */
#define pA0     %rcx
#define lda     %rbx
#define ldb3    %rbp
#define pfA     %rdi
#define pB0     %rax
#define ldb     %rsi
#define pfB     %rdx
#define incAn   %r8
#define incCn   %r9
#define pC0     %r10
#define MM      %r11
#define NN      %r12
#define MM0     %r13
#define ldc     %r14
#define ldc3    %r15

#define rA0 	%xmm0
#define rA1 	%xmm1
#define ra0 	%xmm2
#define ra1 	%xmm3
#define rB0 	%xmm4
#define rB1 	%xmm5
#define rB2 	%xmm6
#define rB3 	%xmm7
#define rC00 	%xmm8
#define rC10 	%xmm9
#define rC01 	%xmm10
#define rC11 	%xmm11
#define rC02 	%xmm12
#define rC12 	%xmm13
#define rC03 	%xmm14
#define rC13 	%xmm15

/*
 * Save some inst space by using short version of instructions
 */
#define movapd movaps
#define movupd movups
#define movlpd movlps
#define movhpd movhps

/*
                      %rdi/4       %rsi/8       %rdx/12          %xmm0/16
 void ATL_USERMM(const int M, const int N, const int K, const TYPE alpha,
                       %rcx/24         %r8/28         %r9/32           8/36
                 const TYPE *A, const int lda, const TYPE *B, const int ldb,
                        %xmm1/40    16/48          24/52
                 const TYPE beta, TYPE *C, const int ldc)
*/
#define MOVCPD movupd
        .text
.global ATL_asmdecor(ATL_USERMM)
ALIGN16
ATL_asmdecor(ATL_USERMM):
/*
 *      Save callee-saved iregs
 */
        movq    %rbp, -8(%rsp)
        movq    %rbx, -16(%rsp)
        movq    %r12, -24(%rsp)
        movq    %r13, -32(%rsp)
        movq    %r14, -40(%rsp)
                                        prefetcht0 (pA0)
        movq    %r15, -48(%rsp)

/*
 *      Setup input parameters
 */
   #ifdef BETAX
      #define BETAOFF -72
@ROUT ATL_smm2x4x256_fma4
        movlhps %xmm1, %xmm1
        unpcklps %xmm1, %xmm1
        movaps  %xmm1, BETAOFF(%rsp)
@ROUT ATL_dmm2x4x256_fma4
        unpcklpd        %xmm1, %xmm1
        movapd  %xmm1, BETAOFF(%rsp)
@ROUT ATL_dmm2x4x256_fma4 ATL_smm2x4x256_fma4
   #endif
        movq    %rdi, MM0
        movq    %rsi, NN
        movq    %r8, lda
                                        prefetcht0      (pA0,lda)
        movq    %r9, pB0
                                        prefetcht0      (pB0)
        movslq  8(%rsp), ldb
                                        prefetcht0      (pB0,ldb)
        movq    16(%rsp), pC0
        movslq  24(%rsp), incCn
	movq	incCn, ldc
                                        prefetcht0      (pB0,lda,2)
/*
 *      incCn = (4*ldc-M)*sizeof
 */
	shl	$2, incCn
        sub     MM0, incCn
#ifdef @(cplx)
        shl     $@(shp1), incCn
        shl	$@(shp1), ldc
#else
        shl     $@(sh), incCn
        shl	$@(sh), ldc
#endif
/*
 *      pA0 += 128; pB0 += 128
 */
        sub     $-128, pA0
        sub     $-128, pB0
                                        prefetcht0      -64(pB0)
/*
 *      ldb = ldb*sizeof;  ldb3 = ldb*3;   ldc3 = ldc*3
 */
        shl     $@(sh), ldb
                                                prefetcht0      (pB0)
        lea     (ldb,ldb,2), ldb3
        lea     (ldc,ldc,2), ldc3
/*
 *      lda = lda*sizeof
 */
        shl     $@(sh), lda
                                                prefetcht0      64(pB0)
/*
 *      pfA = A + lda*M ; incAn = lda*M
 */
        movq    lda, pfA
                                                prefetcht0      128(pB0)
        imulq   MM0, pfA
        movq    pfA, incAn
        lea     -128(pA0, pfA), pfA
        movq    MM0, MM
        lea     -128(pB0,ldb,2), pfB
ALIGN16
   MNLOOP:
@ROUT ATL_smm2x4x256_fma4
/*
 *       K=0-3, with rCxx starting at zero
 */
         movaps -128(pB0), rB0
         xorpd  rC13, rC13
         movaps -128(pA0), rA0
         vfmaddps rC13, rA0, rB0, rC00
         movaps -128(pB0,ldb), rB1
         vfmaddps rC13, rA0, rB1, rC01
         movaps -128(pB0,ldb,2), rB2
         vfmaddps rC13, rA0, rB2, rC02
         movaps -128(pB0,ldb3), rB3
         vfmaddps rC13, rA0, rB3, rC03
         movaps -128(pA0,lda), rA1
         vfmaddps rC13, rA1, rB0, rC10
         #if KB > 4
            movaps -112(pA0), ra0
         #endif
         vfmaddps rC13, rA1, rB1, rC11
         #if KB > 4
            movaps -112(pA0,lda), ra1
         #endif
         vfmaddps rC13, rA1, rB2, rC12
         #if KB > 4
            movaps -112(pB0), rB0
         #endif
         vfmaddps rC13, rA1, rB3, rC13
         #if KB > 4
            movaps -112(pB0,ldb), rB1
         #endif
/*
 *       K=4-7
 */
        #if KB > 4
           vfmaddps rC00, ra0, rB0, rC00
           movaps -112(pB0,ldb,2), rB2
           vfmaddps rC10, ra1, rB0, rC10
           movaps -112(pB0,ldb3), rB3
           vfmaddps rC01, ra0, rB1, rC01
           #if KB > 8
              movaps -96(pA0), rA0
           #endif
           vfmaddps rC11, ra1, rB1, rC11
           #if KB > 8
              movaps -96(pA0,lda), rA1
           #endif
           vfmaddps rC02, ra0, rB2, rC02
           #if KB > 8
              movaps -96(pB0), rB0
           #endif
           vfmaddps rC12, ra1, rB2, rC12
           #if KB > 8
              movaps -96(pB0,ldb), rB1
           #endif
           vfmaddps rC03, ra0, rB3, rC03
           #if KB > 8
              movaps -96(pB0,ldb,2), rB2
           #endif
           vfmaddps rC13, ra1, rB3, rC13
           #if KB > 8
              movaps -96(pB0,ldb3), rB3
           #endif
        #endif
        #if KB <= 8
        #endif
   @define N @256@
   @iexp off -96 16 +
   @iexp i 8 0 +
   @iwhile i < @(N)
        #if KB > @(i)
      @iexp i @(i) 4 +
           vfmaddps rC00, rA0, rB0, rC00
           #if KB > @(i)
              movaps @(off)(pA0), ra0
           #endif
           vfmaddps rC10, rA1, rB0, rC10
           #if KB > @(i)
              movaps @(off)(pA0,lda), ra1
           #endif
           vfmaddps rC01, rA0, rB1, rC01
           #if KB > @(i)
              movaps @(off)(pB0), rB0
           #elif KB == @(i) && defined(BETA1)
           #endif
           vfmaddps rC11, rA1, rB1, rC11
           #if KB > @(i)
              movaps @(off)(pB0,ldb), rB1
           #elif KB == @(i) && defined(BETA1)
   @iif i ! 12
           #endif
           vfmaddps rC02, rA0, rB2, rC02
           #if KB == @(i) && defined(BETA0)
   @endiif
   @iif i = 12
           #endif
           vfmaddps rC02, rA0, rB2, rC02
           #if KB == @(i)
              prefetcht0 (pfB)
              add $64, pfB
   @endiif
           #endif
           vfmaddps rC12, rA1, rB2, rC12
           #if KB > @(i)
              movaps @(off)(pB0,ldb,2), rB2
           #elif KB == @(i) && defined(BETA1)
           #endif
           vfmaddps rC03, rA0, rB3, rC03
           #if KB == @(i) && defined(BETA1)
           #endif
           vfmaddps rC13, rA1, rB3, rC13
           #if KB > @(i)
              movaps @(off)(pB0,ldb3), rB3
           #elif KB == @(i) && defined(BETA0)
           #endif
        #endif
      @iexp off @(off) 16 +
        #if KB > @(i)
      @iexp i @(i) 4 +
           vfmaddps rC00, ra0, rB0, rC00
           #if KB > @(i)
              movaps @(off)(pA0), rA0
           #endif
           vfmaddps rC10, ra1, rB0, rC10
           #if KB > @(i)
              movaps @(off)(pA0,lda), rA1
           #endif
           vfmaddps rC01, ra0, rB1, rC01
           #if KB > @(i)
              movaps @(off)(pB0), rB0
           #elif KB == @(i) && defined(BETA1)
           #endif
           vfmaddps rC11, ra1, rB1, rC11
           #if KB > @(i)
              movaps @(off)(pB0,ldb), rB1
   @iif i = 16
           #elif KB == @(i) && defined(BETA1)
           #endif
           vfmaddps rC02, ra0, rB2, rC02
           #if KB == @(i)
              prefetcht0 (pfB)
              add $64, pfB
   @endiif
   @iif i ! 16
           #elif KB == @(i) && defined(BETA1)
           #endif
           vfmaddps rC02, ra0, rB2, rC02
           #if KB == @(i) && defined(BETA1)
   @endiif
           #endif
           vfmaddps rC12, ra1, rB2, rC12
           #if KB > @(i)
              movaps @(off)(pB0,ldb,2), rB2
           #elif KB == @(i) && defined(BETA1)
           #endif
           vfmaddps rC03, ra0, rB3, rC03
           #if KB == @(i) && defined(BETA1)
           #endif
           vfmaddps rC13, ra1, rB3, rC13
           #if KB > @(i)
              movaps @(off)(pB0,ldb3), rB3
           #elif KB == @(i) && defined(BETA1)
           #endif
        #endif
      @iexp off @(off) 16 +
   @endiwhile
/*
 *       Add in original C if necessary
 */
         #ifdef BETA1
            addss (pC0), rC00
            addss CMUL(4)(pC0), rC10
            addss (pC0,ldc), rC01
            addss CMUL(4)(pC0,ldc), rC11
            addss (pC0,ldc,2), rC02
            addss CMUL(4)(pC0,ldc,2), rC12
            addss (pC0,ldc3), rC03
            addss CMUL(4)(pC0,ldc3), rC13
         #elif defined(BETAX)
            movaps BETAOFF(%rsp), rA0
            xorps ra0, ra0
            vfmaddss ra0, (pC0), rA0, rB0
            vfmaddss ra0, CMUL(4)(pC0), rA0, rB1
            unpcklps rB1, rB0                   /* rB0={0,0,c10,c00} */
            vfmaddss ra0, (pC0,ldc), rA0, rB2
            vfmaddss ra0, CMUL(4)(pC0,ldc), rA0, rB3
            unpcklps rB3, rB2                   /* rB2={0,0,c11,c01} */
            movlhps  rB2, rB0                   /* rB0={c11,c01,c10,c00} */
            vfmaddss ra0, (pC0,ldc,2), rA0, rB1
            vfmaddss ra0, CMUL(4)(pC0,ldc,2), rA0, rB3
            unpcklps rB3, rB1                   /* rB1={0,0,c12,c02} */
            vfmaddss ra0, (pC0,ldc3), rA0, rA1
            vfmaddss ra0, CMUL(4)(pC0,ldc3), rA0, ra1
            unpcklps ra1, rA1                   /* rA1={0,0,c13,c03} */
            movlhps  rA1, rB1                   /* rB1={c13,c03,c12,c02} */
         #endif
/*
 *       K-loop finished, sum up vectors
 */
         prefetcht1 (pfA)
         haddps rC10, rC00    /* rC00 = {c10cd, c10ab, c00cd, c00ab} */
         add $64, pfA
         haddps rC11, rC01    /* rC01 = {c11cd, c11ab, c01cd, c01ab} */
         haddps rC12, rC02    /* rC02 = {c12cd, c12ab, c02cd, c02ab} */
         haddps rC13, rC03    /* rC03 = {c13cd, c13ab, c03cd, c03ab} */
         haddps rC01, rC00    /* rC00 = {c11a-d, c01a-d, c10a-d, c00a-d} */
         haddps rC03, rC02    /* rC02 = {c13a-d, c03a-d, c12a-d, c02a-d} */
         #ifdef BETAX
            addps rB0, rC00
            addps rB1, rC02
         #endif

                                        /* rC00 = {c11, c01, c10, c00} */
                                        /* rC02 = {c13, c03, c12, c02} */
         movss rC00, (pC0)
         pshufd $0xB1, rC00, rC01       /* rC01 = {C01, c11, c00, c10} */
         movss rC02, (pC0,ldc,2)
         pshufd $0xB1, rC02, rC03       /* rC03 = {C03, c13, c02, c12} */
         movss rC01, CMUL(4)(pC0)
         movhlps rC00, rC00             /* rC00 = {c11, c01, c11, c01} */
         movss rC03, CMUL(4)(pC0,ldc,2)
         movhlps rC02, rC02             /* rC02 = {c13, c03, c13, c03} */
         movss rC00, (pC0,ldc)
         movhlps rC01, rC01             /* rC01 = {c01, c11, c01, c11} */
         movss rC02, (pC0,ldc3)
         movhlps rC03, rC03             /* rC03 = {c03, c13, c03, c13} */
         movss rC01, CMUL(4)(pC0,ldc)
         movss rC03, CMUL(4)(pC0,ldc3)
   @define eltsz @4@
@ROUT ATL_dmm2x4x256_fma4
/*
 *       K=0,1, with rCxx starting at zero
 */
         movapd -128(pB0), rB0
         xorpd  rC13, rC13
         movapd -128(pA0), rA0
         vfmaddpd rC13, rA0, rB0, rC00
         movapd -128(pB0,ldb), rB1
         vfmaddpd rC13, rA0, rB1, rC01
         movapd -128(pB0,ldb,2), rB2
         vfmaddpd rC13, rA0, rB2, rC02
         movapd -128(pB0,ldb3), rB3
         vfmaddpd rC13, rA0, rB3, rC03
         movapd -128(pA0,lda), rA1
         vfmaddpd rC13, rA1, rB0, rC10
         #if KB > 2
            movapd -112(pA0), ra0
         #endif
         vfmaddpd rC13, rA1, rB1, rC11
         #if KB > 2
            movapd -112(pA0,lda), ra1
         #endif
         vfmaddpd rC13, rA1, rB2, rC12
         #if KB > 2
            movapd -112(pB0), rB0
         #endif
         vfmaddpd rC13, rA1, rB3, rC13
         #if KB > 2
            movapd -112(pB0,ldb), rB1
         #endif
/*
 *       K=2,3
 */
        #if KB > 2
           vfmaddpd rC00, ra0, rB0, rC00
           movapd -112(pB0,ldb,2), rB2
           vfmaddpd rC10, ra1, rB0, rC10
           movapd -112(pB0,ldb3), rB3
           vfmaddpd rC01, ra0, rB1, rC01
           #if KB > 4
              movapd -96(pA0), rA0
           #endif
           vfmaddpd rC11, ra1, rB1, rC11
           #if KB > 4
              movapd -96(pA0,lda), rA1
           #endif
           vfmaddpd rC02, ra0, rB2, rC02
           #if KB > 4
              movapd -96(pB0), rB0
           #endif
           vfmaddpd rC12, ra1, rB2, rC12
           #if KB > 4
              movapd -96(pB0,ldb), rB1
           #endif
           vfmaddpd rC03, ra0, rB3, rC03
           #if KB > 4
              movapd -96(pB0,ldb,2), rB2
           #endif
           vfmaddpd rC13, ra1, rB3, rC13
           #if KB > 4
              movapd -96(pB0,ldb3), rB3
           #endif
        #endif
        #if KB <= 4
            movlpd (pC0),rB0
            movhpd CMUL(8)(pC0),rB0
            movlpd (pC0,ldc),rB1
            movhpd CMUL(8)(pC0,ldc),rB1
            movlpd (pC0,ldc,2),rB2
            movhpd CMUL(8)(pC0,ldc,2),rB2
            movlpd (pC0,ldc3),rB3
            movhpd CMUL(8)(pC0,ldc3),rB3
        #endif
   @define N @256@
   @iexp off -96 16 +
   @iexp i 4 0 +
   @iwhile i < @(N)
        #if KB > @(i)
      @iexp i @(i) 2 +
           vfmaddpd rC00, rA0, rB0, rC00
           #if KB > @(i)
              movapd @(off)(pA0), ra0
           #endif
           vfmaddpd rC10, rA1, rB0, rC10
           #if KB > @(i)
              movapd @(off)(pA0,lda), ra1
           #endif
           vfmaddpd rC01, rA0, rB1, rC01
           #if KB > @(i)
              movapd @(off)(pB0), rB0
           #elif KB == @(i) && !defined(BETA0)
               movlpd (pC0),rB0
               movhpd CMUL(8)(pC0),rB0
           #endif
           vfmaddpd rC11, rA1, rB1, rC11
           #if KB > @(i)
              movapd @(off)(pB0,ldb), rB1
           #elif KB == @(i) && !defined(BETA0)
   @iif i ! 6
               movlpd (pC0,ldc),rB1
           #endif
           vfmaddpd rC02, rA0, rB2, rC02
           #if KB == @(i) && !defined(BETA0)
               movhpd CMUL(8)(pC0,ldc),rB1
   @endiif
   @iif i = 6
               movlpd (pC0,ldc),rB1
               movhpd CMUL(8)(pC0,ldc),rB1
           #endif
           vfmaddpd rC02, rA0, rB2, rC02
           #if KB == @(i)
              prefetcht0 (pfB)
              add $64, pfB
   @endiif
           #endif
           vfmaddpd rC12, rA1, rB2, rC12
           #if KB > @(i)
              movapd @(off)(pB0,ldb,2), rB2
           #elif KB == @(i) && !defined(BETA0)
               movlpd (pC0,ldc,2),rB2
           #endif
           vfmaddpd rC03, rA0, rB3, rC03
           #if KB == @(i) && !defined(BETA0)
               movhpd CMUL(8)(pC0,ldc,2),rB2
           #endif
           vfmaddpd rC13, rA1, rB3, rC13
           #if KB > @(i)
              movapd @(off)(pB0,ldb3), rB3
           #elif KB == @(i) && !defined(BETA0)
               movlpd (pC0,ldc3),rB3
               movhpd CMUL(8)(pC0,ldc3),rB3
           #endif
        #endif
      @iexp off @(off) 16 +
        #if KB > @(i)
      @iexp i @(i) 2 +
           vfmaddpd rC00, ra0, rB0, rC00
           #if KB > @(i)
              movapd @(off)(pA0), rA0
           #endif
           vfmaddpd rC10, ra1, rB0, rC10
           #if KB > @(i)
              movapd @(off)(pA0,lda), rA1
           #endif
           vfmaddpd rC01, ra0, rB1, rC01
           #if KB > @(i)
              movapd @(off)(pB0), rB0
           #elif KB == @(i) && !defined(BETA0)
               movlpd (pC0),rB0
               movhpd CMUL(8)(pC0),rB0
           #endif
           vfmaddpd rC11, ra1, rB1, rC11
           #if KB > @(i)
              movapd @(off)(pB0,ldb), rB1
   @iif i = 8
           #elif KB == @(i) && !defined(BETA0)
               movlpd (pC0,ldc),rB1
               movhpd CMUL(8)(pC0,ldc),rB1
           #endif
           vfmaddpd rC02, ra0, rB2, rC02
           #if KB == @(i)
              prefetcht0 (pfB)
              add $64, pfB
   @endiif
   @iif i ! 8
           #elif KB == @(i) && !defined(BETA0)
               movlpd (pC0,ldc),rB1
           #endif
           vfmaddpd rC02, ra0, rB2, rC02
           #if KB == @(i) && !defined(BETA0)
               movhpd CMUL(8)(pC0,ldc),rB1
   @endiif
           #endif
           vfmaddpd rC12, ra1, rB2, rC12
           #if KB > @(i)
              movapd @(off)(pB0,ldb,2), rB2
           #elif KB == @(i) && !defined(BETA0)
               movlpd (pC0,ldc,2),rB2
           #endif
           vfmaddpd rC03, ra0, rB3, rC03
           #if KB == @(i) && !defined(BETA0)
               movhpd CMUL(8)(pC0,ldc,2),rB2
           #endif
           vfmaddpd rC13, ra1, rB3, rC13
           #if KB > @(i)
              movapd @(off)(pB0,ldb3), rB3
           #elif KB == @(i) && !defined(BETA0)
               movlpd (pC0,ldc3),rB3
               movhpd CMUL(8)(pC0,ldc3),rB3
           #endif
        #endif
      @iexp off @(off) 16 +
   @endiwhile

/*
 *       K-loop finished, sum up vectors
 */
         prefetcht1 (pfA)
         haddpd rC10, rC00
         add $64, pfA
         haddpd rC11, rC01
         haddpd rC12, rC02
         haddpd rC13, rC03
         #ifndef BETA0
@beginskip
/*
 *          This code assumes C is 16-byte aligned!
 */
            #ifdef BETAX
               movapd BETAOFF(%rsp), rB0
               vfmaddpd rC00, (pC0), rB0, rC00
               vfmaddpd rC01, (pC0,ldc), rB0, rC01
               vfmaddpd rC02, (pC0,ldc,2), rB0, rC02
               vfmaddpd rC03, (pC0,ldc3), rB0, rC03
            #else
               addpd (pC0), rC00
               addpd (pC0,ldc), rC01
               addpd (pC0,ldc,2), rC02
               addpd (pC0,ldc3), rC03
            #endif
@endskip
            #ifdef BETAX
               movapd BETAOFF(%rsp), rA0
               vfmaddpd rC00, rB0, rA0, rC00
               vfmaddpd rC01, rB1, rA0, rC01
               vfmaddpd rC02, rB2, rA0, rC02
               vfmaddpd rC03, rB3, rA0, rC03
            #else
               addpd rB0, rC00
               addpd rB1, rC01
               addpd rB2, rC02
               addpd rB3, rC03
            #endif
         #endif
         #ifndef DCPLX
            MOVCPD rC00, (pC0)
            MOVCPD rC01, (pC0,ldc)
            MOVCPD rC02, (pC0,ldc,2)
            MOVCPD rC03, (pC0,ldc3)
         #else
            movlpd rC00, (pC0)
            movhpd rC00, 16(pC0)
            movlpd rC01, (pC0,ldc)
            movhpd rC01, 16(pC0,ldc)
            movlpd rC02, (pC0,ldc,2)
            movhpd rC02, 16(pC0,ldc,2)
            movlpd rC03, (pC0,ldc3)
            movhpd rC03, 16(pC0,ldc3)
         #endif
   @define eltsz @8@
@ROUT ATL_dmm2x4x256_fma4 ATL_smm2x4x256_fma4

         lea (pA0,lda,2), pA0
         add $2*CMUL(@(eltsz)), pC0

      sub $2, MM
      jnz MNLOOP

      mov MM0, MM
      sub incAn, pA0
      add incCn, pC0
      lea (pB0, ldb, 4), pB0
   sub $4, NN
   jnz MNLOOP

/* DONE: */
        movq    -8(%rsp), %rbp
        movq    -16(%rsp), %rbx
        movq    -24(%rsp), %r12
        movq    -32(%rsp), %r13
        movq    -40(%rsp), %r14
        movq    -48(%rsp), %r15
        ret
@ROUT ATL_amm8xUxVL_simd
@ifdef ! nu
   @define nu @5@
@endifdef
@iif nu > 8
   @abort "NU (@(nu)) can be at most 8!"
@endiif
@iif nu < 2
   @abort "NU (@(nu)) must be at least 2!"
@endiif
/*
 * Automatically Tuned Linear Algebra Software v3.10.3
 * Copyright (C) 2016 R. Clint Whaley
 */
#if !defined(SREAL) && !defined(DREAL) && !defined(SCPLX) && !defined(DCPLX)
   #define DREAL 1
#endif
#include <stddef.h>
#include "atlas_simd.h"
#include "atlas_prefetch.h"
#if defined(SCPLX) || defined(DCPLX)
   #include "atlas_cplxsimd.h"
   #ifndef TCPLX
      #define TCPLX 1
   #endif
   #define SHIFT <<1
#else
   #define SHIFT
   #ifndef TCPLX
      #define TREAL 1
   #endif
#endif
#ifndef TYPE
   #if defined(SREAL) || defined(SCPLX)
      #define TYPE float
   #else
      #define TYPE double
   #endif
#endif
#ifndef ATL_MM_KB 
   #ifdef KB
      #if KB > 0
         #define ATL_KBCONST 1
         #define ATL_MM_KB KB
      #else
         #define ATL_KBCONST 0
         #define ATL_MM_KB K
      #endif
   #else
      #define ATL_KBCONST 0
      #define ATL_MM_KB K
   #endif
#else
   #if ATL_MM_KB > 0
      #define ATL_KBCONST 1
   #else
      #undef ATL_MM_KB
      #define ATL_MM_KB K
      #define ATL_KBCONST 0
   #endif
#endif
#ifdef BETA1
   #define ATL_vbeta(p_, d_) \
   { \
      ATL_vuld(rA0, p_); \
      ATL_vadd(d_, d_, rA0); \
      ATL_vust(p_, d_); \
   }
#elif defined(BETA0)
   #define ATL_vbeta(p_, d_) ATL_vust(p_, d_)
#else
   #define ATL_vbeta(p_, d_) \
   { \
      ATL_vuld(rA0, p_); \
      ATL_vmac(d_, rA0, vBE); \
      ATL_vust(p_, d_); \
   }
#endif

#ifdef SCPLX
   #ifdef BETA0
      #define wrtC(p_, r1_) \
      { \
         (p_)[ 0] = r1_[0]; \
         (p_)[ 2] = r1_[1]; \
         (p_)[ 4] = r1_[2]; \
         (p_)[ 6] = r1_[3]; \
      }
   #elif defined(BETA1)
      #define wrtC(p_, r1_) \
      { \
         (p_)[ 0] += r1_[0]; \
         (p_)[ 2] += r1_[1]; \
         (p_)[ 4] += r1_[2]; \
         (p_)[ 6] += r1_[3]; \
      }
   #else
      #define wrtC(p_, r1_) \
      { \
         (p_)[ 0] = beta*(p_)[ 0] + r1_[0]; \
         (p_)[ 2] = beta*(p_)[ 2] + r1_[1]; \
         (p_)[ 4] = beta*(p_)[ 4] + r1_[2]; \
         (p_)[ 6] = beta*(p_)[ 6] + r1_[3]; \
      }
   #endif
   #ifndef BETA0
      #define vwrtC(p_, rc_, rt_, rz_) \
      { \
         ATL_vunpckHI(rt_, rc_, rz_); /* rt_={0, r3, 0, r2} */ \
         ATL_vunpckLO(rc_, rc_, rz_); /* rc_={0, r1, 0, r0} */ \
         ATL_vbeta(p_, rc_); \
         ATL_vbeta((p_)+4, rt_); \
      }
    #endif
#elif defined(DCPLX)
   #ifdef BETA0
      #define wrtC(p_, r1_) \
      { \
         (p_)[ 0] = r1_[0]; \
         (p_)[ 2] = r1_[1]; \
      }
   #elif defined(BETA1)
      #define wrtC(p_, r1_) \
      { \
         (p_)[ 0] += r1_[0]; \
         (p_)[ 2] += r1_[1]; \
      }
   #else
      #define wrtC(p_, r1_) \
      { \
         (p_)[ 0] = beta*(p_)[ 0] + r1_[0]; \
         (p_)[ 2] = beta*(p_)[ 2] + r1_[1]; \
      }
   #endif
#endif                  
#ifndef ATL_RESTRICT
   #if defined(__STDC_VERSION__) && (__STDC_VERSION__/100 >= 1999)
      #define ATL_RESTRICT restrict
   #else
      #define ATL_RESTRICT
   #endif
#endif
void ATL_USERMM
   (const int M, const int N, const int K, const TYPE alpha,
    const TYPE * ATL_RESTRICT A, const int lda,
    const TYPE * ATL_RESTRICT B, const int ldb, const TYPE beta,
    TYPE * ATL_RESTRICT C, const int ldc)
/*
 * Performs a GEMM with M,N,K unrolling (& jam) of (8,6,VLEN).
 * Vectorization of VLEN=[4,8] (d,s) along K dim, vec unroll=(8,6,1).
 * You may set compile-time constant K dim by defining ATL_MM_KB.
 */
{
   @declare "   const TYPE " y n ";"
      *pB0=B
      @iexp j 2 0 +
      @iwhile j < nu
         @iexp k @(j) -2 +
         *pB@(j)=pB@(k)+(ldb<<1)
         @iexp j @(j) 2 +
      @endiwhile
      *aa=A
      *pA0=A *pA2=pA0+(lda<<1) *pA4=pA2+(lda<<1) *pA6=pA4+(lda<<1)
      *pfA=A+lda*M *pfB=B+ldb*N
   @enddeclare
   const size_t ldc2 = ldc SHIFT;
   TYPE *pC0=C;
   int i, j, k;
   #if !defined(BETA0) && !defined(BETA1) && !defined(TCPLX)
      ATL_VTYPE vBE;
   #elif ATL_VLEN == 4 && defined(TCPLX) && defined(BETAX)
      const ATL_VTYPE vBE={beta, 1.0, beta, 1.0};
   #endif
   #if ATL_KBCONST == 0
      const size_t incAm = (lda<<3), incBn = ldb*@(nu);
   #else
      #define incAm (ATL_MM_KB<<3)
      #define incBn (@(nu)*ATL_MM_KB)
   #endif
   const size_t incC=ldc2*@(nu);
   #if !defined(BETA0) && !defined(BETA1) && !defined(TCPLX)
      ATL_vbcast(vBE, &beta);
   #endif

   for (j=0; j < N; j += @(nu))
   {
      for (i=0; i < M; i += 8)
      {
         @declare "         register ATL_VTYPE " y n ";"
            @iexp j 0 0 +
            rA0 rA1 rA2 rA3 rA4 rA5 rA6 rA7
            @iwhile j < @(nu)
               rB@(j)
               rC0@(j) rC1@(j) rC2@(j) rC3@(j) rC4@(j) rC5@(j) rC6@(j) rC7@(j)
               @iexp j @(j) 1 +
            @endiwhile
         @enddeclare
         /* Peel K=0 iteration to avoid zero of rCxx and extra add  */
         ATL_vld(rB0, pB0);
         ATL_vld(rA0, pA0); 
         ATL_vmul(rC00, rA0, rB0);
         ATL_vld(rA1, pA0+lda); pA0 += ATL_VLEN;
         ATL_vmul(rC10, rA1, rB0);
         ATL_vld(rA2, pA2); 
         ATL_vmul(rC20, rA2, rB0);
         ATL_vld(rA3, pA2+lda); pA2 += ATL_VLEN;
         ATL_vmul(rC30, rA3, rB0);
         ATL_vld(rA4, pA4); 
         ATL_vmul(rC40, rA4, rB0);
         ATL_vld(rA5, pA4+lda); pA4 += ATL_VLEN;
         ATL_vmul(rC50, rA5, rB0);
         ATL_vld(rA6, pA6); 
         ATL_vmul(rC60, rA6, rB0);
         ATL_vld(rA7, pA6+lda); pA6 += ATL_VLEN;
         ATL_vmul(rC70, rA7, rB0);
   @iif nu = 1
            ATL_vld(rB0, pB0+ATL_VLEN); pB0 += (ATL_VLEN<<1);
   @endiif

   @iexp npf 8 @(nu) *
   @iif nu > 1
            ATL_vld(rB1, pB0+ldb); pB0 += ATL_VLEN;
            ATL_vld(rB0, pB0);
         ATL_vmul(rC01, rA0, rB1);
      @iif nu > 2
            ATL_vld(rB2, pB2); 
            @iif nu == 3
            pB2 += ATL_VLEN;
            @endiif
      @endiif
      @iif nu = 2
            ATL_vld(rA0, pA0);
      @endiif
         ATL_vmul(rC11, rA1, rB1);
      @iif nu > 3
            ATL_vld(rB3, pB2+ldb); pB2 += ATL_VLEN;
         @iif nu > 4
            ATL_vld(rB4, pB4);
            @iif nu == 5
            pB4 += ATL_VLEN;
            @endiif
         @endiif
      @endiif
      @iif nu = 2
            ATL_vld(rA1, pA0+lda); pA0 += ATL_VLEN;
      @endiif
         ATL_vmul(rC21, rA2, rB1);
      @iif nu = 2
            ATL_vld(rA2, pA2);
      @endiif
      @iif nu > 5
            ATL_vld(rB5, pB4+ldb);  pB4 += ATL_VLEN;
      @endiif
         ATL_vmul(rC31, rA3, rB1);
      @iif nu = 2
            ATL_vld(rA3, pA2+lda); pA2 += ATL_VLEN;
      @endiif
      @iif nu > 6
            ATL_vld(rB6, pB6);
         @iif nu == 7
            pB6 += ATL_VLEN;
         @endiif
      @endiif
         ATL_vmul(rC41, rA4, rB1);
      @iif nu > 7
            ATL_vld(rB7, pB6+ldb);  pB6 += ATL_VLEN;
      @endiif
      @iif nu = 2
            ATL_vld(rA4, pA4);
      @endiif
         ATL_vmul(rC51, rA5, rB1);
      @iif nu = 2
            ATL_vld(rA5, pA4+lda);  pA4 += ATL_VLEN;
      @endiif
            ATL_pfl1R(pfA);
            ATL_pfl1R(pfA+8);
         ATL_vmul(rC61, rA6, rB1);
      @iexp k 8 @(nu) *
      @iif nu = 2
            ATL_vld(rA6, pA6);
      @endiif
      @iif 16 < npf
            ATL_pfl1R(pfA+16);
            ATL_pfl1R(pfA+24);
      @endiif
         ATL_vmul(rC71, rA7, rB1);
      @iif nu = 2
            ATL_vld(rA7, pA6+lda);  pA6 += ATL_VLEN;
      @endiif
      @iif nu > 2
            ATL_vld(rB1, pB0+ldb);  pB0 += ATL_VLEN;
      @endiif
   @endiif

   @iif nu > 2
      @iexp j 2 0 +
      @iexp lst @(nu) -1 +
      @iwhile j < @(lst)
         ATL_vmul(rC0@(j), rA0, rB@(j));
         @iif j = 2
            ATL_pfl1R(pfB);
            ATL_pfl1R(pfB+8);
         @endiif
         ATL_vmul(rC1@(j), rA1, rB@(j));
         @iif @iexp @(j) 2 = @(npf) 16 < &
            ATL_pfl1R(pfB+16);
            ATL_pfl1R(pfB+24);
         @endiif
         ATL_vmul(rC2@(j), rA2, rB@(j));
         @iif @iexp @(j) 2 = @(npf) 32 < &
            ATL_pfl1R(pfB+32);
            ATL_pfl1R(pfB+40);
         @endiif
         ATL_vmul(rC3@(j), rA3, rB@(j));
         @iif @iexp @(j) 2 = @(npf) 48 < &
            ATL_pfl1R(pfB+48);
            ATL_pfl1R(pfB+56);
         @endiif
         ATL_vmul(rC4@(j), rA4, rB@(j));
         @iif @iexp @(j) 2 = @(npf) 32 < &
            ATL_pfl1R(pfA+32);
            ATL_pfl1R(pfA+40);
         @endiif
         ATL_vmul(rC5@(j), rA5, rB@(j));
         @iif @iexp @(j) 2 = @(npf) 48 < &
            ATL_pfl1R(pfA+48);
            ATL_pfl1R(pfA+56);
         @endiif
         ATL_vmul(rC6@(j), rA6, rB@(j));
         ATL_vmul(rC7@(j), rA7, rB@(j));
            @iif @iexp 2 @(j)%
               @iexp k @(j) -1 +
            ATL_vld(rB@(j), pB@(k)+lda); pB@(k) += ATL_VLEN;
            @endiif
            @iif @iexp 2 @(j) % 0 =
            ATL_vld(rB@(j), pB@(j));
            @endiif

         @iexp j @(j) 1 +
      @endiwhile
         ATL_vmul(rC0@(j), rA0, rB@(j));
            ATL_vld(rA0, pA0);
         ATL_vmul(rC1@(j), rA1, rB@(j));
            ATL_vld(rA1, pA0+lda); pA0 += ATL_VLEN;
         ATL_vmul(rC2@(j), rA2, rB@(j));
            ATL_vld(rA2, pA2);
         ATL_vmul(rC3@(j), rA3, rB@(j));
            ATL_vld(rA3, pA2+lda); pA2 += ATL_VLEN;
         ATL_vmul(rC4@(j), rA4, rB@(j));
            ATL_vld(rA4, pA4);
         ATL_vmul(rC5@(j), rA5, rB@(j));
            ATL_vld(rA5, pA4+lda); pA4 += ATL_VLEN;
         ATL_vmul(rC6@(j), rA6, rB@(j));
            ATL_vld(rA6, pA6);
         ATL_vmul(rC7@(j), rA7, rB@(j));
            ATL_vld(rA7, pA6+lda); pA6 += ATL_VLEN;
   @endiif
         pfA += @(npf); pfB += @(npf);
/*
 *       Stop one iteration early to drain preload pipline of A/B
 */
         for (k=(ATL_VLEN<<1); k < ATL_MM_KB; k += ATL_VLEN)
         {
            ATL_vmac(rC00, rA0, rB0);
      @iexp k @(nu) -1 +
      @iif @iexp 2 @(k) %
         @iexp j @(k) -1 +
            ATL_vld(rB@(k), pB@(j)+ldb);  pB@(j) += ATL_VLEN;
      @endiif
      @iif @iexp 2 @(k) % 0 =
            ATL_vld(rB@(k), pB@(k));  pB@(k) += ATL_VLEN;
      @endiif
            ATL_vmac(rC10, rA1, rB0);
            ATL_vmac(rC20, rA2, rB0);
            ATL_vmac(rC30, rA3, rB0);
            ATL_vmac(rC40, rA4, rB0);
            ATL_vmac(rC50, rA5, rB0);
            ATL_vmac(rC60, rA6, rB0);
            ATL_vmac(rC70, rA7, rB0);
               ATL_vld(rB0, pB0);

      @iexp j 1 0 +
      @iexp lst @(nu) -1 +
      @iwhile j < @(lst)
            ATL_vmac(rC0@(j), rA0, rB@(j));
            ATL_vmac(rC1@(j), rA1, rB@(j));
            ATL_vmac(rC2@(j), rA2, rB@(j));
            ATL_vmac(rC3@(j), rA3, rB@(j));
            ATL_vmac(rC4@(j), rA4, rB@(j));
            ATL_vmac(rC5@(j), rA5, rB@(j));
            ATL_vmac(rC6@(j), rA6, rB@(j));
            ATL_vmac(rC7@(j), rA7, rB@(j));
         @iif @iexp 2 @(j) %
            @iexp k @(j) -1 +
               ATL_vld(rB@(j), pB@(k)+ldb); pB@(k) += ATL_VLEN;
         @endiif
         @iif @iexp 2 @(j) % 0 =
               ATL_vld(rB@(j), pB@(j));
         @endiif

         @iexp j @(j) 1 +
      @endiwhile

            ATL_vmac(rC0@(j), rA0, rB@(j));
               ATL_vld(rA0, pA0);
            ATL_vmac(rC1@(j), rA1, rB@(j));
               ATL_vld(rA1, pA0+lda); pA0 += ATL_VLEN;
            ATL_vmac(rC2@(j), rA2, rB@(j));
               ATL_vld(rA2, pA2); 
            ATL_vmac(rC3@(j), rA3, rB@(j));
               ATL_vld(rA3, pA2+lda); pA2 += ATL_VLEN;
            ATL_vmac(rC4@(j), rA4, rB@(j));
               ATL_vld(rA4, pA4); 
            ATL_vmac(rC5@(j), rA5, rB@(j));
               ATL_vld(rA5, pA4+lda); pA4 += ATL_VLEN;
            ATL_vmac(rC6@(j), rA6, rB@(j));
               ATL_vld(rA6, pA6); 
            ATL_vmac(rC7@(j), rA7, rB@(j));
               ATL_vld(rA7, pA6+lda); pA6 += ATL_VLEN;
         }  /* end K-loop */
/*
 *       Last iteration peeled out to drain preload pipeline
 */
      @iexp j 0 0 +
      @iwhile j < @(nu)
            ATL_vmac(rC0@(j), rA0, rB@(j));
         @iif j = 0
            @iexp k @(nu) -1 +
            @iif @iexp 2 @(k) % 0 =
               ATL_vld(rB@(k), pB@(k));
            @endiif
            @iif @iexp 2 @(k) %
               @iexp lst @(k) -1 +
               ATL_vld(rB@(k), pB@(lst)+ldb);
            @endiif
         @endiif
            ATL_vmac(rC1@(j), rA1, rB@(j));
            ATL_vmac(rC2@(j), rA2, rB@(j));
            ATL_vmac(rC3@(j), rA3, rB@(j));
            ATL_vmac(rC4@(j), rA4, rB@(j));
            ATL_vmac(rC5@(j), rA5, rB@(j));
            ATL_vmac(rC6@(j), rA6, rB@(j));
            ATL_vmac(rC7@(j), rA7, rB@(j));
      
         @iexp j @(j) 1 +
      @endiwhile
         #if ATL_VLEN == 2
      @iexp j 0 0 +
      @iwhile j < @(nu)
            ATL_vvrsum2(rC0@(j), rC1@(j));
            ATL_vvrsum2(rC2@(j), rC3@(j));
            ATL_vvrsum2(rC4@(j), rC5@(j));
            ATL_vvrsum2(rC6@(j), rC7@(j));
         @iexp j @(j) 1 +
      @endiwhile
            #ifdef TCPLX
      @iexp lst @(nu) -1 +
      @iexp j 0 0 +
      @iwhile j < @(nu)
               wrtC(pC0, rC0@(j));
               wrtC(pC0+4, rC2@(j));
               wrtC(pC0+8, rC4@(j));
               wrtC(pC0+12, rC6@(j));
         @iif j < @(lst)
               pC0 += ldc2;
         @endiif
         @iexp j @(j) 1 +
      @endiwhile
            #else
      @iexp j 0 0 +
      @iwhile j < @(nu)
               ATL_vbeta(pC0, rC0@(j));
               ATL_vbeta(pC0+2, rC2@(j));
               ATL_vbeta(pC0+4, rC4@(j));
               ATL_vbeta(pC0+6, rC6@(j));
         @iif j < @(lst)
               pC0 += ldc2;
         @endiif
         @iexp j @(j) 1 +
      @endiwhile
            #endif
         #elif ATL_VLEN == 4
            #if defined(TCPLX) && !defined(BETA0) // want permuted order 
      @iexp j 0 0 +
      @iwhile j < @(lst)
                  ATL_vvrsum4(rC0@(j), rC2@(j), rC1@(j), rC3@(j)); // rC00={r3, r1, r2, r0}
                  ATL_vvrsum4(rC4@(j), rC6@(j), rC5@(j), rC7@(j)); // rC40={r7, r5, r6, r4}
         @iexp j @(j) 1 +
      @endiwhile
                  ATL_vvrsum4(rC0@(j), rC2@(j), rC1@(j), rC3@(j)); // rC00={r3, r1, r2, r0}
            #else                                 // want natural order
      @iexp j 0 0 +
      @iwhile j < @(lst)
                  ATL_vvrsum4(rC0@(j), rC1@(j), rC2@(j), rC3@(j));
                  ATL_vvrsum4(rC4@(j), rC5@(j), rC6@(j), rC7@(j));
         @iexp j @(j) 1 +
      @endiwhile
                  ATL_vvrsum4(rC0@(j), rC1@(j), rC2@(j), rC3@(j));
            #endif
               @iexp j @(nu) -1 +
               ATL_vvrsum4(rC4@(j), rC5@(j), rC6@(j), rC7@(j));
            #ifdef TCPLX
               #ifndef BETA0
                  ATL_vzero(rB0);                      // rB0 ={ 0,  0,  0,  0}
      @iexp j 0 0 +
      @iwhile j < @(lst)
                  vwrtC(pC0, rC0@(j), rC2@(j), rB0);
                  vwrtC(pC0+8, rC4@(j), rC6@(j), rB0);
                  pC0 += ldc2;
         @iexp j @(j) 1 +
      @endiwhile
                  vwrtC(pC0, rC0@(j), rC2@(j), rB0);
                  wrtC(pC0+8, rC4@(j));
               #else
      @iexp j 0 0 +
      @iwhile j < @(lst)
                  wrtC(pC0, rC0@(j));
                  wrtC(pC0+8, rC4@(j));
                  pC0 += ldc2;
         @iexp j @(j) 1 +
      @endiwhile
                  wrtC(pC0, rC0@(j));
                  wrtC(pC0+8, rC4@(j));
               #endif
            #else  /* real */
      @iexp j 0 0 +
      @iwhile j < @(lst)
               ATL_vbeta(pC0, rC0@(j));
               ATL_vbeta(pC0+4, rC4@(j));
               pC0 += ldc;
         @iexp j @(j) 1 +
      @endiwhile
               ATL_vbeta(pC0, rC0@(j));
               ATL_vbeta(pC0+4, rC4@(j));
            #endif
         #else
            #error "VLEN NOT SUPPORTED!"
         #endif
         pC0 -= ldc2*@(lst) - (8 SHIFT);
         pB0 = B;
      @iif nu > 2
         pB2 = B   + (ldb<<1);
      @endiif
      @iexp j 4 0 +
      @iwhile j < @(nu)
         @iexp k @(j) -2 +
         pB@(j) = pB@(k) + (ldb<<1);
         @iexp j @(j) 2 +
      @endiwhile
         A += incAm;
         pA0 = A;
         pA2 = pA0 + (lda<<1);
         pA4 = pA2 + (lda<<1);
         pA6 = pA4 + (lda<<1);
      }  /* end of loop over M */
      A = aa;
      pA0 = A;
      pA2 = A   + (lda<<1);
      pA4 = pA2 + (lda<<1);
      pA6 = pA4 + (lda<<1);
      C += incC;
      pC0 = C;
      B += incBn;
      pB0 = B;
      @iif nu > 2
      pB2 =   B + (ldb<<1);
         @iexp j 4 0 +
         @iwhile j < @(nu)
            @iexp k @(j) -2 +
      pB@(j) = pB@(k) + (ldb<<1);
            @iexp j @(j) 2 +
         @endiwhile
      @endiif
   }  /* end of loop over N */
}
